Subject: Linux-RT 2.6.27-RT
From: http://www.kernel.org/pub/linux/kernel/projects/rt/
Acked-by: Tony Jones <tonyj@suse.de>
Michal Schmidt's fix for load average calculation

From: Michal Schmidt <mschmidt@redhat.com>

Subject: Re: [PATCH] idle load == # of CPUs

This is an attempt to fix https://bugzilla.redhat.com/show_bug.cgi?id=253099

The bug is caused by the fact that the local timer interrupts happen usually
at the same time on all CPUs. All CPUs then raise their timer softirqs. When
calc_load() runs, it sees not only itself but all the other softirq-timer
per-cpu threads running too. And softirq-sched too, which is woken up from
scheduler_tick().

In a BZ comment I speculated about three possible solutions:
(a) Somehow make sure the timer interrupts are synchronized between CPUs, but with a per-CPU offset, so that they don't fire at the same time.
(b) Make the timer softirq threads (and softirq-sched?) special and not take them into loadavg calculation.
(c) Don't calculate loadavg from the timer softirq. It should be possible to run calc_load() periodically from a hrtimer.

This patch implements (b).
Comments welcome.

Michal
---
 fs/proc/proc_misc.c |   11 ++++++++---
 kernel/timer.c      |   49 ++++++++++++++++++++++++++++++++++++++++++++++++-
 2 files changed, 56 insertions(+), 4 deletions(-)

Index: linux-2.6.27-RT/fs/proc/proc_misc.c
===================================================================
--- linux-2.6.27-RT.orig/fs/proc/proc_misc.c
+++ linux-2.6.27-RT/fs/proc/proc_misc.c
@@ -112,10 +112,15 @@ static int loadavg_rt_read_proc(char *pa
 	extern unsigned long rt_nr_running(void);
 	int a, b, c;
 	int len;
+	unsigned long seq;
+
+	do {
+		seq = read_seqbegin(&xtime_lock);
+		a = avenrun_rt[0] + (FIXED_1/200);
+		b = avenrun_rt[1] + (FIXED_1/200);
+		c = avenrun_rt[2] + (FIXED_1/200);
+	} while (read_seqretry(&xtime_lock, seq));
 
-	a = avenrun_rt[0] + (FIXED_1/200);
-	b = avenrun_rt[1] + (FIXED_1/200);
-	c = avenrun_rt[2] + (FIXED_1/200);
 	len = sprintf(page,"%d.%02d %d.%02d %d.%02d %ld/%d %d\n",
 		LOAD_INT(a), LOAD_FRAC(a),
 		LOAD_INT(b), LOAD_FRAC(b),
Index: linux-2.6.27-RT/kernel/timer.c
===================================================================
--- linux-2.6.27-RT.orig/kernel/timer.c
+++ linux-2.6.27-RT/kernel/timer.c
@@ -39,6 +39,7 @@
 #include <linux/tick.h>
 #include <linux/kallsyms.h>
 #include <trace/timer.h>
+#include <linux/kthread.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -1058,7 +1059,7 @@ void update_process_times(int user_tick)
 static unsigned long count_active_tasks(void)
 {
 	/*
-	 * On PREEMPT_RT, we are running in the timer softirq thread,
+	 * On PREEMPT_RT, we are running in the loadavg thread,
 	 * so consider 1 less running tasks:
 	 */
 #ifdef CONFIG_PREEMPT_RT
@@ -1128,6 +1129,50 @@ static inline void calc_load(unsigned lo
 	}
 }
 
+#ifdef CONFIG_PREEMPT_RT
+static int loadavg_calculator(void *data)
+{
+	unsigned long now, last;
+
+	last = jiffies;
+	while (!kthread_should_stop()) {
+		struct timespec delay = {
+			.tv_sec = LOAD_FREQ / HZ,
+			.tv_nsec = 0
+		};
+
+		hrtimer_nanosleep(&delay, NULL, HRTIMER_MODE_REL,
+			CLOCK_MONOTONIC);
+		now = jiffies;
+		write_seqlock_irq(&xtime_lock);
+		calc_load(now - last);
+		write_sequnlock_irq(&xtime_lock);
+		last = now;
+	}
+
+	return 0;
+}
+
+static int __init start_loadavg_calculator(void)
+{
+	struct task_struct *p;
+	struct sched_param param = { .sched_priority = MAX_USER_RT_PRIO/2 };
+
+	p = kthread_create(loadavg_calculator, NULL, "loadavg");
+	if (IS_ERR(p)) {
+		printk(KERN_ERR "Could not create the loadavg thread.\n");
+		return 1;
+	}
+
+	sched_setscheduler(p, SCHED_FIFO, &param);
+	wake_up_process(p);
+
+	return 0;
+}
+
+late_initcall(start_loadavg_calculator);
+#endif
+
 /*
  * Called by the local, per-CPU timer interrupt on SMP.
  */
@@ -1157,7 +1202,9 @@ static inline void update_times(void)
 	ticks = jiffies - last_tick;
 	if (ticks) {
 		last_tick += ticks;
+#ifndef CONFIG_PREEMPT_RT
 		calc_load(ticks);
+#endif
 	}
 	write_sequnlock_irqrestore(&xtime_lock, flags);
 }
