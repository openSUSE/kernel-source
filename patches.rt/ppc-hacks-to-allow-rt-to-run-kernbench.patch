Subject: Linux-RT 2.6.25-rt
From: http://www.kernel.org/pub/linux/kernel/projects/rt/
Acked-by: Sven-Thorsten Dietrich <sdietrich@suse.de>
From paulmck@linux.vnet.ibm.com Fri Jan 11 14:00:39 2008
Date: Wed, 12 Dec 2007 22:10:29 -0800
From: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
To: Steven Rostedt <rostedt@goodmis.org>
Cc: linux-kernel@vger.kernel.org, tony@bakeyournoodle.com, paulus@samba.org,
     benh@kernel.crashing.org, dino@in.ibm.com, tytso@us.ibm.com,
     dvhltc@us.ibm.com, antonb@us.ibm.com
Subject: Re: [PATCH, RFC] hacks to allow -rt to run kernbench on POWER

On Wed, Dec 12, 2007 at 10:56:12PM -0500, Steven Rostedt wrote:
> 
> On Mon, 29 Oct 2007, Paul E. McKenney wrote:
> > diff -urpNa -X dontdiff linux-2.6.23.1-rt4/mm/memory.c linux-2.6.23.1-rt4-fix/mm/memory.c
> > --- linux-2.6.23.1-rt4/mm/memory.c	2007-10-27 22:20:57.000000000 -0700
> > +++ linux-2.6.23.1-rt4-fix/mm/memory.c	2007-10-28 15:40:36.000000000 -0700
> > @@ -664,6 +664,7 @@ static unsigned long zap_pte_range(struc
> >  	int anon_rss = 0;
> >
> >  	pte = pte_offset_map_lock(mm, pmd, addr, &ptl);
> > +	preempt_disable();
> >  	arch_enter_lazy_mmu_mode();
> >  	do {
> >  		pte_t ptent = *pte;
> > @@ -732,6 +733,7 @@ static unsigned long zap_pte_range(struc
> >
> >  	add_mm_rss(mm, file_rss, anon_rss);
> >  	arch_leave_lazy_mmu_mode();
> > +	preempt_enable();
> >  	pte_unmap_unlock(pte - 1, ptl);
> >
> >  	return addr;
> 
> I'm pulling your patch for the above added code. Took me a few hours to
> find the culprit, but I was getting scheduling in atomic bugs. Turns out
> that this code you put "preempt_disable" in calls sleeping spinlocks.
> 
> Might want to run with DEBUG_PREEMPT.

I thought that you had already pulled the above version...

Here is the replacement that I posted on November 9th (with much help
from Ben H):

http://lkml.org/lkml/2007/11/9/114

							Thanx, Paul

Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
---

---
 arch/powerpc/kernel/process.c        |   22 ++++++++++++++++++++++
 arch/powerpc/kernel/prom.c           |    2 +-
 arch/powerpc/mm/tlb_64.c             |    5 ++++-
 arch/powerpc/platforms/pseries/eeh.c |    2 +-
 drivers/of/base.c                    |    2 +-
 include/asm-powerpc/tlb.h            |    5 ++++-
 include/asm-powerpc/tlbflush.h       |   15 ++++++++++-----
 7 files changed, 43 insertions(+), 10 deletions(-)

Index: linux-2.6.25.2-rt1/arch/powerpc/kernel/process.c
===================================================================
--- linux-2.6.25.2-rt1.orig/arch/powerpc/kernel/process.c	2008-05-08 16:17:58.000000000 -0400
+++ linux-2.6.25.2-rt1/arch/powerpc/kernel/process.c	2008-05-08 16:19:38.000000000 -0400
@@ -269,6 +269,10 @@ struct task_struct *__switch_to(struct t
 	struct thread_struct *new_thread, *old_thread;
 	unsigned long flags;
 	struct task_struct *last;
+#ifdef CONFIG_PREEMPT_RT
+	struct ppc64_tlb_batch *batch;
+	int hadbatch;
+#endif /* #ifdef CONFIG_PREEMPT_RT */
 
 #ifdef CONFIG_SMP
 	/* avoid complexity of lazy save/restore of fpu
@@ -347,6 +351,17 @@ struct task_struct *__switch_to(struct t
 	}
 #endif
 
+#ifdef CONFIG_PREEMPT_RT
+	batch = &__get_cpu_var(ppc64_tlb_batch);
+	if (batch->active) {
+		hadbatch = 1;
+		if (batch->index) {
+			__flush_tlb_pending(batch);
+		}
+		batch->active = 0;
+	}
+#endif /* #ifdef CONFIG_PREEMPT_RT */
+
 	local_irq_save(flags);
 
 	account_system_vtime(current);
@@ -363,6 +378,13 @@ struct task_struct *__switch_to(struct t
 
 	local_irq_restore(flags);
 
+#ifdef CONFIG_PREEMPT_RT
+	if (hadbatch) {
+		batch = &__get_cpu_var(ppc64_tlb_batch);
+		batch->active = 1;
+	}
+#endif /* #ifdef CONFIG_PREEMPT_RT */
+
 	return last;
 }
 
Index: linux-2.6.25.2-rt1/arch/powerpc/kernel/prom.c
===================================================================
--- linux-2.6.25.2-rt1.orig/arch/powerpc/kernel/prom.c	2008-05-08 16:17:58.000000000 -0400
+++ linux-2.6.25.2-rt1/arch/powerpc/kernel/prom.c	2008-05-08 16:19:38.000000000 -0400
@@ -79,7 +79,7 @@ struct boot_param_header *initial_boot_p
 
 extern struct device_node *allnodes;	/* temporary while merging */
 
-extern rwlock_t devtree_lock;	/* temporary while merging */
+extern raw_rwlock_t devtree_lock;	/* temporary while merging */
 
 /* export that to outside world */
 struct device_node *of_chosen;
Index: linux-2.6.25.2-rt1/arch/powerpc/mm/tlb_64.c
===================================================================
--- linux-2.6.25.2-rt1.orig/arch/powerpc/mm/tlb_64.c	2008-05-08 16:19:30.000000000 -0400
+++ linux-2.6.25.2-rt1/arch/powerpc/mm/tlb_64.c	2008-05-08 16:19:38.000000000 -0400
@@ -131,7 +131,7 @@ void pgtable_free_tlb(struct mmu_gather 
 void hpte_need_flush(struct mm_struct *mm, unsigned long addr,
 		     pte_t *ptep, unsigned long pte, int huge)
 {
-	struct ppc64_tlb_batch *batch = &__get_cpu_var(ppc64_tlb_batch);
+	struct ppc64_tlb_batch *batch = &get_cpu_var(ppc64_tlb_batch);
 	unsigned long vsid, vaddr;
 	unsigned int psize;
 	int ssize;
@@ -182,6 +182,7 @@ void hpte_need_flush(struct mm_struct *m
 	 */
 	if (!batch->active) {
 		flush_hash_page(vaddr, rpte, psize, ssize, 0);
+		put_cpu_var(ppc64_tlb_batch);
 		return;
 	}
 
@@ -216,12 +217,14 @@ void hpte_need_flush(struct mm_struct *m
 	 */
 	if (machine_is(celleb)) {
 		__flush_tlb_pending(batch);
+		put_cpu_var(ppc64_tlb_batch);
 		return;
 	}
 #endif /* CONFIG_PREEMPT_RT */
 
 	if (i >= PPC64_TLB_BATCH_NR)
 		__flush_tlb_pending(batch);
+	put_cpu_var(ppc64_tlb_batch);
 }
 
 /*
Index: linux-2.6.25.2-rt1/arch/powerpc/platforms/pseries/eeh.c
===================================================================
--- linux-2.6.25.2-rt1.orig/arch/powerpc/platforms/pseries/eeh.c	2008-05-08 16:17:58.000000000 -0400
+++ linux-2.6.25.2-rt1/arch/powerpc/platforms/pseries/eeh.c	2008-05-08 16:19:38.000000000 -0400
@@ -99,7 +99,7 @@ int eeh_subsystem_enabled;
 EXPORT_SYMBOL(eeh_subsystem_enabled);
 
 /* Lock to avoid races due to multiple reports of an error */
-static DEFINE_SPINLOCK(confirm_error_lock);
+static DEFINE_RAW_SPINLOCK(confirm_error_lock);
 
 /* Buffer for reporting slot-error-detail rtas calls. Its here
  * in BSS, and not dynamically alloced, so that it ends up in
Index: linux-2.6.25.2-rt1/drivers/of/base.c
===================================================================
--- linux-2.6.25.2-rt1.orig/drivers/of/base.c	2008-05-08 16:17:58.000000000 -0400
+++ linux-2.6.25.2-rt1/drivers/of/base.c	2008-05-08 16:19:38.000000000 -0400
@@ -25,7 +25,7 @@ struct device_node *allnodes;
 /* use when traversing tree through the allnext, child, sibling,
  * or parent members of struct device_node.
  */
-DEFINE_RWLOCK(devtree_lock);
+DEFINE_RAW_RWLOCK(devtree_lock);
 
 int of_n_addr_cells(struct device_node *np)
 {
Index: linux-2.6.25.2-rt1/include/asm-powerpc/tlbflush.h
===================================================================
--- linux-2.6.25.2-rt1.orig/include/asm-powerpc/tlbflush.h	2008-05-08 16:17:58.000000000 -0400
+++ linux-2.6.25.2-rt1/include/asm-powerpc/tlbflush.h	2008-05-08 16:19:38.000000000 -0400
@@ -109,18 +109,23 @@ extern void hpte_need_flush(struct mm_st
 
 static inline void arch_enter_lazy_mmu_mode(void)
 {
-	struct ppc64_tlb_batch *batch = &__get_cpu_var(ppc64_tlb_batch);
+	struct ppc64_tlb_batch *batch = &get_cpu_var(ppc64_tlb_batch);
 
 	batch->active = 1;
+	put_cpu_var(ppc64_tlb_batch);
 }
 
 static inline void arch_leave_lazy_mmu_mode(void)
 {
-	struct ppc64_tlb_batch *batch = &__get_cpu_var(ppc64_tlb_batch);
+	struct ppc64_tlb_batch *batch = &get_cpu_var(ppc64_tlb_batch);
 
-	if (batch->index)
-		__flush_tlb_pending(batch);
-	batch->active = 0;
+	if (batch->active) {
+		if (batch->index) {
+			__flush_tlb_pending(batch);
+		}
+		batch->active = 0;
+	}
+	put_cpu_var(ppc64_tlb_batch);
 }
 
 #define arch_flush_lazy_mmu_mode()      do {} while (0)
Index: linux-2.6.25.2-rt1/include/asm-powerpc/tlb.h
===================================================================
--- linux-2.6.25.2-rt1.orig/include/asm-powerpc/tlb.h	2008-05-08 16:17:58.000000000 -0400
+++ linux-2.6.25.2-rt1/include/asm-powerpc/tlb.h	2008-05-08 16:19:38.000000000 -0400
@@ -46,8 +46,11 @@ static inline void tlb_flush(struct mmu_
 	 * pages are going to be freed and we really don't want to have a CPU
 	 * access a freed page because it has a stale TLB
 	 */
-	if (tlbbatch->index)
+	if (tlbbatch->index) {
+		preempt_disable();
 		__flush_tlb_pending(tlbbatch);
+		preempt_enable();
+	}
 
 	pte_free_finish();
 }
