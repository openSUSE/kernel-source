Subject: Linux-RT 2.6.24-rc8-rt1
From: http://www.kernel.org/pub/linux/kernel/projects/rt/
Acked-by: Sven-Thorsten Dietrich <sdietrich@suse.de>
Subject: lock_list - a fine grain locked double linked list

Provide a simple fine grain locked double link list.

It build upon the regular double linked list primitives, spinlocks and RCU.

In order to avoid deadlocks a prev -> next locking order is observed. This
prevents reverse iteration.

Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
Signed-off-by: Ingo Molnar <mingo@elte.hu>
---
 include/linux/lock_list.h |   74 +++++++++++++++++++++++++++++++
 lib/Makefile              |    2 
 lib/lock_list.c           |  107 ++++++++++++++++++++++++++++++++++++++++++++++
 3 files changed, 182 insertions(+), 1 deletion(-)

Index: linux-2.6.24-rc8-rt1/include/linux/lock_list.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.24-rc8-rt1/include/linux/lock_list.h	2008-01-16 22:28:59.000000000 -0500
@@ -0,0 +1,74 @@
+/*
+ * Copyright (C) 2006, Red Hat, Inc., Peter Zijlstra <pzijlstr@redhat.com>
+ * Licenced under the GPLv2.
+ *
+ * Simple fine grain locked double linked list.
+ */
+#ifndef _LINUX_LOCK_LIST_H
+#define _LINUX_LOCK_LIST_H
+
+#ifdef __KERNEL__
+
+#include <linux/list.h>
+#include <linux/rcupdate.h>
+#include <linux/spinlock.h>
+
+struct lock_list_head {
+	union {
+		struct list_head head;
+		struct {
+			struct lock_list_head *next, *prev;
+		};
+	};
+	spinlock_t lock;
+};
+
+enum {
+	LOCK_LIST_NESTING_PREV = 1,
+	LOCK_LIST_NESTING_CUR,
+	LOCK_LIST_NESTING_NEXT,
+};
+
+static inline void INIT_LOCK_LIST_HEAD(struct lock_list_head *list)
+{
+	INIT_LIST_HEAD(&list->head);
+	spin_lock_init(&list->lock);
+}
+
+/*
+ * Passed pointers are assumed stable by external means (refcount, rcu)
+ */
+extern void lock_list_add(struct lock_list_head *new,
+			  struct lock_list_head *list);
+extern void lock_list_del_init(struct lock_list_head *entry);
+extern void lock_list_splice_init(struct lock_list_head *list,
+				  struct lock_list_head *head);
+
+struct lock_list_head *lock_list_next_entry(struct lock_list_head *list,
+					    struct lock_list_head *entry);
+struct lock_list_head *lock_list_first_entry(struct lock_list_head *list);
+
+#define lock_list_for_each_entry(pos, list, member)			\
+	for (pos = list_entry(lock_list_first_entry(list), 		\
+			      typeof(*pos), member); 			\
+	     pos;							\
+	     pos = list_entry(lock_list_next_entry(list, &pos->member),	\
+			      typeof(*pos), member))
+
+/*
+ * to be used when iteration is terminated by breaking out of the
+ * lock_list_for_each_entry() loop.
+ *
+ * 	lock_list_for_each_entry(i, list, member) {
+ * 		if (cond) {
+ * 			lock_list_for_each_entry_stop(i, member);
+ * 			goto foo;
+ * 		}
+ * 	}
+ *
+ */
+#define lock_list_for_each_entry_stop(pos, member)			\
+	spin_unlock(&(pos->member.lock))
+
+#endif /* __KERNEL__ */
+#endif /* _LINUX_LOCK_LIST_H */
Index: linux-2.6.24-rc8-rt1/lib/Makefile
===================================================================
--- linux-2.6.24-rc8-rt1.orig/lib/Makefile	2008-01-16 22:28:49.000000000 -0500
+++ linux-2.6.24-rc8-rt1/lib/Makefile	2008-01-16 22:28:59.000000000 -0500
@@ -3,7 +3,7 @@
 #
 
 lib-y := ctype.o string.o vsprintf.o cmdline.o \
-	 rbtree.o radix-tree.o dump_stack.o \
+	 rbtree.o radix-tree.o dump_stack.o lock_list.o \
 	 idr.o int_sqrt.o extable.o prio_tree.o \
 	 sha1.o irq_regs.o reciprocal_div.o argv_split.o \
 	 proportions.o prio_heap.o
Index: linux-2.6.24-rc8-rt1/lib/lock_list.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.24-rc8-rt1/lib/lock_list.c	2008-01-16 22:28:59.000000000 -0500
@@ -0,0 +1,107 @@
+/*
+ * Copyright (C) 2006, Red Hat, Inc., Peter Zijlstra <pzijlstr@redhat.com>
+ * Licenced under the GPLv2.
+ *
+ * Simple fine grain locked double linked list.
+ *
+ * Locking order is from prev -> next.
+ * Edges are locked not nodes; that is, cur->lock protects:
+ *  - cur->next,
+ *  - cur->next->prev.
+ *
+ * Passed pointers are assumed to be stable by external means such as
+ * refcounts or RCU. The individual list entries are assumed to be RCU
+ * freed (requirement of __lock_list_del).
+ */
+
+#include <linux/lock_list.h>
+
+void lock_list_add(struct lock_list_head *new,
+		   struct lock_list_head *list)
+{
+	struct lock_list_head *next;
+
+	spin_lock(&new->lock);
+	spin_lock_nested(&list->lock, LOCK_LIST_NESTING_PREV);
+	next = list->next;
+	__list_add(&new->head, &list->head, &next->head);
+	spin_unlock(&list->lock);
+	spin_unlock(&new->lock);
+}
+
+static spinlock_t *__lock_list(struct lock_list_head *entry)
+{
+	struct lock_list_head *prev;
+	spinlock_t *lock = NULL;
+
+again:
+	prev = entry->prev;
+	if (prev == entry)
+		goto one;
+	spin_lock_nested(&prev->lock, LOCK_LIST_NESTING_PREV);
+	if (unlikely(entry->prev != prev)) {
+		/*
+		 * we lost
+		 */
+		spin_unlock(&prev->lock);
+		goto again;
+	}
+	lock = &prev->lock;
+one:
+	spin_lock_nested(&entry->lock, LOCK_LIST_NESTING_CUR);
+	return lock;
+}
+
+void lock_list_del_init(struct lock_list_head *entry)
+{
+	spinlock_t *lock;
+
+	rcu_read_lock();
+	lock = __lock_list(entry);
+	list_del_init(&entry->head);
+	spin_unlock(&entry->lock);
+	if (lock)
+		spin_unlock(lock);
+	rcu_read_unlock();
+}
+
+void lock_list_splice_init(struct lock_list_head *list,
+			struct lock_list_head *head)
+{
+	spinlock_t *lock;
+
+	rcu_read_lock();
+	lock = __lock_list(list);
+	if (!list_empty(&list->head)) {
+		spin_lock_nested(&head->lock, LOCK_LIST_NESTING_NEXT);
+		__list_splice(&list->head, &head->head);
+		INIT_LIST_HEAD(&list->head);
+		spin_unlock(&head->lock);
+	}
+	spin_unlock(&list->lock);
+	if (lock)
+		spin_unlock(lock);
+	rcu_read_unlock();
+}
+
+struct lock_list_head *lock_list_next_entry(struct lock_list_head *list,
+					    struct lock_list_head *entry)
+{
+	struct lock_list_head *next = entry->next;
+	if (likely(next != list)) {
+		lock_set_subclass(&entry->lock.dep_map,
+				  LOCK_LIST_NESTING_CUR, _THIS_IP_);
+		spin_lock_nested(&next->lock, LOCK_LIST_NESTING_NEXT);
+		BUG_ON(entry->next != next);
+	} else
+		next = NULL;
+	spin_unlock(&entry->lock);
+	return next;
+}
+
+struct lock_list_head *lock_list_first_entry(struct lock_list_head *list)
+{
+	spin_lock(&list->lock);
+	return lock_list_next_entry(list, list);
+}
+
