Subject: Linux-RT 2.6.25.4-RT
From: http://www.kernel.org/pub/linux/kernel/projects/rt/
Acked-by: Sven-Thorsten Dietrich <sdietrich@suse.de>
From h-shimamoto@ct.jp.nec.com Thu May 15 09:57:50 2008
Date: Thu, 17 Apr 2008 16:57:20 +0200
From: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
To: Peter Zijlstra <peterz@infradead.org>
Cc: Steven Rostedt <rostedt@goodmis.org>,
     linux-rt-users <linux-rt-users@vger.kernel.org>,
     Ingo Molnar <mingo@elte.hu>, Thomas Gleixner <tglx@linutronix.de>,
     LKML <linux-kernel@vger.kernel.org>
Subject: [PATCH -rt] avoid deadlock related with PG_nonewrefs and swap_lock
Resent-Date: Thu, 17 Apr 2008 14:57:28 +0000 (UTC)
Resent-From: Peter Zijlstra <peterz@infradead.org>
Resent-To: Steven Rostedt <rostedt@goodmis.org>

Hi Peter,

I've updated the patch. Could you please review it?

I'm also thinking that it can be in the mainline because it makes
the lock period shorter, correct?

---
From: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>

There is a deadlock scenario; remove_mapping() vs free_swap_and_cache().
remove_mapping() turns PG_nonewrefs bit on, then locks swap_lock.
free_swap_and_cache() locks swap_lock, then wait to turn PG_nonewrefs bit
off in find_get_page().

swap_lock can be unlocked before calling find_get_page().

In remove_exclusive_swap_page(), there is similar lock sequence;
swap_lock, then PG_nonewrefs bit. swap_lock can be unlocked before
turning PG_nonewrefs bit on.

Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
---
 mm/swapfile.c |    5 +++--
 1 file changed, 3 insertions(+), 2 deletions(-)

Index: linux-2.6.25.4-rt2/mm/swapfile.c
===================================================================
--- linux-2.6.25.4-rt2.orig/mm/swapfile.c	2008-05-19 16:56:33.000000000 -0400
+++ linux-2.6.25.4-rt2/mm/swapfile.c	2008-05-19 16:56:36.000000000 -0400
@@ -367,6 +367,7 @@ int remove_exclusive_swap_page(struct pa
 	/* Is the only swap cache user the cache itself? */
 	retval = 0;
 	if (p->swap_map[swp_offset(entry)] == 1) {
+		spin_unlock(&swap_lock);
 		/* Recheck the page count with the swapcache lock held.. */
 		lock_page_ref_irq(page);
 		if ((page_count(page) == 2) && !PageWriteback(page)) {
@@ -375,8 +376,8 @@ int remove_exclusive_swap_page(struct pa
 			retval = 1;
 		}
 		unlock_page_ref_irq(page);
-	}
-	spin_unlock(&swap_lock);
+	} else
+		spin_unlock(&swap_lock);
 
 	if (retval) {
 		swap_free(entry);
