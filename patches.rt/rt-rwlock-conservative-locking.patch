Subject: Linux-RT 2.6.26-RT
From: http://www.kernel.org/pub/linux/kernel/projects/rt/
Acked-by: Sven-Thorsten Dietrich <sdietrich@suse.de>
Date: Tue, 15 Jul 2008 20:26:50 -0400 (EDT)
From: Steven Rostedt <rostedt@goodmis.org>
Subject: [PATCH RT] rwlock: be more conservative in locking reader_lock_count

John Stultz was hitting one of the rwlock warnings. This was indeed a bug.
The assumption of trying not to take locks was incorrect and prone to
bugs.  This patch adds a few locks around the needed areas to correct the
issue and make the code a bit more robust.

Signed-off-by: Steven Rostedt <srostedt@redhat.com>
Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
---
 kernel/rtmutex.c |   16 ++++++++--------
 1 file changed, 8 insertions(+), 8 deletions(-)

Index: linux-2.6.26/kernel/rtmutex.c
===================================================================
--- linux-2.6.26.orig/kernel/rtmutex.c
+++ linux-2.6.26/kernel/rtmutex.c
@@ -1138,16 +1138,13 @@ rt_rwlock_update_owner(struct rw_mutex *
 	if (own == RT_RW_READER)
 		return;
 
-	/*
-	 * We don't need to grab the pi_lock to look at the reader list
-	 * since we hold the rwm wait_lock. We only care about the pointer
-	 * to this lock, and we own the wait_lock, so that pointer
-	 * can't be changed.
-	 */
+	spin_lock(&own->pi_lock);
 	for (i = own->reader_lock_count - 1; i >= 0; i--) {
 		if (own->owned_read_locks[i].lock == rwm)
 			break;
 	}
+	spin_unlock(&own->pi_lock);
+
 	/* It is possible the owner didn't add it yet */
 	if (i < 0)
 		return;
@@ -1454,7 +1451,6 @@ __rt_read_fasttrylock(struct rw_mutex *r
 			current->owned_read_locks[reader_count].count = 1;
 		} else
 			WARN_ON_ONCE(1);
-		spin_unlock(&current->pi_lock);
 		/*
 		 * If this task is no longer the sole owner of the lock
 		 * or someone is blocking, then we need to add the task
@@ -1464,12 +1460,16 @@ __rt_read_fasttrylock(struct rw_mutex *r
 			struct rt_mutex *mutex = &rwm->mutex;
 			struct reader_lock_struct *rls;
 
+			/* preserve lock order, we only need wait_lock now */
+			spin_unlock(&current->pi_lock);
+
 			spin_lock(&mutex->wait_lock);
 			rls = &current->owned_read_locks[reader_count];
 			if (!rls->list.prev || list_empty(&rls->list))
 				list_add(&rls->list, &rwm->readers);
 			spin_unlock(&mutex->wait_lock);
-		}
+		} else
+			spin_unlock(&current->pi_lock);
 		local_irq_restore(flags);
 		return 1;
 	}
