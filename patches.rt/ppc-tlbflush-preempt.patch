Subject: Linux-RT 2.6.25-rt
From: http://www.kernel.org/pub/linux/kernel/projects/rt/
Acked-by: Sven-Thorsten Dietrich <sdietrich@suse.de>
From estarkov@ru.mvista.com Mon Mar 24 17:41:35 2008
Date: Wed, 12 Mar 2008 18:37:42 +0300
From: Egor Starkov <estarkov@ru.mvista.com>
To: mingo@elte.hu
Subject: Memory corruption fixes
Resent-Date: Wed, 12 Mar 2008 17:06:59 +0100
Resent-Date: Wed, 12 Mar 2008 12:10:04 -0400
Resent-From: Ingo Molnar <mingo@elte.hu>
Resent-To: Steven Rostedt <rostedt@goodmis.org>

Hi Ingo,

I have found out that functions __flush_tlb_pending and hpte_need_flush 
must be called from within some kind of spinlock/non-preempt region. Fix 
"flush_hash_page_fix.patch" is attached.

Also debug version of function add_preempt_count can be called
on early stage of boot when current is not set and is 0.
So we can have memory corruption. I had it as stack pointer
exception after "Freeing unused kernel memory" message. Fix 
"preempt_debug_trace_fix.patch" is attached.

Egor Starkov

    [ Part 2: "Attached Text" ]

Signed-off-by: Egor Starkov <estarkov@ru.mvista.com>
Description:
	Functions __flush_tlb_pending and hpte_need_flush must
	be called from within some kind of spinlock/non-preempt region

---
 include/asm-powerpc/pgtable-ppc64.h |    9 ++++++++-
 include/asm-powerpc/tlbflush.h      |   20 ++++++++++++++++++--
 2 files changed, 26 insertions(+), 3 deletions(-)

Index: linux-2.6.25.2-rt1/include/asm-powerpc/tlbflush.h
===================================================================
--- linux-2.6.25.2-rt1.orig/include/asm-powerpc/tlbflush.h	2008-05-08 16:19:38.000000000 -0400
+++ linux-2.6.25.2-rt1/include/asm-powerpc/tlbflush.h	2008-05-08 16:19:42.000000000 -0400
@@ -109,7 +109,15 @@ extern void hpte_need_flush(struct mm_st
 
 static inline void arch_enter_lazy_mmu_mode(void)
 {
-	struct ppc64_tlb_batch *batch = &get_cpu_var(ppc64_tlb_batch);
+	struct ppc64_tlb_batch *batch;
+#ifdef CONFIG_PREEMPT_RT
+	preempt_disable();
+#endif
+	batch = &get_cpu_var(ppc64_tlb_batch);
+
+#ifdef CONFIG_PREEMPT_RT
+	preempt_enable();
+#endif
 
 	batch->active = 1;
 	put_cpu_var(ppc64_tlb_batch);
@@ -117,7 +125,12 @@ static inline void arch_enter_lazy_mmu_m
 
 static inline void arch_leave_lazy_mmu_mode(void)
 {
-	struct ppc64_tlb_batch *batch = &get_cpu_var(ppc64_tlb_batch);
+	struct ppc64_tlb_batch *batch;
+
+#ifdef CONFIG_PREEMPT_RT
+	preempt_disable();
+#endif
+	batch = &get_cpu_var(ppc64_tlb_batch);
 
 	if (batch->active) {
 		if (batch->index) {
@@ -125,6 +138,9 @@ static inline void arch_leave_lazy_mmu_m
 		}
 		batch->active = 0;
 	}
+#ifdef CONFIG_PREEMPT_RT
+	preempt_enable();
+#endif
 	put_cpu_var(ppc64_tlb_batch);
 }
 
Index: linux-2.6.25.2-rt1/include/asm-powerpc/pgtable-ppc64.h
===================================================================
--- linux-2.6.25.2-rt1.orig/include/asm-powerpc/pgtable-ppc64.h	2008-05-08 16:17:57.000000000 -0400
+++ linux-2.6.25.2-rt1/include/asm-powerpc/pgtable-ppc64.h	2008-05-08 16:19:42.000000000 -0400
@@ -277,8 +277,15 @@ static inline unsigned long pte_update(s
 	: "r" (ptep), "r" (clr), "m" (*ptep), "i" (_PAGE_BUSY)
 	: "cc" );
 
-	if (old & _PAGE_HASHPTE)
+	if (old & _PAGE_HASHPTE) {
+#ifdef CONFIG_PREEMPT_RT
+		preempt_disable();
+#endif
 		hpte_need_flush(mm, addr, ptep, old, huge);
+#ifdef CONFIG_PREEMPT_RT
+		preempt_enable();
+#endif
+	}
 	return old;
 }
 
