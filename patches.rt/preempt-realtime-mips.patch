 arch/mips/Kconfig                 |   13 +-
 arch/mips/kernel/asm-offsets.c    |    2 
 arch/mips/kernel/entry.S          |   22 ++--
 arch/mips/kernel/i8259.c          |    2 
 arch/mips/kernel/module.c         |    2 
 arch/mips/kernel/process.c        |    8 -
 arch/mips/kernel/scall32-o32.S    |    2 
 arch/mips/kernel/scall64-64.S     |    2 
 arch/mips/kernel/scall64-n32.S    |    2 
 arch/mips/kernel/scall64-o32.S    |    2 
 arch/mips/kernel/semaphore.c      |   22 ++--
 arch/mips/kernel/signal.c         |    4 
 arch/mips/kernel/signal32.c       |    4 
 arch/mips/kernel/smp.c            |   27 ++++
 arch/mips/kernel/time.c           |  208 ++++++++++++++++++++++++++++++++++++--
 arch/mips/kernel/traps.c          |    2 
 arch/mips/mm/init.c               |    2 
 arch/mips/sibyte/cfe/smp.c        |    4 
 arch/mips/sibyte/sb1250/irq.c     |   10 +
 arch/mips/sibyte/sb1250/smp.c     |    2 
 arch/mips/sibyte/swarm/setup.c    |    6 +
 include/asm-mips/asmmacro.h       |    8 -
 include/asm-mips/atomic.h         |    1 
 include/asm-mips/bitops.h         |    5 
 include/asm-mips/hw_irq.h         |    1 
 include/asm-mips/i8259.h          |    2 
 include/asm-mips/io.h             |    1 
 include/asm-mips/linkage.h        |    5 
 include/asm-mips/m48t35.h         |    2 
 include/asm-mips/mipsregs.h       |    4 
 include/asm-mips/rwsem.h          |  176 ++++++++++++++++++++++++++++++++
 include/asm-mips/semaphore.h      |   33 +++---
 include/asm-mips/spinlock.h       |   18 +--
 include/asm-mips/spinlock_types.h |    4 
 include/asm-mips/thread_info.h    |    2 
 include/asm-mips/time.h           |    2 
 include/asm-mips/timeofday.h      |    5 
 include/asm-mips/uaccess.h        |   12 --
 38 files changed, 536 insertions(+), 93 deletions(-)

Index: linux-2.6.22/arch/mips/Kconfig
===================================================================
--- linux-2.6.22.orig/arch/mips/Kconfig	2007-07-24 08:57:19.000000000 +0200
+++ linux-2.6.22/arch/mips/Kconfig	2007-07-24 08:57:25.000000000 +0200
@@ -674,18 +674,16 @@ source "arch/mips/philips/pnx8550/common
 
 endmenu
 
+
 config RWSEM_GENERIC_SPINLOCK
 	bool
-	depends on !PREEMPT_RT
 	default y
 
 config RWSEM_XCHGADD_ALGORITHM
 	bool
-	depends on !PREEMPT_RT
 
 config ASM_SEMAPHORES
 	bool
-#	depends on !PREEMPT_RT
 	default y
 
 config ARCH_HAS_ILOG2_U32
@@ -1786,6 +1784,15 @@ config SECCOMP
 
 	  If unsure, say Y. Only embedded should say N here.
 
+config GENERIC_TIME
+	bool
+	default y
+
+source "kernel/time/Kconfig"
+
+config CPU_SPEED
+	int "CPU speed used for clocksource/clockevent calculations"
+	default 600
 endmenu
 
 config LOCKDEP_SUPPORT
Index: linux-2.6.22/arch/mips/kernel/asm-offsets.c
===================================================================
--- linux-2.6.22.orig/arch/mips/kernel/asm-offsets.c	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/arch/mips/kernel/asm-offsets.c	2007-07-24 08:57:25.000000000 +0200
@@ -10,9 +10,11 @@
  */
 #include <linux/compat.h>
 #include <linux/types.h>
+#include <linux/linkage.h>
 #include <linux/sched.h>
 #include <linux/mm.h>
 #include <linux/interrupt.h>
+#include <linux/irqflags.h>
 
 #include <asm/ptrace.h>
 #include <asm/processor.h>
Index: linux-2.6.22/arch/mips/kernel/entry.S
===================================================================
--- linux-2.6.22.orig/arch/mips/kernel/entry.S	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/arch/mips/kernel/entry.S	2007-07-24 08:57:25.000000000 +0200
@@ -30,7 +30,7 @@
 	.align	5
 #ifndef CONFIG_PREEMPT
 FEXPORT(ret_from_exception)
-	local_irq_disable			# preempt stop
+	raw_local_irq_disable			# preempt stop
 	b	__ret_from_irq
 #endif
 FEXPORT(ret_from_irq)
@@ -41,7 +41,7 @@ FEXPORT(__ret_from_irq)
 	beqz	t0, resume_kernel
 
 resume_userspace:
-	local_irq_disable		# make sure we dont miss an
+	raw_local_irq_disable	# make sure we dont miss an
 					# interrupt setting need_resched
 					# between sampling and return
 	LONG_L	a2, TI_FLAGS($28)	# current->work
@@ -51,7 +51,9 @@ resume_userspace:
 
 #ifdef CONFIG_PREEMPT
 resume_kernel:
-	local_irq_disable
+	raw_local_irq_disable
+	lw	t0, kernel_preemption
+	beqz	t0, restore_all
 	lw	t0, TI_PRE_COUNT($28)
 	bnez	t0, restore_all
 need_resched:
@@ -61,7 +63,9 @@ need_resched:
 	LONG_L	t0, PT_STATUS(sp)		# Interrupts off?
 	andi	t0, 1
 	beqz	t0, restore_all
+	raw_local_irq_disable
 	jal	preempt_schedule_irq
+	sw      zero, TI_PRE_COUNT($28)
 	b	need_resched
 #endif
 
@@ -69,7 +73,7 @@ FEXPORT(ret_from_fork)
 	jal	schedule_tail		# a0 = struct task_struct *prev
 
 FEXPORT(syscall_exit)
-	local_irq_disable		# make sure need_resched and
+	raw_local_irq_disable	# make sure need_resched and
 					# signals dont change between
 					# sampling and return
 	LONG_L	a2, TI_FLAGS($28)	# current->work
@@ -140,19 +144,21 @@ FEXPORT(restore_partial)		# restore part
 	.set	at
 
 work_pending:
-	andi	t0, a2, _TIF_NEED_RESCHED # a2 is preloaded with TI_FLAGS
+					# a2 is preloaded with TI_FLAGS
+	andi	t0, a2, (_TIF_NEED_RESCHED|_TIF_NEED_RESCHED_DELAYED)
 	beqz	t0, work_notifysig
 work_resched:
+	raw_local_irq_enable  t0
 	jal	schedule
 
-	local_irq_disable		# make sure need_resched and
+	raw_local_irq_disable	# make sure need_resched and
 					# signals dont change between
 					# sampling and return
 	LONG_L	a2, TI_FLAGS($28)
 	andi	t0, a2, _TIF_WORK_MASK	# is there any work to be done
 					# other than syscall tracing?
 	beqz	t0, restore_all
-	andi	t0, a2, _TIF_NEED_RESCHED
+	andi	t0, a2, (_TIF_NEED_RESCHED|_TIF_NEED_RESCHED_DELAYED)
 	bnez	t0, work_resched
 
 work_notifysig:				# deal with pending signals and
@@ -168,7 +174,7 @@ syscall_exit_work:
 	li	t0, _TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT
 	and	t0, a2			# a2 is preloaded with TI_FLAGS
 	beqz	t0, work_pending	# trace bit set?
-	local_irq_enable		# could let do_syscall_trace()
+	raw_local_irq_enable	# could let do_syscall_trace()
 					# call schedule() instead
 	move	a0, sp
 	li	a1, 1
Index: linux-2.6.22/arch/mips/kernel/i8259.c
===================================================================
--- linux-2.6.22.orig/arch/mips/kernel/i8259.c	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/arch/mips/kernel/i8259.c	2007-07-24 08:57:25.000000000 +0200
@@ -29,9 +29,9 @@
  */
 
 static int i8259A_auto_eoi = -1;
-DEFINE_SPINLOCK(i8259A_lock);
 /* some platforms call this... */
 void mask_and_ack_8259A(unsigned int);
+DEFINE_RAW_SPINLOCK(i8259A_lock);
 
 static struct irq_chip i8259A_chip = {
 	.name		= "XT-PIC",
Index: linux-2.6.22/arch/mips/kernel/module.c
===================================================================
--- linux-2.6.22.orig/arch/mips/kernel/module.c	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/arch/mips/kernel/module.c	2007-07-24 08:57:25.000000000 +0200
@@ -40,7 +40,7 @@ struct mips_hi16 {
 static struct mips_hi16 *mips_hi16_list;
 
 static LIST_HEAD(dbe_list);
-static DEFINE_SPINLOCK(dbe_lock);
+static DEFINE_RAW_SPINLOCK(dbe_lock);
 
 void *module_alloc(unsigned long size)
 {
Index: linux-2.6.22/arch/mips/kernel/process.c
===================================================================
--- linux-2.6.22.orig/arch/mips/kernel/process.c	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/arch/mips/kernel/process.c	2007-07-24 08:57:25.000000000 +0200
@@ -50,7 +50,7 @@ ATTRIB_NORET void cpu_idle(void)
 {
 	/* endless idle loop with no priority at all */
 	while (1) {
-		while (!need_resched()) {
+		while (!need_resched() && !need_resched_delayed()) {
 #ifdef CONFIG_SMTC_IDLE_HOOK_DEBUG
 			extern void smtc_idle_loop_hook(void);
 
@@ -59,9 +59,11 @@ ATTRIB_NORET void cpu_idle(void)
 			if (cpu_wait)
 				(*cpu_wait)();
 		}
-		preempt_enable_no_resched();
-		schedule();
+		local_irq_disable();
+		__preempt_enable_no_resched();
+		__schedule();
 		preempt_disable();
+		local_irq_enable();
 	}
 }
 
Index: linux-2.6.22/arch/mips/kernel/scall32-o32.S
===================================================================
--- linux-2.6.22.orig/arch/mips/kernel/scall32-o32.S	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/arch/mips/kernel/scall32-o32.S	2007-07-24 08:57:25.000000000 +0200
@@ -73,7 +73,7 @@ stack_done:
 1:	sw	v0, PT_R2(sp)		# result
 
 o32_syscall_exit:
-	local_irq_disable		# make sure need_resched and
+	raw_local_irq_disable	# make sure need_resched and
 					# signals dont change between
 					# sampling and return
 	lw	a2, TI_FLAGS($28)	# current->work
Index: linux-2.6.22/arch/mips/kernel/scall64-64.S
===================================================================
--- linux-2.6.22.orig/arch/mips/kernel/scall64-64.S	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/arch/mips/kernel/scall64-64.S	2007-07-24 08:57:25.000000000 +0200
@@ -72,7 +72,7 @@ NESTED(handle_sys64, PT_SIZE, sp)
 1:	sd	v0, PT_R2(sp)		# result
 
 n64_syscall_exit:
-	local_irq_disable		# make sure need_resched and
+	raw_local_irq_disable		# make sure need_resched and
 					# signals dont change between
 					# sampling and return
 	LONG_L	a2, TI_FLAGS($28)	# current->work
Index: linux-2.6.22/arch/mips/kernel/scall64-n32.S
===================================================================
--- linux-2.6.22.orig/arch/mips/kernel/scall64-n32.S	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/arch/mips/kernel/scall64-n32.S	2007-07-24 08:57:25.000000000 +0200
@@ -69,7 +69,7 @@ NESTED(handle_sysn32, PT_SIZE, sp)
 	sd	v0, PT_R0(sp)		# set flag for syscall restarting
 1:	sd	v0, PT_R2(sp)		# result
 
-	local_irq_disable		# make sure need_resched and
+	raw_local_irq_disable		# make sure need_resched and
 					# signals dont change between
 					# sampling and return
 	LONG_L  a2, TI_FLAGS($28)	# current->work
Index: linux-2.6.22/arch/mips/kernel/scall64-o32.S
===================================================================
--- linux-2.6.22.orig/arch/mips/kernel/scall64-o32.S	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/arch/mips/kernel/scall64-o32.S	2007-07-24 08:57:25.000000000 +0200
@@ -98,7 +98,7 @@ NESTED(handle_sys, PT_SIZE, sp)
 1:	sd	v0, PT_R2(sp)		# result
 
 o32_syscall_exit:
-	local_irq_disable		# make need_resched and
+	raw_local_irq_disable		# make need_resched and
 					# signals dont change between
 					# sampling and return
 	LONG_L	a2, TI_FLAGS($28)
Index: linux-2.6.22/arch/mips/kernel/semaphore.c
===================================================================
--- linux-2.6.22.orig/arch/mips/kernel/semaphore.c	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/arch/mips/kernel/semaphore.c	2007-07-24 08:57:25.000000000 +0200
@@ -36,7 +36,7 @@
  * sem->count and sem->waking atomic.  Scalability isn't an issue because
  * this lock is used on UP only so it's just an empty variable.
  */
-static inline int __sem_update_count(struct semaphore *sem, int incr)
+static inline int __sem_update_count(struct compat_semaphore *sem, int incr)
 {
 	int old_count, tmp;
 
@@ -67,7 +67,7 @@ static inline int __sem_update_count(str
 		: "=&r" (old_count), "=&r" (tmp), "=m" (sem->count)
 		: "r" (incr), "m" (sem->count));
 	} else {
-		static DEFINE_SPINLOCK(semaphore_lock);
+		static DEFINE_RAW_SPINLOCK(semaphore_lock);
 		unsigned long flags;
 
 		spin_lock_irqsave(&semaphore_lock, flags);
@@ -80,7 +80,7 @@ static inline int __sem_update_count(str
 	return old_count;
 }
 
-void __up(struct semaphore *sem)
+void __compat_up(struct compat_semaphore *sem)
 {
 	/*
 	 * Note that we incremented count in up() before we came here,
@@ -94,7 +94,7 @@ void __up(struct semaphore *sem)
 	wake_up(&sem->wait);
 }
 
-EXPORT_SYMBOL(__up);
+EXPORT_SYMBOL(__compat_up);
 
 /*
  * Note that when we come in to __down or __down_interruptible,
@@ -104,7 +104,7 @@ EXPORT_SYMBOL(__up);
  * Thus it is only when we decrement count from some value > 0
  * that we have actually got the semaphore.
  */
-void __sched __down(struct semaphore *sem)
+void __sched __compat_down(struct compat_semaphore *sem)
 {
 	struct task_struct *tsk = current;
 	DECLARE_WAITQUEUE(wait, tsk);
@@ -133,9 +133,9 @@ void __sched __down(struct semaphore *se
 	wake_up(&sem->wait);
 }
 
-EXPORT_SYMBOL(__down);
+EXPORT_SYMBOL(__compat_down);
 
-int __sched __down_interruptible(struct semaphore * sem)
+int __sched __compat_down_interruptible(struct compat_semaphore * sem)
 {
 	int retval = 0;
 	struct task_struct *tsk = current;
@@ -165,4 +165,10 @@ int __sched __down_interruptible(struct 
 	return retval;
 }
 
-EXPORT_SYMBOL(__down_interruptible);
+EXPORT_SYMBOL(__compat_down_interruptible);
+
+int fastcall compat_sem_is_locked(struct compat_semaphore *sem)
+{
+	return (int) atomic_read(&sem->count) < 0;
+}
+EXPORT_SYMBOL(compat_sem_is_locked);
Index: linux-2.6.22/arch/mips/kernel/signal.c
===================================================================
--- linux-2.6.22.orig/arch/mips/kernel/signal.c	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/arch/mips/kernel/signal.c	2007-07-24 08:57:25.000000000 +0200
@@ -629,6 +629,10 @@ static void do_signal(struct pt_regs *re
 	siginfo_t info;
 	int signr;
 
+#ifdef CONFIG_PREEMPT_RT
+	local_irq_enable();
+	preempt_check_resched();
+#endif
 	/*
 	 * We want the common case to go fast, which is why we may in certain
 	 * cases get here from kernel mode. Just return without doing anything
Index: linux-2.6.22/arch/mips/kernel/signal32.c
===================================================================
--- linux-2.6.22.orig/arch/mips/kernel/signal32.c	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/arch/mips/kernel/signal32.c	2007-07-24 08:57:25.000000000 +0200
@@ -656,6 +656,10 @@ static int setup_rt_frame_32(struct k_si
 	if (err)
 		goto give_sigsegv;
 
+#ifdef CONFIG_PREEMPT_RT
+	local_irq_enable();
+	preempt_check_resched();
+#endif
 	/*
 	 * Arguments to signal handler:
 	 *
Index: linux-2.6.22/arch/mips/kernel/smp.c
===================================================================
--- linux-2.6.22.orig/arch/mips/kernel/smp.c	2007-07-24 08:56:50.000000000 +0200
+++ linux-2.6.22/arch/mips/kernel/smp.c	2007-07-24 08:57:25.000000000 +0200
@@ -88,7 +88,22 @@ asmlinkage __cpuinit void start_secondar
 	cpu_idle();
 }
 
-DEFINE_SPINLOCK(smp_call_lock);
+DEFINE_RAW_SPINLOCK(smp_call_lock);
+
+/*
+ * this function sends a 'reschedule' IPI to all other CPUs.
+ * This is used when RT tasks are starving and other CPUs
+ * might be able to run them.
+ */
+void smp_send_reschedule_allbutself(void)
+{
+	int cpu = smp_processor_id();
+	int i;
+
+	for (i = 0; i < NR_CPUS; i++)
+		if (cpu_online(i) && i != cpu)
+			core_send_ipi(i, SMP_RESCHEDULE_YOURSELF);
+}
 
 struct call_data_struct *call_data;
 
@@ -275,6 +290,8 @@ int setup_profiling_timer(unsigned int m
 	return 0;
 }
 
+static DEFINE_RAW_SPINLOCK(tlbstate_lock);
+
 static void flush_tlb_all_ipi(void *info)
 {
 	local_flush_tlb_all();
@@ -332,6 +349,7 @@ static inline void smp_on_each_tlb(void 
 void flush_tlb_mm(struct mm_struct *mm)
 {
 	preempt_disable();
+	spin_lock(&tlbstate_lock);
 
 	if ((atomic_read(&mm->mm_users) != 1) || (current->mm != mm)) {
 		smp_on_other_tlbs(flush_tlb_mm_ipi, (void *)mm);
@@ -341,6 +359,7 @@ void flush_tlb_mm(struct mm_struct *mm)
 			if (smp_processor_id() != i)
 				cpu_context(i, mm) = 0;
 	}
+	spin_unlock(&tlbstate_lock);
 	local_flush_tlb_mm(mm);
 
 	preempt_enable();
@@ -364,6 +383,8 @@ void flush_tlb_range(struct vm_area_stru
 	struct mm_struct *mm = vma->vm_mm;
 
 	preempt_disable();
+	spin_lock(&tlbstate_lock);
+
 	if ((atomic_read(&mm->mm_users) != 1) || (current->mm != mm)) {
 		struct flush_tlb_data fd;
 
@@ -377,6 +398,7 @@ void flush_tlb_range(struct vm_area_stru
 			if (smp_processor_id() != i)
 				cpu_context(i, mm) = 0;
 	}
+	spin_unlock(&tlbstate_lock);
 	local_flush_tlb_range(vma, start, end);
 	preempt_enable();
 }
@@ -407,6 +429,8 @@ static void flush_tlb_page_ipi(void *inf
 void flush_tlb_page(struct vm_area_struct *vma, unsigned long page)
 {
 	preempt_disable();
+	spin_lock(&tlbstate_lock);
+
 	if ((atomic_read(&vma->vm_mm->mm_users) != 1) || (current->mm != vma->vm_mm)) {
 		struct flush_tlb_data fd;
 
@@ -419,6 +443,7 @@ void flush_tlb_page(struct vm_area_struc
 			if (smp_processor_id() != i)
 				cpu_context(i, vma->vm_mm) = 0;
 	}
+	spin_unlock(&tlbstate_lock);
 	local_flush_tlb_page(vma, page);
 	preempt_enable();
 }
Index: linux-2.6.22/arch/mips/kernel/time.c
===================================================================
--- linux-2.6.22.orig/arch/mips/kernel/time.c	2007-07-24 08:57:15.000000000 +0200
+++ linux-2.6.22/arch/mips/kernel/time.c	2007-07-24 08:57:25.000000000 +0200
@@ -10,6 +10,11 @@
  * under  the terms of  the GNU General  Public License as published by the
  * Free Software Foundation;  either version 2 of the  License, or (at your
  * option) any later version.
+ *
+ * This implementation of High Res Timers uses two timers. One is the system
+ * timer. The second is used for the high res timers. The high res timers
+ * require the CPU to have count/compare registers. The mips_set_next_event()
+ * function schedules the next high res timer interrupt.
  */
 #include <linux/types.h>
 #include <linux/kernel.h>
@@ -23,6 +28,7 @@
 #include <linux/spinlock.h>
 #include <linux/interrupt.h>
 #include <linux/module.h>
+#include <linux/clockchips.h>
 
 #include <asm/bootinfo.h>
 #include <asm/cache.h>
@@ -47,7 +53,27 @@
 /*
  * forward reference
  */
-DEFINE_SPINLOCK(rtc_lock);
+DEFINE_RAW_SPINLOCK(rtc_lock);
+
+/* any missed timer interrupts */
+int missed_timer_count;
+
+#ifdef CONFIG_HIGH_RES_TIMERS
+static void mips_set_next_event(unsigned long evt);
+static void mips_set_mode(int mode, void *priv);
+
+static struct clock_event lapic_clockevent = {
+	.name = "mips clockevent interface",
+	.capabilities = CLOCK_CAP_NEXTEVT | CLOCK_CAP_PROFILE |
+			CLOCK_HAS_IRQHANDLER
+#ifdef CONFIG_SMP
+			| CLOCK_CAP_UPDATE
+#endif
+	,
+	.shift = 32,
+	.set_next_event = mips_set_next_event,
+};
+#endif
 
 /*
  * By default we provide the null RTC ops
@@ -56,6 +82,129 @@ static unsigned long null_rtc_get_time(v
 {
 	return mktime(2000, 1, 1, 0, 0, 0);
 }
+#ifdef CONFIG_SMP
+/*
+ * We have to synchronize the master CPU with all the slave CPUs
+ */
+static atomic_t cpus_started;
+static atomic_t cpus_ready;
+static atomic_t cpus_count;
+/*
+ * Master processor inits
+ */
+static void sync_cpus_init(int v)
+{
+	atomic_set(&cpus_count, 0);
+	mb();
+	atomic_set(&cpus_started, v);
+	mb();
+	atomic_set(&cpus_ready, v);
+	mb();
+}
+
+/*
+ * Called by the master processor
+ */
+static void sync_cpus_master(int v)
+{
+	atomic_set(&cpus_count, 0);
+	mb();
+	atomic_set(&cpus_started, v);
+	mb();
+	/* Wait here till all other CPUs are now ready */
+	while (atomic_read(&cpus_count) != (num_online_cpus() -1) )
+		mb();
+	atomic_set(&cpus_ready, v);
+	mb();
+}
+/*
+ * Called by the slave processors
+ */
+static void sync_cpus_slave(int v)
+{
+        /* Check if the master has been through this */
+        while (atomic_read(&cpus_started) != v)
+                mb();
+        atomic_inc(&cpus_count);
+        mb();
+        while (atomic_read(&cpus_ready) != v)
+                mb();
+}
+/*
+ * Called by the slave CPUs when done syncing the count register
+ * with the master processor
+ */
+static void sync_cpus_slave_exit(int v)
+{
+	while (atomic_read(&cpus_started) != v)
+		mb();
+	atomic_inc(&cpus_count);
+	mb();
+}
+
+#define LOOPS	100
+static u32 c0_count[NR_CPUS];		/* Count register per CPU */
+static u32 c[NR_CPUS][LOOPS + 1];	/* Count register per CPU per loop for syncing */
+
+/*
+ * Slave processors execute this via IPI
+ */
+static void sync_c0_count_slave(void *info)
+{
+	int cpus = 1, loop, prev_count = 0, cpu = smp_processor_id();
+	unsigned long flags;
+	u32 diff_count; /* CPU count registers are 32-bit */
+	local_irq_save(flags);
+
+	for(loop = 0; loop <= LOOPS; loop++) {
+		/* Sync with the Master processor */
+		sync_cpus_slave(cpus++);
+		c[cpu][loop] = c0_count[cpu] = read_c0_count();
+		mb();
+		sync_cpus_slave(cpus++);
+		diff_count = c0_count[0] - c0_count[cpu];
+		diff_count += prev_count;
+		diff_count += read_c0_count();
+		write_c0_count(diff_count);
+		prev_count = (prev_count >> 1) +
+			((int)(c0_count[0] - c0_count[cpu]) >> 1);
+        }
+
+	/* Slave processor is done syncing count register with Master */
+	sync_cpus_slave_exit(cpus++);
+	printk("SMP: Slave processor %d done syncing count \n", cpu);
+	local_irq_restore(flags);
+}
+
+/*
+ * Master kicks off the syncing process
+ */
+void sync_c0_count_master(void)
+{
+	int cpus = 0, loop, cpu = smp_processor_id();
+	unsigned long flags;
+
+	printk("SMP: Starting to sync the c0 count register ... \n");
+	sync_cpus_init(cpus++);
+
+	/* Kick off the slave processors to also start the syncing process */
+	smp_call_function(sync_c0_count_slave, NULL, 0, 0);
+	local_irq_save(flags);
+
+	for (loop = 0; loop <= LOOPS; loop++) {
+		/* Wait for all the CPUs here */
+		sync_cpus_master(cpus++);
+		c[cpu][loop] = c0_count[cpu] = read_c0_count();
+		mb();
+		/* Do syncing once more */
+		sync_cpus_master(cpus++);
+	}
+	sync_cpus_master(cpus++);
+	local_irq_restore(flags);
+
+	printk("SMP: Syncing process completed accross CPUs ... \n");
+}
+#endif /* CONFIG_SMP */
 
 static int null_rtc_set_time(unsigned long sec)
 {
@@ -66,19 +215,30 @@ unsigned long (*rtc_mips_get_time)(void)
 int (*rtc_mips_set_time)(unsigned long) = null_rtc_set_time;
 int (*rtc_mips_set_mmss)(unsigned long);
 
-
 /* how many counter cycles in a jiffy */
 static unsigned long cycles_per_jiffy __read_mostly;
 
+static unsigned long hrt_cycles_per_jiffy __read_mostly;
+
+
 /* expirelo is the count value for next CPU timer interrupt */
 static unsigned int expirelo;
 
-
 /*
  * Null timer ack for systems not needing one (e.g. i8254).
  */
 static void null_timer_ack(void) { /* nothing */ }
 
+#ifdef CONFIG_HIGH_RES_TIMERS
+/*
+ * Set the next event
+ */
+static void mips_set_next_event(unsigned long evt)
+{
+	write_c0_compare(read_c0_count() + evt);
+}
+#endif
+
 /*
  * Null high precision timer functions for systems lacking one.
  */
@@ -95,13 +255,13 @@ static void c0_timer_ack(void)
 	unsigned int count;
 
 	/* Ack this timer interrupt and set the next one.  */
-	expirelo += cycles_per_jiffy;
+	expirelo += hrt_cycles_per_jiffy;
 	write_c0_compare(expirelo);
-
 	/* Check to see if we have missed any timer interrupts.  */
-	while (((count = read_c0_count()) - expirelo) < 0x7fffffff) {
-		/* missed_timer_count++; */
-		expirelo = count + cycles_per_jiffy;
+	count = read_c0_count();
+	if ((count - expirelo) < 0x7fffffff) {
+		/* missed_timer_count++;  */
+		expirelo = count + hrt_cycles_per_jiffy;
 		write_c0_compare(expirelo);
 	}
 }
@@ -160,7 +320,7 @@ irqreturn_t timer_interrupt(int irq, voi
 
 	/*
 	 * If we have an externally synchronized Linux clock, then update
-	 * CMOS clock accordingly every ~11 minutes. rtc_mips_set_time() has to be
+	 * CMOS clock accordingly every ~11 minutes. rtc_set_time() has to be
 	 * called as close as possible to 500 ms before the new second starts.
 	 */
 	if (ntp_synced() &&
@@ -228,6 +388,15 @@ static inline int handle_perf_irq (int r
 		!r2;
 }
 
+#ifdef CONFIG_HIGH_RES_TIMERS
+void event_timer_handler(struct pt_regs *regs)
+{
+	c0_timer_ack();
+	if (lapic_clockevent.event_handler)
+		lapic_clockevent.event_handler(regs,NULL);
+}
+#endif
+
 asmlinkage void ll_timer_interrupt(int irq)
 {
 	int r2 = cpu_has_mips_r2;
@@ -235,6 +404,16 @@ asmlinkage void ll_timer_interrupt(int i
 	irq_enter();
 	kstat_this_cpu.irqs[irq]++;
 
+
+#ifdef CONFIG_HIGH_RES_TIMERS
+	/*
+	 * Run the event handler
+	 */
+	if (!r2 || (read_c0_cause() & (1 << 26)))
+		if (lapic_clockevent.event_handler)
+			lapic_clockevent.event_handler(regs,NULL);
+#endif
+
 	if (handle_perf_irq(r2))
 		goto out;
 
@@ -267,7 +446,7 @@ asmlinkage void ll_local_timer_interrupt
  *      b) (optional) calibrate and set the mips_hpt_frequency
  *	    (only needed if you intended to use cpu counter as timer interrupt
  *	     source)
- * 2) setup xtime based on rtc_mips_get_time().
+ * 2) setup xtime based on rtc_get_time().
  * 3) calculate a couple of cached variables for later usage
  * 4) plat_timer_setup() -
  *	a) (optional) over-write any choices made above by time_init().
@@ -358,6 +537,9 @@ static void __init init_mips_clocksource
 
 void __init time_init(void)
 {
+#ifdef CONFIG_HIGH_RES_TIMERS
+	u64 temp;
+#endif
 	if (board_time_init)
 		board_time_init();
 
@@ -401,6 +583,12 @@ void __init time_init(void)
 		if (!mips_hpt_frequency)
 			mips_hpt_frequency = calibrate_hpt();
 
+#ifdef CONFIG_HIGH_RES_TIMERS
+		hrt_cycles_per_jiffy = ( (CONFIG_CPU_SPEED * 1000000) + HZ / 2) / HZ;
+#else
+		hrt_cycles_per_jiffy = cycles_per_jiffy;
+#endif
+
 		/* Report the high precision timer rate for a reference.  */
 		printk("Using %u.%03u MHz high precision timer.\n",
 		       ((mips_hpt_frequency + 500) / 1000) / 1000,
Index: linux-2.6.22/arch/mips/kernel/traps.c
===================================================================
--- linux-2.6.22.orig/arch/mips/kernel/traps.c	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/arch/mips/kernel/traps.c	2007-07-24 08:57:25.000000000 +0200
@@ -309,7 +309,7 @@ void show_registers(struct pt_regs *regs
 	printk("\n");
 }
 
-static DEFINE_SPINLOCK(die_lock);
+static DEFINE_RAW_SPINLOCK(die_lock);
 
 NORET_TYPE void ATTRIB_NORET die(const char * str, struct pt_regs * regs)
 {
Index: linux-2.6.22/arch/mips/mm/init.c
===================================================================
--- linux-2.6.22.orig/arch/mips/mm/init.c	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/arch/mips/mm/init.c	2007-07-24 08:57:25.000000000 +0200
@@ -59,7 +59,7 @@
 
 #endif /* CONFIG_MIPS_MT_SMTC */
 
-DEFINE_PER_CPU(struct mmu_gather, mmu_gathers);
+DEFINE_PER_CPU_LOCKED(struct mmu_gather, mmu_gathers);
 
 /*
  * We have up to 8 empty zeroed pages so we can map one of the right colour
Index: linux-2.6.22/arch/mips/sibyte/cfe/smp.c
===================================================================
--- linux-2.6.22.orig/arch/mips/sibyte/cfe/smp.c	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/arch/mips/sibyte/cfe/smp.c	2007-07-24 08:57:25.000000000 +0200
@@ -107,4 +107,8 @@ void prom_smp_finish(void)
  */
 void prom_cpus_done(void)
 {
+#ifdef CONFIG_HIGH_RES_TIMERS
+	extern void sync_c0_count_master(void);
+	sync_c0_count_master();
+#endif
 }
Index: linux-2.6.22/arch/mips/sibyte/sb1250/irq.c
===================================================================
--- linux-2.6.22.orig/arch/mips/sibyte/sb1250/irq.c	2007-07-24 08:57:15.000000000 +0200
+++ linux-2.6.22/arch/mips/sibyte/sb1250/irq.c	2007-07-24 08:57:25.000000000 +0200
@@ -81,7 +81,7 @@ static struct irq_chip sb1250_irq_type =
 /* Store the CPU id (not the logical number) */
 int sb1250_irq_owner[SB1250_NR_IRQS];
 
-DEFINE_SPINLOCK(sb1250_imr_lock);
+DEFINE_RAW_SPINLOCK(sb1250_imr_lock);
 
 void sb1250_mask_irq(int cpu, int irq)
 {
@@ -352,6 +352,10 @@ void __init arch_init_irq(void)
 #ifdef CONFIG_KGDB
 	imask |= STATUSF_IP6;
 #endif
+
+#ifdef CONFIG_HIGH_RES_TIMERS
+	imask |= STATUSF_IP7;
+#endif
 	/* Enable necessary IPs, disable the rest */
 	change_c0_status(ST0_IM, imask);
 
@@ -429,6 +433,10 @@ asmlinkage void plat_irq_dispatch(void)
 	else
 #endif
 
+#ifdef CONFIG_HIGH_RES_TIMERS
+	if (pending & CAUSEF_IP7)
+		event_timer_handler(regs);
+#endif
 	if (pending & CAUSEF_IP4)
 		sb1250_timer_interrupt();
 
Index: linux-2.6.22/arch/mips/sibyte/sb1250/smp.c
===================================================================
--- linux-2.6.22.orig/arch/mips/sibyte/sb1250/smp.c	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/arch/mips/sibyte/sb1250/smp.c	2007-07-24 08:57:25.000000000 +0200
@@ -59,7 +59,7 @@ void sb1250_smp_finish(void)
 {
 	extern void sb1250_time_init(void);
 	sb1250_time_init();
-	local_irq_enable();
+	raw_local_irq_enable();
 }
 
 /*
Index: linux-2.6.22/arch/mips/sibyte/swarm/setup.c
===================================================================
--- linux-2.6.22.orig/arch/mips/sibyte/swarm/setup.c	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/arch/mips/sibyte/swarm/setup.c	2007-07-24 08:57:25.000000000 +0200
@@ -131,6 +131,12 @@ void __init plat_mem_setup(void)
 		rtc_mips_set_time = m41t81_set_time;
 	}
 
+#ifdef CONFIG_HIGH_RES_TIMERS
+	/*
+	 * set the mips_hpt_frequency here
+	 */
+	mips_hpt_frequency = CONFIG_CPU_SPEED * 1000000;
+#endif
 	printk("This kernel optimized for "
 #ifdef CONFIG_SIMULATION
 	       "simulation"
Index: linux-2.6.22/include/asm-mips/asmmacro.h
===================================================================
--- linux-2.6.22.orig/include/asm-mips/asmmacro.h	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/include/asm-mips/asmmacro.h	2007-07-24 08:57:25.000000000 +0200
@@ -21,7 +21,7 @@
 #endif
 
 #ifdef CONFIG_MIPS_MT_SMTC
-	.macro	local_irq_enable reg=t0
+	.macro	raw_local_irq_enable reg=t0
 	mfc0	\reg, CP0_TCSTATUS
 	ori	\reg, \reg, TCSTATUS_IXMT
 	xori	\reg, \reg, TCSTATUS_IXMT
@@ -29,21 +29,21 @@
 	_ehb
 	.endm
 
-	.macro	local_irq_disable reg=t0
+	.macro	raw_local_irq_disable reg=t0
 	mfc0	\reg, CP0_TCSTATUS
 	ori	\reg, \reg, TCSTATUS_IXMT
 	mtc0	\reg, CP0_TCSTATUS
 	_ehb
 	.endm
 #else
-	.macro	local_irq_enable reg=t0
+	.macro	raw_local_irq_enable reg=t0
 	mfc0	\reg, CP0_STATUS
 	ori	\reg, \reg, 1
 	mtc0	\reg, CP0_STATUS
 	irq_enable_hazard
 	.endm
 
-	.macro	local_irq_disable reg=t0
+	.macro	raw_local_irq_disable reg=t0
 	mfc0	\reg, CP0_STATUS
 	ori	\reg, \reg, 1
 	xori	\reg, \reg, 1
Index: linux-2.6.22/include/asm-mips/atomic.h
===================================================================
--- linux-2.6.22.orig/include/asm-mips/atomic.h	2007-07-24 08:57:19.000000000 +0200
+++ linux-2.6.22/include/asm-mips/atomic.h	2007-07-24 08:57:25.000000000 +0200
@@ -573,7 +573,6 @@ static __inline__ long atomic64_add_retu
 		raw_local_irq_restore(flags);
 	}
 #endif
-#endif
 
 	smp_mb();
 
Index: linux-2.6.22/include/asm-mips/bitops.h
===================================================================
--- linux-2.6.22.orig/include/asm-mips/bitops.h	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/include/asm-mips/bitops.h	2007-07-24 08:57:25.000000000 +0200
@@ -500,9 +500,6 @@ static inline unsigned long __ffs(unsign
 }
 
 /*
- * fls - find last bit set.
- * @word: The word to search
- *
  * This is defined the same way as ffs.
  * Note fls(0) = 0, fls(1) = 1, fls(0x80000000) = 32.
  */
@@ -520,6 +517,8 @@ static inline int fls64(__u64 word)
 
 	return 64 - word;
 }
+#define __bi_local_irq_save(x)		raw_local_irq_save(x)
+#define __bi_local_irq_restore(x)	raw_local_irq_restore(x)
 #else
 #include <asm-generic/bitops/fls64.h>
 #endif
Index: linux-2.6.22/include/asm-mips/hw_irq.h
===================================================================
--- linux-2.6.22.orig/include/asm-mips/hw_irq.h	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/include/asm-mips/hw_irq.h	2007-07-24 08:57:25.000000000 +0200
@@ -10,6 +10,7 @@
 
 #include <linux/profile.h>
 #include <asm/atomic.h>
+#include <linux/irqflags.h>
 
 extern void disable_8259A_irq(unsigned int irq);
 extern void enable_8259A_irq(unsigned int irq);
Index: linux-2.6.22/include/asm-mips/i8259.h
===================================================================
--- linux-2.6.22.orig/include/asm-mips/i8259.h	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/include/asm-mips/i8259.h	2007-07-24 08:57:25.000000000 +0200
@@ -35,7 +35,7 @@
 #define SLAVE_ICW4_DEFAULT	0x01
 #define PIC_ICW4_AEOI		2
 
-extern spinlock_t i8259A_lock;
+extern raw_spinlock_t i8259A_lock;
 
 extern void init_8259A(int auto_eoi);
 extern void enable_8259A_irq(unsigned int irq);
Index: linux-2.6.22/include/asm-mips/io.h
===================================================================
--- linux-2.6.22.orig/include/asm-mips/io.h	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/include/asm-mips/io.h	2007-07-24 08:57:25.000000000 +0200
@@ -15,6 +15,7 @@
 #include <linux/compiler.h>
 #include <linux/kernel.h>
 #include <linux/types.h>
+#include <linux/irqflags.h>
 
 #include <asm/addrspace.h>
 #include <asm/byteorder.h>
Index: linux-2.6.22/include/asm-mips/linkage.h
===================================================================
--- linux-2.6.22.orig/include/asm-mips/linkage.h	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/include/asm-mips/linkage.h	2007-07-24 08:57:25.000000000 +0200
@@ -3,6 +3,11 @@
 
 #ifdef __ASSEMBLY__
 #include <asm/asm.h>
+
+/* FASTCALL stuff */
+#define FASTCALL(x)	x
+#define fastcall
+
 #endif
 
 #endif
Index: linux-2.6.22/include/asm-mips/m48t35.h
===================================================================
--- linux-2.6.22.orig/include/asm-mips/m48t35.h	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/include/asm-mips/m48t35.h	2007-07-24 08:57:25.000000000 +0200
@@ -6,7 +6,7 @@
 
 #include <linux/spinlock.h>
 
-extern spinlock_t rtc_lock;
+extern raw_spinlock_t rtc_lock;
 
 struct m48t35_rtc {
 	volatile u8	pad[0x7ff8];    /* starts at 0x7ff8 */
Index: linux-2.6.22/include/asm-mips/mipsregs.h
===================================================================
--- linux-2.6.22.orig/include/asm-mips/mipsregs.h	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/include/asm-mips/mipsregs.h	2007-07-24 08:57:25.000000000 +0200
@@ -705,7 +705,7 @@ do {									\
 	unsigned long long val;						\
 	unsigned long flags;						\
 									\
-	local_irq_save(flags);						\
+	local_irq_save(flags);					\
 	if (sel == 0)							\
 		__asm__ __volatile__(					\
 			".set\tmips64\n\t"				\
@@ -733,7 +733,7 @@ do {									\
 do {									\
 	unsigned long flags;						\
 									\
-	local_irq_save(flags);						\
+	local_irq_save(flags);					\
 	if (sel == 0)							\
 		__asm__ __volatile__(					\
 			".set\tmips64\n\t"				\
Index: linux-2.6.22/include/asm-mips/rwsem.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.22/include/asm-mips/rwsem.h	2007-07-24 08:57:25.000000000 +0200
@@ -0,0 +1,176 @@
+/*
+ * include/asm-mips/rwsem.h: R/W semaphores for MIPS using the stuff
+ * in lib/rwsem.c.  Adapted largely from include/asm-ppc/rwsem.h
+ * by john.cooper@timesys.com
+ */
+
+#ifndef _MIPS_RWSEM_H
+#define _MIPS_RWSEM_H
+
+#ifndef _LINUX_RWSEM_H
+#error "please don't include asm/rwsem.h directly, use linux/rwsem.h instead"
+#endif
+
+#ifdef __KERNEL__
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <asm/atomic.h>
+#include <asm/system.h>
+
+/*
+ * the semaphore definition
+ */
+struct compat_rw_semaphore {
+	/* XXX this should be able to be an atomic_t  -- paulus */
+	signed long		count;
+#define RWSEM_UNLOCKED_VALUE		0x00000000
+#define RWSEM_ACTIVE_BIAS		0x00000001
+#define RWSEM_ACTIVE_MASK		0x0000ffff
+#define RWSEM_WAITING_BIAS		(-0x00010000)
+#define RWSEM_ACTIVE_READ_BIAS		RWSEM_ACTIVE_BIAS
+#define RWSEM_ACTIVE_WRITE_BIAS		(RWSEM_WAITING_BIAS + RWSEM_ACTIVE_BIAS)
+	raw_spinlock_t		wait_lock;
+	struct list_head	wait_list;
+#if RWSEM_DEBUG
+	int			debug;
+#endif
+};
+
+/*
+ * initialisation
+ */
+#if RWSEM_DEBUG
+#define __RWSEM_DEBUG_INIT      , 0
+#else
+#define __RWSEM_DEBUG_INIT	/* */
+#endif
+
+#define __COMPAT_RWSEM_INITIALIZER(name) \
+	{ RWSEM_UNLOCKED_VALUE, SPIN_LOCK_UNLOCKED, \
+	  LIST_HEAD_INIT((name).wait_list) \
+	  __RWSEM_DEBUG_INIT }
+
+#define COMPAT_DECLARE_RWSEM(name)		\
+	struct compat_rw_semaphore name = __COMPAT_RWSEM_INITIALIZER(name)
+
+extern struct compat_rw_semaphore *rwsem_down_read_failed(struct compat_rw_semaphore *sem);
+extern struct compat_rw_semaphore *rwsem_down_write_failed(struct compat_rw_semaphore *sem);
+extern struct compat_rw_semaphore *rwsem_wake(struct compat_rw_semaphore *sem);
+extern struct compat_rw_semaphore *rwsem_downgrade_wake(struct compat_rw_semaphore *sem);
+
+static inline void compat_init_rwsem(struct compat_rw_semaphore *sem)
+{
+	sem->count = RWSEM_UNLOCKED_VALUE;
+	spin_lock_init(&sem->wait_lock);
+	INIT_LIST_HEAD(&sem->wait_list);
+#if RWSEM_DEBUG
+	sem->debug = 0;
+#endif
+}
+
+/*
+ * lock for reading
+ */
+static inline void __down_read(struct compat_rw_semaphore *sem)
+{
+	if (atomic_inc_return((atomic_t *)(&sem->count)) > 0)
+		smp_wmb();
+	else
+		rwsem_down_read_failed(sem);
+}
+
+static inline int __down_read_trylock(struct compat_rw_semaphore *sem)
+{
+	int tmp;
+
+	while ((tmp = sem->count) >= 0) {
+		if (tmp == cmpxchg(&sem->count, tmp,
+				   tmp + RWSEM_ACTIVE_READ_BIAS)) {
+			smp_wmb();
+			return 1;
+		}
+	}
+	return 0;
+}
+
+/*
+ * lock for writing
+ */
+static inline void __down_write(struct compat_rw_semaphore *sem)
+{
+	int tmp;
+
+	tmp = atomic_add_return(RWSEM_ACTIVE_WRITE_BIAS,
+				(atomic_t *)(&sem->count));
+	if (tmp == RWSEM_ACTIVE_WRITE_BIAS)
+		smp_wmb();
+	else
+		rwsem_down_write_failed(sem);
+}
+
+static inline int __down_write_trylock(struct compat_rw_semaphore *sem)
+{
+	int tmp;
+
+	tmp = cmpxchg(&sem->count, RWSEM_UNLOCKED_VALUE,
+		      RWSEM_ACTIVE_WRITE_BIAS);
+	smp_wmb();
+	return tmp == RWSEM_UNLOCKED_VALUE;
+}
+
+/*
+ * unlock after reading
+ */
+static inline void __up_read(struct compat_rw_semaphore *sem)
+{
+	int tmp;
+
+	smp_wmb();
+	tmp = atomic_dec_return((atomic_t *)(&sem->count));
+	if (tmp < -1 && (tmp & RWSEM_ACTIVE_MASK) == 0)
+		rwsem_wake(sem);
+}
+
+/*
+ * unlock after writing
+ */
+static inline void __up_write(struct compat_rw_semaphore *sem)
+{
+	smp_wmb();
+	if (atomic_sub_return(RWSEM_ACTIVE_WRITE_BIAS,
+			      (atomic_t *)(&sem->count)) < 0)
+		rwsem_wake(sem);
+}
+
+/*
+ * implement atomic add functionality
+ */
+static inline void rwsem_atomic_add(int delta, struct compat_rw_semaphore *sem)
+{
+	atomic_add(delta, (atomic_t *)(&sem->count));
+}
+
+/*
+ * downgrade write lock to read lock
+ */
+static inline void __downgrade_write(struct compat_rw_semaphore *sem)
+{
+	int tmp;
+
+	smp_wmb();
+	tmp = atomic_add_return(-RWSEM_WAITING_BIAS, (atomic_t *)(&sem->count));
+	if (tmp < 0)
+		rwsem_downgrade_wake(sem);
+}
+
+/*
+ * implement exchange and add functionality
+ */
+static inline int rwsem_atomic_update(int delta, struct compat_rw_semaphore *sem)
+{
+	smp_mb();
+	return atomic_add_return(delta, (atomic_t *)(&sem->count));
+}
+
+#endif /* __KERNEL__ */
+#endif /* _MIPS_RWSEM_H */
Index: linux-2.6.22/include/asm-mips/semaphore.h
===================================================================
--- linux-2.6.22.orig/include/asm-mips/semaphore.h	2007-07-24 08:57:19.000000000 +0200
+++ linux-2.6.22/include/asm-mips/semaphore.h	2007-07-24 08:57:25.000000000 +0200
@@ -47,39 +47,42 @@ struct compat_semaphore {
 	wait_queue_head_t wait;
 };
 
-#define __SEMAPHORE_INITIALIZER(name, n)				\
+#define __COMPAT_SEMAPHORE_INITIALIZER(name, n)				\
 {									\
 	.count		= ATOMIC_INIT(n),				\
 	.wait		= __WAIT_QUEUE_HEAD_INITIALIZER((name).wait)	\
 }
 
-#define __DECLARE_SEMAPHORE_GENERIC(name, count) \
-	struct semaphore name = __SEMAPHORE_INITIALIZER(name,count)
+#define __COMPAT_MUTEX_INITIALIZER(name) \
+	__COMPAT_SEMAPHORE_INITIALIZER(name, 1)
 
-#define DECLARE_MUTEX(name)		__DECLARE_SEMAPHORE_GENERIC(name, 1)
-#define DECLARE_MUTEX_LOCKED(name)	__DECLARE_SEMAPHORE_GENERIC(name, 0)
+#define __COMPAT_DECLARE_SEMAPHORE_GENERIC(name, count) \
+	struct compat_semaphore name = __COMPAT_SEMAPHORE_INITIALIZER(name,count)
 
-static inline void sema_init (struct semaphore *sem, int val)
+#define COMPAT_DECLARE_MUTEX(name)		__COMPAT_DECLARE_SEMAPHORE_GENERIC(name, 1)
+#define COMPAT_DECLARE_MUTEX_LOCKED(name)	__COMPAT_DECLARE_SEMAPHORE_GENERIC(name, 0)
+
+static inline void compat_sema_init (struct compat_semaphore *sem, int val)
 {
 	atomic_set(&sem->count, val);
 	init_waitqueue_head(&sem->wait);
 }
 
-static inline void init_MUTEX (struct semaphore *sem)
+static inline void compat_init_MUTEX (struct compat_semaphore *sem)
 {
-	sema_init(sem, 1);
+	compat_sema_init(sem, 1);
 }
 
-static inline void init_MUTEX_LOCKED (struct semaphore *sem)
+static inline void compat_init_MUTEX_LOCKED (struct compat_semaphore *sem)
 {
-	sema_init(sem, 0);
+	compat_sema_init(sem, 0);
 }
 
-extern void __down(struct semaphore * sem);
-extern int  __down_interruptible(struct semaphore * sem);
-extern void __up(struct semaphore * sem);
+extern void __compat_down(struct compat_semaphore * sem);
+extern int  __compat_down_interruptible(struct compat_semaphore * sem);
+extern void __compat_up(struct compat_semaphore * sem);
 
-static inline void down(struct semaphore * sem)
+static inline void compat_down(struct compat_semaphore * sem)
 {
 	might_sleep();
 
@@ -112,6 +115,8 @@ static inline void compat_up(struct comp
 		__compat_up(sem);
 }
 
+extern int compat_sem_is_locked(struct compat_semaphore *sem);
+
 #define compat_sema_count(sem) atomic_read(&(sem)->count)
 
 #include <linux/semaphore.h>
Index: linux-2.6.22/include/asm-mips/spinlock.h
===================================================================
--- linux-2.6.22.orig/include/asm-mips/spinlock.h	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/include/asm-mips/spinlock.h	2007-07-24 08:57:25.000000000 +0200
@@ -28,7 +28,7 @@
  * We make no fairness assumptions.  They have a cost.
  */
 
-static inline void __raw_spin_lock(raw_spinlock_t *lock)
+static inline void __raw_spin_lock(__raw_spinlock_t *lock)
 {
 	unsigned int tmp;
 
@@ -70,7 +70,7 @@ static inline void __raw_spin_lock(raw_s
 	smp_mb();
 }
 
-static inline void __raw_spin_unlock(raw_spinlock_t *lock)
+static inline void __raw_spin_unlock(__raw_spinlock_t *lock)
 {
 	smp_mb();
 
@@ -83,7 +83,7 @@ static inline void __raw_spin_unlock(raw
 	: "memory");
 }
 
-static inline unsigned int __raw_spin_trylock(raw_spinlock_t *lock)
+static inline unsigned int __raw_spin_trylock(__raw_spinlock_t *lock)
 {
 	unsigned int temp, res;
 
@@ -144,7 +144,7 @@ static inline unsigned int __raw_spin_tr
  */
 #define __raw_write_can_lock(rw)	(!(rw)->lock)
 
-static inline void __raw_read_lock(raw_rwlock_t *rw)
+static inline void __raw_read_lock(__raw_rwlock_t *rw)
 {
 	unsigned int tmp;
 
@@ -189,7 +189,7 @@ static inline void __raw_read_lock(raw_r
 /* Note the use of sub, not subu which will make the kernel die with an
    overflow exception if we ever try to unlock an rwlock that is already
    unlocked or is being held by a writer.  */
-static inline void __raw_read_unlock(raw_rwlock_t *rw)
+static inline void __raw_read_unlock(__raw_rwlock_t *rw)
 {
 	unsigned int tmp;
 
@@ -223,7 +223,7 @@ static inline void __raw_read_unlock(raw
 	}
 }
 
-static inline void __raw_write_lock(raw_rwlock_t *rw)
+static inline void __raw_write_lock(__raw_rwlock_t *rw)
 {
 	unsigned int tmp;
 
@@ -265,7 +265,7 @@ static inline void __raw_write_lock(raw_
 	smp_mb();
 }
 
-static inline void __raw_write_unlock(raw_rwlock_t *rw)
+static inline void __raw_write_unlock(__raw_rwlock_t *rw)
 {
 	smp_mb();
 
@@ -277,7 +277,7 @@ static inline void __raw_write_unlock(ra
 	: "memory");
 }
 
-static inline int __raw_read_trylock(raw_rwlock_t *rw)
+static inline int __raw_read_trylock(__raw_rwlock_t *rw)
 {
 	unsigned int tmp;
 	int ret;
@@ -321,7 +321,7 @@ static inline int __raw_read_trylock(raw
 	return ret;
 }
 
-static inline int __raw_write_trylock(raw_rwlock_t *rw)
+static inline int __raw_write_trylock(__raw_rwlock_t *rw)
 {
 	unsigned int tmp;
 	int ret;
Index: linux-2.6.22/include/asm-mips/spinlock_types.h
===================================================================
--- linux-2.6.22.orig/include/asm-mips/spinlock_types.h	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/include/asm-mips/spinlock_types.h	2007-07-24 08:57:25.000000000 +0200
@@ -7,13 +7,13 @@
 
 typedef struct {
 	volatile unsigned int lock;
-} raw_spinlock_t;
+} __raw_spinlock_t;
 
 #define __RAW_SPIN_LOCK_UNLOCKED	{ 0 }
 
 typedef struct {
 	volatile unsigned int lock;
-} raw_rwlock_t;
+} __raw_rwlock_t;
 
 #define __RAW_RW_LOCK_UNLOCKED		{ 0 }
 
Index: linux-2.6.22/include/asm-mips/thread_info.h
===================================================================
--- linux-2.6.22.orig/include/asm-mips/thread_info.h	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/include/asm-mips/thread_info.h	2007-07-24 08:57:25.000000000 +0200
@@ -114,6 +114,7 @@ register struct thread_info *__current_t
 #define TIF_NEED_RESCHED	3	/* rescheduling necessary */
 #define TIF_SYSCALL_AUDIT	4	/* syscall auditing active */
 #define TIF_SECCOMP		5	/* secure computing */
+#define TIF_NEED_RESCHED_DELAYED 6	/* reschedule on return to userspace */
 #define TIF_RESTORE_SIGMASK	9	/* restore signal mask in do_signal() */
 #define TIF_USEDFPU		16	/* FPU was used by this task this quantum (SMP) */
 #define TIF_POLLING_NRFLAG	17	/* true if poll_idle() is polling TIF_NEED_RESCHED */
@@ -127,6 +128,7 @@ register struct thread_info *__current_t
 #define _TIF_NEED_RESCHED	(1<<TIF_NEED_RESCHED)
 #define _TIF_SYSCALL_AUDIT	(1<<TIF_SYSCALL_AUDIT)
 #define _TIF_SECCOMP		(1<<TIF_SECCOMP)
+#define _TIF_NEED_RESCHED_DELAYED (1<<TIF_NEED_RESCHED_DELAYED)
 #define _TIF_RESTORE_SIGMASK	(1<<TIF_RESTORE_SIGMASK)
 #define _TIF_USEDFPU		(1<<TIF_USEDFPU)
 #define _TIF_POLLING_NRFLAG	(1<<TIF_POLLING_NRFLAG)
Index: linux-2.6.22/include/asm-mips/time.h
===================================================================
--- linux-2.6.22.orig/include/asm-mips/time.h	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/include/asm-mips/time.h	2007-07-24 08:57:25.000000000 +0200
@@ -23,7 +23,7 @@
 #include <linux/spinlock.h>
 #include <linux/clocksource.h>
 
-extern spinlock_t rtc_lock;
+extern raw_spinlock_t rtc_lock;
 
 /*
  * RTC ops.  By default, they point to no-RTC functions.
Index: linux-2.6.22/include/asm-mips/timeofday.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.22/include/asm-mips/timeofday.h	2007-07-24 08:57:25.000000000 +0200
@@ -0,0 +1,5 @@
+#ifndef _ASM_MIPS_TIMEOFDAY_H
+#define _ASM_MIPS_TIMEOFDAY_H
+#include <asm-generic/timeofday.h>
+#endif
+
Index: linux-2.6.22/include/asm-mips/uaccess.h
===================================================================
--- linux-2.6.22.orig/include/asm-mips/uaccess.h	2007-07-24 08:56:17.000000000 +0200
+++ linux-2.6.22/include/asm-mips/uaccess.h	2007-07-24 08:57:25.000000000 +0200
@@ -427,7 +427,6 @@ extern size_t __copy_user(void *__to, co
 	const void *__cu_from;						\
 	long __cu_len;							\
 									\
-	might_sleep();							\
 	__cu_to = (to);							\
 	__cu_from = (from);						\
 	__cu_len = (n);							\
@@ -483,7 +482,6 @@ extern size_t __copy_user_inatomic(void 
 	const void *__cu_from;						\
 	long __cu_len;							\
 									\
-	might_sleep();							\
 	__cu_to = (to);							\
 	__cu_from = (from);						\
 	__cu_len = (n);							\
@@ -562,7 +560,6 @@ extern size_t __copy_user_inatomic(void 
 	const void __user *__cu_from;					\
 	long __cu_len;							\
 									\
-	might_sleep();							\
 	__cu_to = (to);							\
 	__cu_from = (from);						\
 	__cu_len = (n);							\
@@ -593,7 +590,6 @@ extern size_t __copy_user_inatomic(void 
 	const void __user *__cu_from;					\
 	long __cu_len;							\
 									\
-	might_sleep();							\
 	__cu_to = (to);							\
 	__cu_from = (from);						\
 	__cu_len = (n);							\
@@ -611,7 +607,6 @@ extern size_t __copy_user_inatomic(void 
 	const void __user *__cu_from;					\
 	long __cu_len;							\
 									\
-	might_sleep();							\
 	__cu_to = (to);							\
 	__cu_from = (from);						\
 	__cu_len = (n);							\
@@ -638,7 +633,6 @@ __clear_user(void __user *addr, __kernel
 {
 	__kernel_size_t res;
 
-	might_sleep();
 	__asm__ __volatile__(
 		"move\t$4, %1\n\t"
 		"move\t$5, $0\n\t"
@@ -687,7 +681,6 @@ __strncpy_from_user(char *__to, const ch
 {
 	long res;
 
-	might_sleep();
 	__asm__ __volatile__(
 		"move\t$4, %1\n\t"
 		"move\t$5, %2\n\t"
@@ -724,7 +717,6 @@ strncpy_from_user(char *__to, const char
 {
 	long res;
 
-	might_sleep();
 	__asm__ __volatile__(
 		"move\t$4, %1\n\t"
 		"move\t$5, %2\n\t"
@@ -743,7 +735,6 @@ static inline long __strlen_user(const c
 {
 	long res;
 
-	might_sleep();
 	__asm__ __volatile__(
 		"move\t$4, %1\n\t"
 		__MODULE_JAL(__strlen_user_nocheck_asm)
@@ -773,7 +764,6 @@ static inline long strlen_user(const cha
 {
 	long res;
 
-	might_sleep();
 	__asm__ __volatile__(
 		"move\t$4, %1\n\t"
 		__MODULE_JAL(__strlen_user_asm)
@@ -790,7 +780,6 @@ static inline long __strnlen_user(const 
 {
 	long res;
 
-	might_sleep();
 	__asm__ __volatile__(
 		"move\t$4, %1\n\t"
 		"move\t$5, %2\n\t"
@@ -821,7 +810,6 @@ static inline long strnlen_user(const ch
 {
 	long res;
 
-	might_sleep();
 	__asm__ __volatile__(
 		"move\t$4, %1\n\t"
 		"move\t$5, %2\n\t"
