Subject: Linux-RT 2.6.25.4-RT
From: http://www.kernel.org/pub/linux/kernel/projects/rt/
Acked-by: Sven-Thorsten Dietrich <sdietrich@suse.de>
---
 drivers/acpi/processor_idle.c |    6 +++---
 include/linux/preempt.h       |   16 ++++++++++++++++
 include/linux/sched.h         |   22 +++++++++++++++++++++-
 kernel/sched.c                |    4 +++-
 4 files changed, 43 insertions(+), 5 deletions(-)

Index: linux-2.6.25.4-rt4/drivers/acpi/processor_idle.c
===================================================================
--- linux-2.6.25.4-rt4.orig/drivers/acpi/processor_idle.c	2008-05-29 09:45:51.000000000 -0400
+++ linux-2.6.25.4-rt4/drivers/acpi/processor_idle.c	2008-05-29 09:46:26.000000000 -0400
@@ -216,7 +216,7 @@ static void acpi_safe_halt(void)
 	 * test NEED_RESCHED:
 	 */
 	smp_mb();
-	if (!need_resched()) {
+	if (!need_resched() || !need_resched_delayed()) {
 		safe_halt();
 		local_irq_disable();
 	}
@@ -1485,7 +1485,7 @@ static int acpi_idle_enter_simple(struct
 	 */
 	smp_mb();
 
-	if (unlikely(need_resched())) {
+	if (unlikely(need_resched() || need_resched_delayed())) {
 		current_thread_info()->status |= TS_POLLING;
 		local_irq_enable();
 		return 0;
@@ -1574,7 +1574,7 @@ static int acpi_idle_enter_bm(struct cpu
 	 */
 	smp_mb();
 
-	if (unlikely(need_resched())) {
+	if (unlikely(need_resched() || need_resched_delayed())) {
 		current_thread_info()->status |= TS_POLLING;
 		local_irq_enable();
 		return 0;
Index: linux-2.6.25.4-rt4/include/linux/preempt.h
===================================================================
--- linux-2.6.25.4-rt4.orig/include/linux/preempt.h	2008-05-29 09:46:25.000000000 -0400
+++ linux-2.6.25.4-rt4/include/linux/preempt.h	2008-05-29 09:46:26.000000000 -0400
@@ -55,6 +55,21 @@ do { \
 		preempt_schedule(); \
 } while (0)
 
+
+/*
+ * If the architecture doens't have TIF_NEED_RESCHED_DELAYED
+ * help it out and define it back to TIF_NEED_RESCHED
+ */
+#ifndef TIF_NEED_RESCHED_DELAYED
+# define TIF_NEED_RESCHED_DELAYED TIF_NEED_RESCHED
+#endif
+
+#define preempt_check_resched_delayed() \
+do { \
+	if (unlikely(test_thread_flag(TIF_NEED_RESCHED_DELAYED))) \
+		preempt_schedule(); \
+} while (0)
+
 #define preempt_enable() \
 do { \
 	__preempt_enable_no_resched(); \
@@ -97,6 +112,7 @@ do { \
 #define __preempt_enable_no_resched()	do { } while (0)
 #define preempt_enable()		do { } while (0)
 #define preempt_check_resched()		do { } while (0)
+#define preempt_check_resched_delayed()	do { } while (0)
 
 #define preempt_disable_notrace()		do { } while (0)
 #define preempt_enable_no_resched_notrace()	do { } while (0)
Index: linux-2.6.25.4-rt4/include/linux/sched.h
===================================================================
--- linux-2.6.25.4-rt4.orig/include/linux/sched.h	2008-05-29 09:46:20.000000000 -0400
+++ linux-2.6.25.4-rt4/include/linux/sched.h	2008-05-29 09:46:26.000000000 -0400
@@ -2029,11 +2029,31 @@ static inline int fatal_signal_pending(s
 	return signal_pending(p) && __fatal_signal_pending(p);
 }
 
-static inline int need_resched(void)
+static inline int _need_resched(void)
 {
 	return unlikely(test_thread_flag(TIF_NEED_RESCHED));
 }
 
+static inline int need_resched(void)
+{
+	return _need_resched();
+}
+
+static inline void set_tsk_need_resched_delayed(struct task_struct *tsk)
+{
+	set_tsk_thread_flag(tsk,TIF_NEED_RESCHED_DELAYED);
+}
+
+static inline void clear_tsk_need_resched_delayed(struct task_struct *tsk)
+{
+	clear_tsk_thread_flag(tsk,TIF_NEED_RESCHED_DELAYED);
+}
+
+static inline int need_resched_delayed(void)
+{
+	return unlikely(test_thread_flag(TIF_NEED_RESCHED_DELAYED));
+}
+
 /*
  * cond_resched() and cond_resched_lock(): latency reduction via
  * explicit rescheduling in places that are safe. The return
Index: linux-2.6.25.4-rt4/kernel/sched.c
===================================================================
--- linux-2.6.25.4-rt4.orig/kernel/sched.c	2008-05-29 09:46:25.000000000 -0400
+++ linux-2.6.25.4-rt4/kernel/sched.c	2008-05-29 09:46:26.000000000 -0400
@@ -3960,6 +3960,7 @@ need_resched_nonpreemptible:
 	update_rq_clock(rq);
 	spin_lock(&rq->lock);
 	clear_tsk_need_resched(prev);
+	clear_tsk_need_resched_delayed(prev);
 
 	if (prev->state && !(preempt_count() & PREEMPT_ACTIVE)) {
 		if (unlikely((prev->state & TASK_INTERRUPTIBLE) &&
@@ -4005,7 +4006,8 @@ need_resched_nonpreemptible:
 		goto need_resched_nonpreemptible;
 
 	__preempt_enable_no_resched();
-	if (unlikely(test_thread_flag(TIF_NEED_RESCHED)))
+	if (unlikely(test_thread_flag(TIF_NEED_RESCHED) ||
+		     test_thread_flag(TIF_NEED_RESCHED_DELAYED)))
 		goto need_resched;
 }
 EXPORT_SYMBOL(schedule);
