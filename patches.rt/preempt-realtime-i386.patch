Subject: Linux-RT 2.6.24-rc5-rt1
From: http://www.kernel.org/pub/linux/kernel/projects/rt/
Acked-by: Sven-Thorsten Dietrich <sdietrich@suse.de>
---
 arch/x86/Kconfig.debug                     |    2 +
 arch/x86/kernel/apic_32.c                  |    2 -
 arch/x86/kernel/cpu/mtrr/generic.c         |    2 -
 arch/x86/kernel/head_32.S                  |    1 
 arch/x86/kernel/i8253.c                    |    2 -
 arch/x86/kernel/i8259_32.c                 |    2 -
 arch/x86/kernel/io_apic_32.c               |    4 +--
 arch/x86/kernel/irq_32.c                   |    4 ++-
 arch/x86/kernel/microcode.c                |    2 -
 arch/x86/kernel/nmi_32.c                   |    5 +++
 arch/x86/kernel/process_32.c               |   19 ++++++++++----
 arch/x86/kernel/signal_32.c                |   14 ++++++++++
 arch/x86/kernel/smp_32.c                   |   21 +++++++++++-----
 arch/x86/kernel/time_32.c                  |    2 -
 arch/x86/kernel/traps_32.c                 |   23 ++++++++++++++----
 arch/x86/kernel/vm86_32.c                  |    1 
 arch/x86/mm/fault_32.c                     |    7 +++--
 arch/x86/mm/highmem_32.c                   |   37 ++++++++++++++++++++++-------
 arch/x86/mm/pgtable_32.c                   |    2 -
 arch/x86/pci/common.c                      |    2 -
 arch/x86/pci/direct.c                      |   29 ++++++++++++++--------
 arch/x86/pci/pci.h                         |    2 -
 include/asm-x86/acpi_32.h                  |    4 +--
 include/asm-x86/dma_32.h                   |    2 -
 include/asm-x86/highmem.h                  |   27 +++++++++++++++++++++
 include/asm-x86/i8253.h                    |    2 -
 include/asm-x86/i8259.h                    |    2 -
 include/asm-x86/mach-default/irq_vectors.h |    2 -
 include/asm-x86/mc146818rtc_32.h           |    2 -
 include/asm-x86/pgtable_32.h               |    2 -
 include/asm-x86/tlbflush_32.h              |   26 ++++++++++++++++++++
 include/asm-x86/xor_32.h                   |   21 ++++++++++++++--
 kernel/Kconfig.instrumentation             |    5 +++
 33 files changed, 221 insertions(+), 59 deletions(-)

--- linux-2.6.23.orig/arch/x86/Kconfig.debug	2007-12-21 16:58:30.000000000 -0500
+++ linux-2.6.23/arch/x86/Kconfig.debug	2007-12-21 17:02:45.000000000 -0500
@@ -50,6 +50,7 @@ config DEBUG_PAGEALLOC
 config DEBUG_RODATA
 	bool "Write protect kernel read-only data structures"
 	depends on DEBUG_KERNEL
+	default y
 	help
 	  Mark the kernel read-only data as write-protected in the pagetables,
 	  in order to catch accidental (and incorrect) writes to such const
@@ -61,6 +62,7 @@ config 4KSTACKS
 	bool "Use 4Kb for kernel stacks instead of 8Kb"
 	depends on DEBUG_KERNEL
 	depends on X86_32
+	default y
 	help
 	  If you say Y here the kernel will use a 4Kb stacksize for the
 	  kernel stack attached to each process/thread. This facilitates
--- linux-2.6.23.orig/arch/x86/kernel/apic_32.c	2007-12-21 16:58:34.000000000 -0500
+++ linux-2.6.23/arch/x86/kernel/apic_32.c	2007-12-21 17:02:45.000000000 -0500
@@ -577,7 +577,7 @@ static void local_apic_timer_interrupt(v
  *   interrupt as well. Thus we cannot inline the local irq ... ]
  */
 
-void fastcall smp_apic_timer_interrupt(struct pt_regs *regs)
+void fastcall notrace smp_apic_timer_interrupt(struct pt_regs *regs)
 {
 	struct pt_regs *old_regs = set_irq_regs(regs);
 
--- linux-2.6.23.orig/arch/x86/kernel/cpu/mtrr/generic.c	2007-12-21 16:58:21.000000000 -0500
+++ linux-2.6.23/arch/x86/kernel/cpu/mtrr/generic.c	2007-12-21 17:02:45.000000000 -0500
@@ -330,7 +330,7 @@ static unsigned long set_mtrr_state(void
 
 
 static unsigned long cr4 = 0;
-static DEFINE_SPINLOCK(set_atomicity_lock);
+static DEFINE_RAW_SPINLOCK(set_atomicity_lock);
 
 /*
  * Since we are disabling the cache don't allow any interrupts - they
--- linux-2.6.23.orig/arch/x86/kernel/head_32.S	2007-12-21 16:58:21.000000000 -0500
+++ linux-2.6.23/arch/x86/kernel/head_32.S	2007-12-21 17:02:45.000000000 -0500
@@ -533,6 +533,7 @@ ignore_int:
 	call printk
 #endif
 	addl $(5*4),%esp
+	call dump_stack
 	popl %ds
 	popl %es
 	popl %edx
--- linux-2.6.23.orig/arch/x86/kernel/i8253.c	2007-12-21 16:58:21.000000000 -0500
+++ linux-2.6.23/arch/x86/kernel/i8253.c	2007-12-21 17:02:45.000000000 -0500
@@ -14,7 +14,7 @@
 #include <asm/i8253.h>
 #include <asm/io.h>
 
-DEFINE_SPINLOCK(i8253_lock);
+DEFINE_RAW_SPINLOCK(i8253_lock);
 EXPORT_SYMBOL(i8253_lock);
 
 /*
--- linux-2.6.23.orig/arch/x86/kernel/i8259_32.c	2007-12-21 16:58:35.000000000 -0500
+++ linux-2.6.23/arch/x86/kernel/i8259_32.c	2007-12-21 17:02:45.000000000 -0500
@@ -33,7 +33,7 @@
  */
 
 static int i8259A_auto_eoi;
-DEFINE_SPINLOCK(i8259A_lock);
+DEFINE_RAW_SPINLOCK(i8259A_lock);
 static void mask_and_ack_8259A(unsigned int);
 
 static struct irq_chip i8259A_chip = {
--- linux-2.6.23.orig/arch/x86/kernel/io_apic_32.c	2007-12-21 16:58:35.000000000 -0500
+++ linux-2.6.23/arch/x86/kernel/io_apic_32.c	2007-12-21 17:02:45.000000000 -0500
@@ -60,8 +60,8 @@ atomic_t irq_mis_count;
 /* Where if anywhere is the i8259 connect in external int mode */
 static struct { int pin, apic; } ioapic_i8259 = { -1, -1 };
 
-static DEFINE_SPINLOCK(ioapic_lock);
-static DEFINE_SPINLOCK(vector_lock);
+static DEFINE_RAW_SPINLOCK(ioapic_lock);
+static DEFINE_RAW_SPINLOCK(vector_lock);
 
 int timer_over_8254 __initdata = 1;
 
--- linux-2.6.23.orig/arch/x86/kernel/irq_32.c	2007-12-21 16:58:34.000000000 -0500
+++ linux-2.6.23/arch/x86/kernel/irq_32.c	2007-12-21 17:02:45.000000000 -0500
@@ -77,6 +77,8 @@ fastcall notrace unsigned int do_IRQ(str
 	u32 *isp;
 #endif
 
+	irq_show_regs_callback(smp_processor_id(), regs);
+
 	if (unlikely((unsigned)irq >= NR_IRQS)) {
 		printk(KERN_EMERG "%s: cannot handle IRQ %d\n",
 					__FUNCTION__, irq);
@@ -98,7 +100,7 @@ fastcall notrace unsigned int do_IRQ(str
 		__asm__ __volatile__("andl %%esp,%0" :
 					"=r" (esp) : "0" (THREAD_SIZE - 1));
 		if (unlikely(esp < (sizeof(struct thread_info) + STACK_WARN))) {
-			printk("do_IRQ: stack overflow: %ld\n",
+			printk("BUG: do_IRQ: stack overflow: %ld\n",
 				esp - sizeof(struct thread_info));
 			dump_stack();
 		}
--- linux-2.6.23.orig/arch/x86/kernel/microcode.c	2007-12-21 16:58:21.000000000 -0500
+++ linux-2.6.23/arch/x86/kernel/microcode.c	2007-12-21 17:02:45.000000000 -0500
@@ -117,7 +117,7 @@ MODULE_LICENSE("GPL");
 #define exttable_size(et) ((et)->count * EXT_SIGNATURE_SIZE + EXT_HEADER_SIZE)
 
 /* serialize access to the physical write to MSR 0x79 */
-static DEFINE_SPINLOCK(microcode_update_lock);
+static DEFINE_RAW_SPINLOCK(microcode_update_lock);
 
 /* no concurrent ->write()s are allowed on /dev/cpu/microcode */
 static DEFINE_MUTEX(microcode_mutex);
--- linux-2.6.23.orig/arch/x86/kernel/nmi_32.c	2007-12-21 16:58:34.000000000 -0500
+++ linux-2.6.23/arch/x86/kernel/nmi_32.c	2007-12-21 17:02:45.000000000 -0500
@@ -61,7 +61,12 @@ static int endflag __initdata = 0;
 static __init void nmi_cpu_busy(void *data)
 {
 #ifdef CONFIG_SMP
+	/*
+	 * avoid a warning, on PREEMPT_RT this wont run in hardirq context:
+	 */
+#ifndef CONFIG_PREEMPT_RT
 	local_irq_enable_in_hardirq();
+#endif
 	/* Intentionally don't use cpu_relax here. This is
 	   to make sure that the performance counter really ticks,
 	   even if there is a simulator or similar that catches the
--- linux-2.6.23.orig/arch/x86/kernel/process_32.c	2007-12-21 17:01:40.000000000 -0500
+++ linux-2.6.23/arch/x86/kernel/process_32.c	2007-12-21 17:02:45.000000000 -0500
@@ -330,9 +330,10 @@ void __show_registers(struct pt_regs *re
 		regs->eax, regs->ebx, regs->ecx, regs->edx);
 	printk("ESI: %08lx EDI: %08lx EBP: %08lx ESP: %08lx\n",
 		regs->esi, regs->edi, regs->ebp, esp);
-	printk(" DS: %04x ES: %04x FS: %04x GS: %04x SS: %04x\n",
+	printk(" DS: %04x ES: %04x FS: %04x GS: %04x SS: %04x"
+	       " preempt:%08x\n",
 	       regs->xds & 0xffff, regs->xes & 0xffff,
-	       regs->xfs & 0xffff, gs, ss);
+	       regs->xfs & 0xffff, gs, ss, preempt_count());
 
 	if (!all)
 		return;
@@ -404,15 +405,23 @@ void exit_thread(void)
 	if (unlikely(test_thread_flag(TIF_IO_BITMAP))) {
 		struct task_struct *tsk = current;
 		struct thread_struct *t = &tsk->thread;
-		int cpu = get_cpu();
-		struct tss_struct *tss = &per_cpu(init_tss, cpu);
+		void *io_bitmap_ptr = t->io_bitmap_ptr;
+		int cpu;
+		struct tss_struct *tss;
 
-		kfree(t->io_bitmap_ptr);
+		/*
+		 * On PREEMPT_RT we must not call kfree() with
+		 * preemption disabled, so we first zap the pointer:
+		 */
 		t->io_bitmap_ptr = NULL;
+		kfree(io_bitmap_ptr);
+
 		clear_thread_flag(TIF_IO_BITMAP);
 		/*
 		 * Careful, clear this in the TSS too:
 		 */
+		cpu = get_cpu();
+		tss = &per_cpu(init_tss, cpu);
 		memset(tss->io_bitmap, 0xff, tss->io_bitmap_max);
 		t->io_bitmap_max = 0;
 		tss->io_bitmap_owner = NULL;
--- linux-2.6.23.orig/arch/x86/kernel/signal_32.c	2007-12-21 16:58:21.000000000 -0500
+++ linux-2.6.23/arch/x86/kernel/signal_32.c	2007-12-21 17:02:45.000000000 -0500
@@ -536,6 +536,13 @@ handle_signal(unsigned long sig, siginfo
 		}
 	}
 
+#ifdef CONFIG_PREEMPT_RT
+	/*
+	 * Fully-preemptible kernel does not need interrupts disabled:
+	 */
+	local_irq_enable();
+	preempt_check_resched();
+#endif
 	/*
 	 * If TF is set due to a debugger (PT_DTRACE), clear the TF flag so
 	 * that register information in the sigcontext is correct.
@@ -576,6 +583,13 @@ static void fastcall do_signal(struct pt
 	struct k_sigaction ka;
 	sigset_t *oldset;
 
+#ifdef CONFIG_PREEMPT_RT
+	/*
+	 * Fully-preemptible kernel does not need interrupts disabled:
+	 */
+	local_irq_enable();
+	preempt_check_resched();
+#endif
 	/*
 	 * We want the common case to go fast, which
 	 * is why we may in certain cases get here from
--- linux-2.6.23.orig/arch/x86/kernel/smp_32.c	2007-12-21 16:58:34.000000000 -0500
+++ linux-2.6.23/arch/x86/kernel/smp_32.c	2007-12-21 17:02:45.000000000 -0500
@@ -247,7 +247,7 @@ void send_IPI_mask_sequence(cpumask_t ma
 static cpumask_t flush_cpumask;
 static struct mm_struct * flush_mm;
 static unsigned long flush_va;
-static DEFINE_SPINLOCK(tlbstate_lock);
+static DEFINE_RAW_SPINLOCK(tlbstate_lock);
 
 /*
  * We cannot call mmdrop() because we are in interrupt context,
@@ -478,10 +478,20 @@ static void native_smp_send_reschedule(i
 }
 
 /*
+ * this function sends a 'reschedule' IPI to all other CPUs.
+ * This is used when RT tasks are starving and other CPUs
+ * might be able to run them:
+ */
+void smp_send_reschedule_allbutself(void)
+{
+	send_IPI_allbutself(RESCHEDULE_VECTOR);
+}
+
+/*
  * Structure and data for smp_call_function(). This is designed to minimise
  * static memory requirements. It also looks cleaner.
  */
-static DEFINE_SPINLOCK(call_lock);
+static DEFINE_RAW_SPINLOCK(call_lock);
 
 struct call_data_struct {
 	void (*func) (void *info);
@@ -636,11 +646,10 @@ static void native_smp_send_stop(void)
 }
 
 /*
- * Reschedule call back. Nothing to do,
- * all the work is done automatically when
- * we return from the interrupt.
+ * Reschedule call back. Trigger a reschedule pass so that
+ * RT-overload balancing can pass tasks around.
  */
-fastcall void smp_reschedule_interrupt(struct pt_regs *regs)
+fastcall notrace void smp_reschedule_interrupt(struct pt_regs *regs)
 {
 	trace_special(regs->eip, 0, 0);
 	ack_APIC_irq();
--- linux-2.6.23.orig/arch/x86/kernel/time_32.c	2007-12-21 16:58:21.000000000 -0500
+++ linux-2.6.23/arch/x86/kernel/time_32.c	2007-12-21 17:02:45.000000000 -0500
@@ -122,7 +122,7 @@ static int set_rtc_mmss(unsigned long no
 
 int timer_ack;
 
-unsigned long profile_pc(struct pt_regs *regs)
+unsigned long notrace profile_pc(struct pt_regs *regs)
 {
 	unsigned long pc = instruction_pointer(regs);
 
--- linux-2.6.23.orig/arch/x86/kernel/traps_32.c	2007-12-21 16:58:34.000000000 -0500
+++ linux-2.6.23/arch/x86/kernel/traps_32.c	2007-12-21 17:02:45.000000000 -0500
@@ -299,6 +299,12 @@ void dump_stack(void)
 
 EXPORT_SYMBOL(dump_stack);
 
+#if defined(CONFIG_DEBUG_STACKOVERFLOW) && defined(CONFIG_EVENT_TRACE)
+extern unsigned long worst_stack_left;
+#else
+# define worst_stack_left -1L
+#endif
+
 void show_registers(struct pt_regs *regs)
 {
 	int i;
@@ -368,7 +374,7 @@ void die(const char * str, struct pt_reg
 		u32 lock_owner;
 		int lock_owner_depth;
 	} die = {
-		.lock =			__RAW_SPIN_LOCK_UNLOCKED,
+		.lock =			RAW_SPIN_LOCK_UNLOCKED(die.lock),
 		.lock_owner =		-1,
 		.lock_owner_depth =	0
 	};
@@ -380,7 +386,7 @@ void die(const char * str, struct pt_reg
 	if (die.lock_owner != raw_smp_processor_id()) {
 		console_verbose();
 		raw_local_irq_save(flags);
-		__raw_spin_lock(&die.lock);
+		spin_lock(&die.lock);
 		die.lock_owner = smp_processor_id();
 		die.lock_owner_depth = 0;
 		bust_spinlocks(1);
@@ -430,7 +436,7 @@ void die(const char * str, struct pt_reg
 	bust_spinlocks(0);
 	die.lock_owner = -1;
 	add_taint(TAINT_DIE);
-	__raw_spin_unlock(&die.lock);
+	spin_unlock(&die.lock);
 	raw_local_irq_restore(flags);
 #ifdef CONFIG_KDB
 	kdb_diemsg = str;
@@ -474,6 +480,11 @@ static void __kprobes do_trap(int trapnr
 	if (!user_mode(regs))
 		goto kernel_trap;
 
+#ifdef CONFIG_PREEMPT_RT
+	local_irq_enable();
+	preempt_check_resched();
+#endif
+
 	trap_signal: {
 		/*
 		 * We want error_code and trap_no set for userspace faults and
@@ -736,10 +747,11 @@ void __kprobes die_nmi(struct pt_regs *r
 		crash_kexec(regs);
 	}
 
+	nmi_exit();
 	do_exit(SIGSEGV);
 }
 
-static __kprobes void default_do_nmi(struct pt_regs * regs)
+static notrace __kprobes void default_do_nmi(struct pt_regs * regs)
 {
 	unsigned char reason = 0;
 
@@ -789,11 +801,12 @@ static __kprobes void default_do_nmi(str
 
 static int ignore_nmis;
 
-fastcall __kprobes void do_nmi(struct pt_regs * regs, long error_code)
+fastcall notrace __kprobes void do_nmi(struct pt_regs * regs, long error_code)
 {
 	int cpu;
 
 	nmi_enter();
+	nmi_trace((unsigned long)do_nmi, regs->eip, regs->eflags);
 
 	cpu = smp_processor_id();
 
--- linux-2.6.23.orig/arch/x86/kernel/vm86_32.c	2007-12-21 16:58:21.000000000 -0500
+++ linux-2.6.23/arch/x86/kernel/vm86_32.c	2007-12-21 17:02:45.000000000 -0500
@@ -135,6 +135,7 @@ struct pt_regs * fastcall save_v86_state
 	local_irq_enable();
 
 	if (!current->thread.vm86_info) {
+		local_irq_disable();
 		printk("no vm86_info: BAD\n");
 		do_exit(SIGSEGV);
 	}
--- linux-2.6.23.orig/arch/x86/mm/fault_32.c	2007-12-21 16:58:34.000000000 -0500
+++ linux-2.6.23/arch/x86/mm/fault_32.c	2007-12-21 17:02:45.000000000 -0500
@@ -293,8 +293,8 @@ int show_unhandled_signals = 1;
  *	bit 3 == 1 means use of reserved bit detected
  *	bit 4 == 1 means fault was an instruction fetch
  */
-fastcall void __kprobes do_page_fault(struct pt_regs *regs,
-				      unsigned long error_code)
+fastcall notrace void __kprobes do_page_fault(struct pt_regs *regs,
+					      unsigned long error_code)
 {
 	struct task_struct *tsk;
 	struct mm_struct *mm;
@@ -310,6 +310,7 @@ fastcall void __kprobes do_page_fault(st
 
 	/* get the address */
         address = read_cr2();
+	trace_special(regs->eip, error_code, address);
 
 	tsk = current;
 
@@ -499,6 +500,8 @@ bad_area_nosemaphore:
 
 		if (nr == 6) {
 		stop_trace();
+		user_trace_stop();
+		zap_rt_locks();
 			do_invalid_op(regs, 0);
 			return;
 		}
--- linux-2.6.23.orig/arch/x86/mm/highmem_32.c	2007-12-21 16:58:34.000000000 -0500
+++ linux-2.6.23/arch/x86/mm/highmem_32.c	2007-12-21 17:02:45.000000000 -0500
@@ -18,6 +18,26 @@ void kunmap(struct page *page)
 	kunmap_high(page);
 }
 
+void kunmap_virt(void *ptr)
+{
+	struct page *page;
+
+	if ((unsigned long)ptr < PKMAP_ADDR(0))
+		return;
+	page = pte_page(pkmap_page_table[PKMAP_NR((unsigned long)ptr)]);
+	kunmap(page);
+}
+
+struct page *kmap_to_page(void *ptr)
+{
+	struct page *page;
+
+	if ((unsigned long)ptr < PKMAP_ADDR(0))
+		return virt_to_page(ptr);
+	page = pte_page(pkmap_page_table[PKMAP_NR((unsigned long)ptr)]);
+	return page;
+}
+
 /*
  * kmap_atomic/kunmap_atomic is significantly faster than kmap/kunmap because
  * no global lock is needed and because the kmap code must perform a global TLB
@@ -26,7 +46,7 @@ void kunmap(struct page *page)
  * However when holding an atomic kmap is is not legal to sleep, so atomic
  * kmaps are appropriate for short, tight code paths only.
  */
-void *kmap_atomic_prot(struct page *page, enum km_type type, pgprot_t prot)
+void *__kmap_atomic_prot(struct page *page, enum km_type type, pgprot_t prot)
 {
 	enum fixed_addresses idx;
 	unsigned long vaddr;
@@ -46,12 +66,12 @@ void *kmap_atomic_prot(struct page *page
 	return (void *)vaddr;
 }
 
-void *kmap_atomic(struct page *page, enum km_type type)
+void *__kmap_atomic(struct page *page, enum km_type type)
 {
 	return kmap_atomic_prot(page, type, kmap_prot);
 }
 
-void kunmap_atomic(void *kvaddr, enum km_type type)
+void __kunmap_atomic(void *kvaddr, enum km_type type)
 {
 	unsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;
 	enum fixed_addresses idx = type + KM_TYPE_NR*smp_processor_id();
@@ -78,7 +98,7 @@ void kunmap_atomic(void *kvaddr, enum km
 /* This is the same as kmap_atomic() but can map memory that doesn't
  * have a struct page associated with it.
  */
-void *kmap_atomic_pfn(unsigned long pfn, enum km_type type)
+void *__kmap_atomic_pfn(unsigned long pfn, enum km_type type)
 {
 	enum fixed_addresses idx;
 	unsigned long vaddr;
@@ -93,7 +113,7 @@ void *kmap_atomic_pfn(unsigned long pfn,
 	return (void*) vaddr;
 }
 
-struct page *kmap_atomic_to_page(void *ptr)
+struct page *__kmap_atomic_to_page(void *ptr)
 {
 	unsigned long idx, vaddr = (unsigned long)ptr;
 	pte_t *pte;
@@ -108,6 +128,7 @@ struct page *kmap_atomic_to_page(void *p
 
 EXPORT_SYMBOL(kmap);
 EXPORT_SYMBOL(kunmap);
-EXPORT_SYMBOL(kmap_atomic);
-EXPORT_SYMBOL(kunmap_atomic);
-EXPORT_SYMBOL(kmap_atomic_to_page);
+EXPORT_SYMBOL(kunmap_virt);
+EXPORT_SYMBOL(__kmap_atomic);
+EXPORT_SYMBOL(__kunmap_atomic);
+EXPORT_SYMBOL(__kmap_atomic_to_page);
--- linux-2.6.23.orig/arch/x86/mm/pgtable_32.c	2007-12-21 16:58:21.000000000 -0500
+++ linux-2.6.23/arch/x86/mm/pgtable_32.c	2007-12-21 17:02:45.000000000 -0500
@@ -210,7 +210,7 @@ void pmd_ctor(struct kmem_cache *cache, 
  * vmalloc faults work because attached pagetables are never freed.
  * -- wli
  */
-DEFINE_SPINLOCK(pgd_lock);
+DEFINE_RAW_SPINLOCK(pgd_lock);
 struct page *pgd_list;
 
 static inline void pgd_list_add(pgd_t *pgd)
--- linux-2.6.23.orig/arch/x86/pci/common.c	2007-12-21 16:58:21.000000000 -0500
+++ linux-2.6.23/arch/x86/pci/common.c	2007-12-21 17:02:45.000000000 -0500
@@ -54,7 +54,7 @@ int pcibios_scanned;
  * This interrupt-safe spinlock protects all accesses to PCI
  * configuration space.
  */
-DEFINE_SPINLOCK(pci_config_lock);
+DEFINE_RAW_SPINLOCK(pci_config_lock);
 
 /*
  * Several buggy motherboards address only 16 devices and mirror
--- linux-2.6.23.orig/arch/x86/pci/direct.c	2007-12-21 16:58:21.000000000 -0500
+++ linux-2.6.23/arch/x86/pci/direct.c	2007-12-21 17:02:45.000000000 -0500
@@ -220,16 +220,23 @@ static int __init pci_check_type1(void)
 	unsigned int tmp;
 	int works = 0;
 
-	local_irq_save(flags);
+	spin_lock_irqsave(&pci_config_lock, flags);
 
 	outb(0x01, 0xCFB);
 	tmp = inl(0xCF8);
 	outl(0x80000000, 0xCF8);
-	if (inl(0xCF8) == 0x80000000 && pci_sanity_check(&pci_direct_conf1)) {
-		works = 1;
+
+	if (inl(0xCF8) == 0x80000000) {
+		spin_unlock_irqrestore(&pci_config_lock, flags);
+
+		if (pci_sanity_check(&pci_direct_conf1))
+			works = 1;
+
+		spin_lock_irqsave(&pci_config_lock, flags);
 	}
 	outl(tmp, 0xCF8);
-	local_irq_restore(flags);
+
+	spin_unlock_irqrestore(&pci_config_lock, flags);
 
 	return works;
 }
@@ -239,17 +246,19 @@ static int __init pci_check_type2(void)
 	unsigned long flags;
 	int works = 0;
 
-	local_irq_save(flags);
+	spin_lock_irqsave(&pci_config_lock, flags);
 
 	outb(0x00, 0xCFB);
 	outb(0x00, 0xCF8);
 	outb(0x00, 0xCFA);
-	if (inb(0xCF8) == 0x00 && inb(0xCFA) == 0x00 &&
-	    pci_sanity_check(&pci_direct_conf2)) {
-		works = 1;
-	}
 
-	local_irq_restore(flags);
+	if (inb(0xCF8) == 0x00 && inb(0xCFA) == 0x00) {
+		spin_unlock_irqrestore(&pci_config_lock, flags);
+
+		if (pci_sanity_check(&pci_direct_conf2))
+			works = 1;
+	} else
+		spin_unlock_irqrestore(&pci_config_lock, flags);
 
 	return works;
 }
--- linux-2.6.23.orig/arch/x86/pci/pci.h	2007-12-21 16:58:21.000000000 -0500
+++ linux-2.6.23/arch/x86/pci/pci.h	2007-12-21 17:02:45.000000000 -0500
@@ -80,7 +80,7 @@ struct irq_routing_table {
 extern unsigned int pcibios_irq_mask;
 
 extern int pcibios_scanned;
-extern spinlock_t pci_config_lock;
+extern raw_spinlock_t pci_config_lock;
 
 extern int (*pcibios_enable_irq)(struct pci_dev *dev);
 extern void (*pcibios_disable_irq)(struct pci_dev *dev);
--- linux-2.6.23.orig/include/asm-x86/acpi_32.h	2007-12-21 16:58:24.000000000 -0500
+++ linux-2.6.23/include/asm-x86/acpi_32.h	2007-12-21 17:02:45.000000000 -0500
@@ -52,8 +52,8 @@
 
 #define ACPI_ASM_MACROS
 #define BREAKPOINT3
-#define ACPI_DISABLE_IRQS() local_irq_disable()
-#define ACPI_ENABLE_IRQS()  local_irq_enable()
+#define ACPI_DISABLE_IRQS() local_irq_disable_nort()
+#define ACPI_ENABLE_IRQS()  local_irq_enable_nort()
 #define ACPI_FLUSH_CPU_CACHE()	wbinvd()
 
 int __acpi_acquire_global_lock(unsigned int *lock);
--- linux-2.6.23.orig/include/asm-x86/dma_32.h	2007-12-21 16:58:24.000000000 -0500
+++ linux-2.6.23/include/asm-x86/dma_32.h	2007-12-21 17:02:45.000000000 -0500
@@ -134,7 +134,7 @@
 #define DMA_AUTOINIT	0x10
 
 
-extern spinlock_t  dma_spin_lock;
+extern spinlock_t dma_spin_lock;
 
 static __inline__ unsigned long claim_dma_lock(void)
 {
--- linux-2.6.23.orig/include/asm-x86/highmem.h	2007-12-21 16:58:24.000000000 -0500
+++ linux-2.6.23/include/asm-x86/highmem.h	2007-12-21 17:02:45.000000000 -0500
@@ -67,6 +67,16 @@ extern void * FASTCALL(kmap_high(struct 
 extern void FASTCALL(kunmap_high(struct page *page));
 
 void *kmap(struct page *page);
+extern void kunmap_virt(void *ptr);
+extern struct page *kmap_to_page(void *ptr);
+void kunmap(struct page *page);
+
+void *__kmap_atomic_prot(struct page *page, enum km_type type, pgprot_t prot);
+void *__kmap_atomic(struct page *page, enum km_type type);
+void __kunmap_atomic(void *kvaddr, enum km_type type);
+void *__kmap_atomic_pfn(unsigned long pfn, enum km_type type);
+struct page *__kmap_atomic_to_page(void *ptr);
+
 void kunmap(struct page *page);
 void *kmap_atomic_prot(struct page *page, enum km_type type, pgprot_t prot);
 void *kmap_atomic(struct page *page, enum km_type type);
@@ -80,6 +90,23 @@ struct page *kmap_atomic_to_page(void *p
 
 #define flush_cache_kmaps()	do { } while (0)
 
+/*
+ * on PREEMPT_RT kmap_atomic() is a wrapper that uses kmap():
+ */
+#ifdef CONFIG_PREEMPT_RT
+# define kmap_atomic_prot(page, type, prot)	kmap(page)
+# define kmap_atomic(page, type)	kmap(page)
+# define kmap_atomic_pfn(pfn, type)	kmap(pfn_to_page(pfn))
+# define kunmap_atomic(kvaddr, type)	kunmap_virt(kvaddr)
+# define kmap_atomic_to_page(kvaddr)	kmap_to_page(kvaddr)
+#else
+# define kmap_atomic_prot(page, type, prot)	__kmap_atomic_prot(page, type, prot)
+# define kmap_atomic(page, type)	__kmap_atomic(page, type)
+# define kmap_atomic_pfn(pfn, type)	__kmap_atomic_pfn(pfn, type)
+# define kunmap_atomic(kvaddr, type)	__kunmap_atomic(kvaddr, type)
+# define kmap_atomic_to_page(kvaddr)	__kmap_atomic_to_page(kvaddr)
+#endif
+
 #endif /* __KERNEL__ */
 
 #endif /* _ASM_HIGHMEM_H */
--- linux-2.6.23.orig/include/asm-x86/i8253.h	2007-12-21 16:58:24.000000000 -0500
+++ linux-2.6.23/include/asm-x86/i8253.h	2007-12-21 17:02:45.000000000 -0500
@@ -6,7 +6,7 @@
 #define PIT_CH0			0x40
 #define PIT_CH2			0x42
 
-extern spinlock_t i8253_lock;
+extern raw_spinlock_t i8253_lock;
 
 extern struct clock_event_device *global_clock_event;
 
--- linux-2.6.23.orig/include/asm-x86/i8259.h	2007-12-21 16:58:24.000000000 -0500
+++ linux-2.6.23/include/asm-x86/i8259.h	2007-12-21 17:02:45.000000000 -0500
@@ -7,7 +7,7 @@ extern unsigned int cached_irq_mask;
 #define cached_master_mask	(__byte(0, cached_irq_mask))
 #define cached_slave_mask	(__byte(1, cached_irq_mask))
 
-extern spinlock_t i8259A_lock;
+extern raw_spinlock_t i8259A_lock;
 
 extern void init_8259A(int auto_eoi);
 extern void enable_8259A_irq(unsigned int irq);
--- linux-2.6.23.orig/include/asm-x86/mach-default/irq_vectors.h	2007-12-21 16:58:30.000000000 -0500
+++ linux-2.6.23/include/asm-x86/mach-default/irq_vectors.h	2007-12-21 17:02:45.000000000 -0500
@@ -65,7 +65,7 @@
  * levels. (0x80 is the syscall vector)
  */
 #define FIRST_DEVICE_VECTOR	0x31
-#define FIRST_SYSTEM_VECTOR	0xef
+#define FIRST_SYSTEM_VECTOR	0xee
 
 #define TIMER_IRQ 0
 
--- linux-2.6.23.orig/include/asm-x86/mc146818rtc_32.h	2007-12-21 16:58:25.000000000 -0500
+++ linux-2.6.23/include/asm-x86/mc146818rtc_32.h	2007-12-21 17:02:45.000000000 -0500
@@ -72,7 +72,7 @@ static inline unsigned char current_lock
 		lock_cmos(reg)
 #define lock_cmos_suffix(reg) \
 		unlock_cmos();			\
-		local_irq_restore(cmos_flags);	\
+		local_irq_restore(cmos_flags); \
 	} while (0)
 #else
 #define lock_cmos_prefix(reg) do {} while (0)
--- linux-2.6.23.orig/include/asm-x86/pgtable_32.h	2007-12-21 16:58:34.000000000 -0500
+++ linux-2.6.23/include/asm-x86/pgtable_32.h	2007-12-21 17:02:45.000000000 -0500
@@ -33,7 +33,7 @@ struct vm_area_struct;
 extern unsigned long empty_zero_page[1024];
 extern pgd_t swapper_pg_dir[1024];
 extern struct kmem_cache *pmd_cache;
-extern spinlock_t pgd_lock;
+extern raw_spinlock_t pgd_lock;
 extern struct page *pgd_list;
 void check_pgt_cache(void);
 
--- linux-2.6.23.orig/include/asm-x86/tlbflush_32.h	2007-12-21 16:58:34.000000000 -0500
+++ linux-2.6.23/include/asm-x86/tlbflush_32.h	2007-12-21 17:02:45.000000000 -0500
@@ -4,6 +4,21 @@
 #include <linux/mm.h>
 #include <asm/processor.h>
 
+/*
+ * TLB-flush needs to be nonpreemptible on PREEMPT_RT due to the
+ * following complex race scenario:
+ *
+ * if the current task is lazy-TLB and does a TLB flush and
+ * gets preempted after the movl %%r3, %0 but before the
+ * movl %0, %%cr3 then its ->active_mm might change and it will
+ * install the wrong cr3 when it switches back. This is not a
+ * problem for the lazy-TLB task itself, but if the next task it
+ * switches to has an ->mm that is also the lazy-TLB task's
+ * new ->active_mm, then the scheduler will assume that cr3 is
+ * the new one, while we overwrote it with the old one. The result
+ * is the wrong cr3 in the new (non-lazy-TLB) task, which typically
+ * causes an infinite pagefault upon the next userspace access.
+ */
 #ifdef CONFIG_PARAVIRT
 #include <asm/paravirt.h>
 #else
@@ -16,11 +31,13 @@
 	do {								\
 		unsigned int tmpreg;					\
 									\
+		preempt_disable();					\
 		__asm__ __volatile__(					\
 			"movl %%cr3, %0;              \n"		\
 			"movl %0, %%cr3;  # flush TLB \n"		\
 			: "=r" (tmpreg)					\
 			:: "memory");					\
+		preempt_enable();					\
 	} while (0)
 
 /*
@@ -31,6 +48,7 @@
 	do {								\
 		unsigned int tmpreg, cr4, cr4_orig;			\
 									\
+		preempt_disable();					\
 		__asm__ __volatile__(					\
 			"movl %%cr4, %2;  # turn off PGE     \n"	\
 			"movl %2, %1;                        \n"	\
@@ -42,6 +60,7 @@
 			: "=&r" (tmpreg), "=&r" (cr4), "=&r" (cr4_orig)	\
 			: "i" (~X86_CR4_PGE)				\
 			: "memory");					\
+		preempt_enable();					\
 	} while (0)
 
 #define __native_flush_tlb_single(addr) 				\
@@ -97,6 +116,13 @@
 
 static inline void flush_tlb_mm(struct mm_struct *mm)
 {
+	/*
+	 * This is safe on PREEMPT_RT because if we preempt
+	 * right after the check but before the __flush_tlb(),
+	 * and if ->active_mm changes, then we might miss a
+	 * TLB flush, but that TLB flush happened already when
+	 * ->active_mm was changed:
+	 */
 	if (mm == current->active_mm)
 		__flush_tlb();
 }
--- linux-2.6.23.orig/include/asm-x86/xor_32.h	2007-12-21 16:58:25.000000000 -0500
+++ linux-2.6.23/include/asm-x86/xor_32.h	2007-12-21 17:02:45.000000000 -0500
@@ -862,7 +862,21 @@ static struct xor_block_template xor_blo
 #include <asm-generic/xor.h>
 
 #undef XOR_TRY_TEMPLATES
-#define XOR_TRY_TEMPLATES				\
+/*
+ * MMX/SSE ops disable preemption for long periods of time,
+ * so on PREEMPT_RT use the register-based ops only:
+ */
+#ifdef CONFIG_PREEMPT_RT
+# define XOR_TRY_TEMPLATES				\
+	do {						\
+		xor_speed(&xor_block_8regs);		\
+		xor_speed(&xor_block_8regs_p);		\
+		xor_speed(&xor_block_32regs);		\
+		xor_speed(&xor_block_32regs_p);		\
+	} while (0)
+# define XOR_SELECT_TEMPLATE(FASTEST) (FASTEST)
+#else
+# define XOR_TRY_TEMPLATES				\
 	do {						\
 		xor_speed(&xor_block_8regs);		\
 		xor_speed(&xor_block_8regs_p);		\
@@ -875,9 +889,10 @@ static struct xor_block_template xor_blo
 	                xor_speed(&xor_block_p5_mmx);	\
 	        }					\
 	} while (0)
-
 /* We force the use of the SSE xor block because it can write around L2.
    We may also be able to load into the L1 only depending on how the cpu
    deals with a load to a line that is being prefetched.  */
-#define XOR_SELECT_TEMPLATE(FASTEST) \
+# define XOR_SELECT_TEMPLATE(FASTEST) \
 	(cpu_has_xmm ? &xor_block_pIII_sse : FASTEST)
+#endif
+
--- linux-2.6.23.orig/kernel/Kconfig.instrumentation	2007-12-21 16:58:25.000000000 -0500
+++ linux-2.6.23/kernel/Kconfig.instrumentation	2007-12-21 17:02:45.000000000 -0500
@@ -29,6 +29,11 @@ config OPROFILE
 
 	  If unsure, say N.
 
+config PROFILE_NMI
+	bool
+	depends on OPROFILE
+	default y
+
 config KPROBES
 	bool "Kprobes"
 	depends on KALLSYMS && MODULES && !UML
