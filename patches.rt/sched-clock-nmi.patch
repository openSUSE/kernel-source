Subject: Linux-RT 2.6.25.4-RT
From: http://www.kernel.org/pub/linux/kernel/projects/rt/
Acked-by: Sven-Thorsten Dietrich <sdietrich@suse.de>
From: Peter Zijlstra

---
 include/linux/hardirq.h |   28 ++++++++++++++++++++++++----
 kernel/sched_clock.c    |   15 ++++++++++-----
 2 files changed, 34 insertions(+), 9 deletions(-)

Index: linux-2.6.25.4-rt4/include/linux/hardirq.h
===================================================================
--- linux-2.6.25.4-rt4.orig/include/linux/hardirq.h	2008-05-29 09:46:43.000000000 -0400
+++ linux-2.6.25.4-rt4/include/linux/hardirq.h	2008-05-29 09:47:21.000000000 -0400
@@ -22,10 +22,13 @@
  * PREEMPT_MASK: 0x000000ff
  * SOFTIRQ_MASK: 0x0000ff00
  * HARDIRQ_MASK: 0x0fff0000
+ * HARDNMI_MASK: 0x40000000
  */
 #define PREEMPT_BITS	8
 #define SOFTIRQ_BITS	8
 
+#define HARDNMI_BITS	1
+
 #ifndef HARDIRQ_BITS
 #define HARDIRQ_BITS	12
 
@@ -46,30 +49,35 @@
 #define PREEMPT_SHIFT		0
 #define SOFTIRQ_SHIFT		(PREEMPT_SHIFT + PREEMPT_BITS)
 #define HARDIRQ_SHIFT		(SOFTIRQ_SHIFT + SOFTIRQ_BITS)
-#define PREEMPT_ACTIVE_SHIFT	(HARDIRQ_SHIFT + HARDIRQ_BITS)
+#define HARDNMI_SHIFT  		(30)
 
 #define __IRQ_MASK(x)		((1UL << (x))-1)
 
 #define PREEMPT_MASK		(__IRQ_MASK(PREEMPT_BITS) << PREEMPT_SHIFT)
 #define SOFTIRQ_MASK		(__IRQ_MASK(SOFTIRQ_BITS) << SOFTIRQ_SHIFT)
 #define HARDIRQ_MASK		(__IRQ_MASK(HARDIRQ_BITS) << HARDIRQ_SHIFT)
+#define HARDNMI_MASK   		(__IRQ_MASK(HARDNMI_BITS) << HARDNMI_SHIFT)
 
 #define PREEMPT_OFFSET		(1UL << PREEMPT_SHIFT)
 #define SOFTIRQ_OFFSET		(1UL << SOFTIRQ_SHIFT)
 #define HARDIRQ_OFFSET		(1UL << HARDIRQ_SHIFT)
+#define HARDNMI_OFFSET 		(1UL << HARDNMI_SHIFT)
 
 #if PREEMPT_ACTIVE < (1 << (HARDIRQ_SHIFT + HARDIRQ_BITS))
 # error PREEMPT_ACTIVE is too low!
 #endif
 
+#define hardnmi_count()	(preempt_count() & HARDNMI_MASK)
 #define hardirq_count()	(preempt_count() & HARDIRQ_MASK)
 #define softirq_count()	(preempt_count() & SOFTIRQ_MASK)
-#define irq_count()	(preempt_count() & (HARDIRQ_MASK | SOFTIRQ_MASK))
+#define irq_count()	\
+	(preempt_count() & (HARDNMI_MASK | HARDIRQ_MASK | SOFTIRQ_MASK))
 
 /*
  * Are we doing bottom half or hardware interrupt processing?
  * Are we in a softirq context? Interrupt context?
  */
+#define in_nmi()	(hardnmi_count())
 #define in_irq()	(hardirq_count() || (current->flags & PF_HARDIRQ))
 #define in_softirq()	(softirq_count() || (current->flags & PF_SOFTIRQ))
 #define in_interrupt()	(irq_count())
@@ -161,7 +169,19 @@ extern void irq_enter(void);
  */
 extern void irq_exit(void);
 
-#define nmi_enter()		do { lockdep_off(); __irq_enter(); } while (0)
-#define nmi_exit()		do { __irq_exit(); lockdep_on(); } while (0)
+#define nmi_enter()					\
+	do {                                            \
+		lockdep_off();                          \
+		BUG_ON(hardnmi_count());                \
+		add_preempt_count(HARDNMI_OFFSET);      \
+		__irq_enter();                          \
+	} while (0)
+
+#define nmi_exit()					\
+	do {                                            \
+		__irq_exit();                           \
+		sub_preempt_count(HARDNMI_OFFSET);      \
+		lockdep_on();                           \
+	} while (0)
 
 #endif /* LINUX_HARDIRQ_H */
Index: linux-2.6.25.4-rt4/kernel/sched_clock.c
===================================================================
--- linux-2.6.25.4-rt4.orig/kernel/sched_clock.c	2008-05-29 09:47:21.000000000 -0400
+++ linux-2.6.25.4-rt4/kernel/sched_clock.c	2008-05-29 09:47:21.000000000 -0400
@@ -144,17 +144,14 @@ u64 sched_clock_cpu(int cpu)
 	if (unlikely(!sched_clock_running))
 		return 0ULL;
 
-	/*
-	 * Normally this is not called in NMI context - but if it is,
-	 * trying to do any locking here is totally lethal.
 	if (unlikely(in_nmi()))
 		return scd->clock;
-	 */
 
 	WARN_ON_ONCE(!irqs_disabled());
 	now = sched_clock();
 
-	if (cpu != raw_smp_processor_id()) {
+#if 0
+	if (cpu != smp_processor_id()) {
 		/*
 		 * in order to update a remote cpu's clock based on our
 		 * unstable raw time rebase it against:
@@ -180,6 +177,14 @@ u64 sched_clock_cpu(int cpu)
 	clock = scd->clock;
 
 	__raw_spin_unlock(&scd->lock);
+#else
+	if (cpu == smp_processor_id() && __raw_spin_trylock(&scd->lock)) {
+		__update_sched_clock(scd, now);
+		clock = scd->clock;
+		__raw_spin_unlock(&scd->lock);
+	} else
+		clock = scd->clock;
+#endif
 
 	return clock;
 }
