Subject: Linux-RT 2.6.27-RT
From: http://www.kernel.org/pub/linux/kernel/projects/rt/
Acked-by: Sven-Thorsten Dietrich <sdietrich@suse.de>
diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 021d8e7..96fec41 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -67,6 +67,17 @@ extern struct nsproxy init_nsproxy;
 	.signalfd_wqh	= __WAIT_QUEUE_HEAD_INITIALIZER(sighand.signalfd_wqh),	\
 }
 
+#ifdef CONFIG_PREEMPT_RCU_BOOST
+#define INIT_RCU_BOOST_PRIO .rcu_prio	= MAX_PRIO,
+#define INIT_PREEMPT_RCU_BOOST(tsk)					\
+	.rcub_rbdp	= NULL,						\
+	.rcub_state	= RCU_BOOST_IDLE,				\
+	.rcub_entry	= LIST_HEAD_INIT(tsk.rcub_entry),
+#else /* #ifdef CONFIG_PREEMPT_RCU_BOOST */
+#define INIT_RCU_BOOST_PRIO
+#define INIT_PREEMPT_RCU_BOOST(tsk)
+#endif /* #else #ifdef CONFIG_PREEMPT_RCU_BOOST */
+
 extern struct group_info init_groups;
 
 #define INIT_STRUCT_PID {						\
@@ -128,6 +139,7 @@ extern struct group_info init_groups;
 	.static_prio	= MAX_PRIO-20,					\
 	.normal_prio	= MAX_PRIO-20,					\
 	.policy		= SCHED_NORMAL,					\
+	INIT_RCU_BOOST_PRIO						\
 	.cpus_allowed	= CPU_MASK_ALL,					\
 	.mm		= NULL,						\
 	.active_mm	= &init_mm,					\
@@ -179,6 +191,7 @@ extern struct group_info init_groups;
 	INIT_IDS							\
 	INIT_TRACE_IRQFLAGS						\
 	INIT_LOCKDEP							\
+	INIT_PREEMPT_RCU_BOOST(tsk)					\
 }
 
 
diff --git a/include/linux/rcupdate.h b/include/linux/rcupdate.h
index e8b4039..8e6b5a4 100644
--- a/include/linux/rcupdate.h
+++ b/include/linux/rcupdate.h
@@ -186,8 +186,10 @@ void name(void) \
 	init_completion(&rcu.completion); \
 	/* Will wake me after RCU finished. */ \
 	func(&rcu.head, wakeme_after_rcu); \
+	rcu_boost_readers(); \
 	/* Wait for it. */ \
 	wait_for_completion(&rcu.completion); \
+	rcu_unboost_readers(); \
 }
 
 /**
@@ -253,4 +255,48 @@ extern void rcu_barrier_sched(void);
 extern void rcu_init(void);
 extern int rcu_needs_cpu(int cpu);
 
+struct dentry;
+
+#ifdef CONFIG_PREEMPT_RCU_BOOST
+extern void init_rcu_boost_late(void);
+extern void rcu_boost_readers(void);
+extern void rcu_unboost_readers(void);
+extern void __rcu_preempt_boost(void);
+#ifdef CONFIG_RCU_TRACE
+extern int rcu_trace_boost_create(struct dentry *rcudir);
+extern void rcu_trace_boost_destroy(void);
+#endif /* CONFIG_RCU_TRACE */
+#define rcu_preempt_boost() /* cpp to avoid #include hell. */ \
+	do { \
+		if (unlikely(current->rcu_read_lock_nesting > 0)) \
+			__rcu_preempt_boost(); \
+	} while (0)
+extern void __rcu_preempt_unboost(void);
+#else /* #ifdef CONFIG_PREEMPT_RCU_BOOST */
+static inline void init_rcu_boost_late(void)
+{
+}
+static inline void rcu_preempt_boost(void)
+{
+}
+static inline void __rcu_preempt_unboost(void)
+{
+}
+static inline void rcu_boost_readers(void)
+{
+}
+static inline void rcu_unboost_readers(void)
+{
+}
+#ifdef CONFIG_RCU_TRACE
+static inline int rcu_trace_boost_create(struct dentry *rcudir)
+{
+	return 0;
+}
+static inline void rcu_trace_boost_destroy(void)
+{
+}
+#endif /* CONFIG_RCU_TRACE */
+#endif /* #else #ifdef CONFIG_PREEMPT_RCU_BOOST */
+
 #endif /* __LINUX_RCUPDATE_H */
diff --git a/include/linux/rcupreempt.h b/include/linux/rcupreempt.h
index d7e2665..ecfc052 100644
--- a/include/linux/rcupreempt.h
+++ b/include/linux/rcupreempt.h
@@ -73,6 +73,26 @@ static inline void rcu_qsctr_inc(int cpu)
 extern void call_rcu_sched(struct rcu_head *head,
 			   void (*func)(struct rcu_head *head));
 
+#ifdef CONFIG_PREEMPT_RCU_BOOST
+/*
+ * Task state with respect to being RCU-boosted.  This state is changed
+ * by the task itself in response to the following three events:
+ * 1. Preemption (or block on lock) while in RCU read-side critical section.
+ * 2. Outermost rcu_read_unlock() for blocked RCU read-side critical section.
+ *
+ * The RCU-boost task also updates the state when boosting priority.
+ */
+enum rcu_boost_state {
+	RCU_BOOST_IDLE = 0,	   /* Not yet blocked if in RCU read-side. */
+	RCU_BOOST_BLOCKED = 1,	   /* Blocked from RCU read-side. */
+	RCU_BOOSTED = 2,	   /* Boosting complete. */
+	RCU_BOOST_INVALID = 3,	   /* For bogus state sightings. */
+};
+
+#define N_RCU_BOOST_STATE (RCU_BOOST_INVALID + 1)
+
+#endif /* #ifdef CONFIG_PREEMPT_RCU_BOOST */
+
 extern void __rcu_read_lock(void)	__acquires(RCU);
 extern void __rcu_read_unlock(void)	__releases(RCU);
 extern int rcu_pending(int cpu);
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5270d44..dee0f93 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -567,6 +567,19 @@ struct signal_struct {
 
 #define SIGNAL_UNKILLABLE	0x00000040 /* for init: ignore fatal signals */
 
+#ifdef CONFIG_PREEMPT_RCU_BOOST
+#define set_rcu_prio(p, prio) /* cpp to avoid #include hell */ \
+	do { \
+		(p)->rcu_prio = (prio); \
+	} while (0)
+#define get_rcu_prio(p) (p)->rcu_prio  /* cpp to avoid #include hell */
+#else /* #ifdef CONFIG_PREEMPT_RCU_BOOST */
+static inline void set_rcu_prio(struct task_struct *p, int prio)
+{
+}
+#define get_rcu_prio(p) (MAX_PRIO)  /* cpp to use MAX_PRIO before it's defined */
+#endif /* #else #ifdef CONFIG_PREEMPT_RCU_BOOST */
+
 /* If true, all threads except ->group_exit_task have pending SIGKILL */
 static inline int signal_group_exit(const struct signal_struct *sig)
 {
@@ -1040,6 +1053,9 @@ struct task_struct {
 
 	int prio, static_prio, normal_prio;
 	unsigned int rt_priority;
+#ifdef CONFIG_PREEMPT_RCU_BOOST
+	int rcu_prio;
+#endif
 	const struct sched_class *sched_class;
 	struct sched_entity se;
 	struct sched_rt_entity rt;
@@ -1074,6 +1090,12 @@ struct task_struct {
 #if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
 	struct sched_info sched_info;
 #endif
+#ifdef CONFIG_PREEMPT_RCU_BOOST
+	struct rcu_boost_dat *rcub_rbdp;
+	enum rcu_boost_state rcub_state;
+	struct list_head rcub_entry;
+	unsigned long rcu_preempt_counter;
+#endif
 
 	struct list_head tasks;
 
diff --git a/kernel/Kconfig.preempt b/kernel/Kconfig.preempt
index 9fdba03..e8b6489 100644
--- a/kernel/Kconfig.preempt
+++ b/kernel/Kconfig.preempt
@@ -66,6 +66,19 @@ config PREEMPT_RCU
 
 	  Say N if you are unsure.
 
+config PREEMPT_RCU_BOOST
+	bool "Enable priority boosting of RCU read-side critical sections"
+	depends on PREEMPT_RCU
+	help
+	  This option permits priority boosting of RCU read-side critical
+	  sections tat have been preempted and a RT process is waiting
+	  on a synchronize_rcu.
+
+	  An RCU thread is also created that periodically wakes up and
+	  performs a synchronize_rcu to make sure that all readers eventually
+	  do complete to prevent an indefinite delay of grace periods and
+	  possible OOM problems.
+
 config RCU_TRACE
 	bool "Enable tracing for RCU - currently stats in debugfs"
 	depends on PREEMPT_RCU
diff --git a/kernel/Makefile b/kernel/Makefile
index 4e1d7df..995a486 100644
--- a/kernel/Makefile
+++ b/kernel/Makefile
@@ -75,6 +75,7 @@ obj-$(CONFIG_SECCOMP) += seccomp.o
 obj-$(CONFIG_RCU_TORTURE_TEST) += rcutorture.o
 obj-$(CONFIG_CLASSIC_RCU) += rcuclassic.o
 obj-$(CONFIG_PREEMPT_RCU) += rcupreempt.o
+obj-$(CONFIG_PREEMPT_RCU_BOOST) += rcupreempt-boost.o
 ifeq ($(CONFIG_PREEMPT_RCU),y)
 obj-$(CONFIG_RCU_TRACE) += rcupreempt_trace.o
 endif
diff --git a/kernel/fork.c b/kernel/fork.c
index 7ce2ebe..f2d8295 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -972,7 +972,13 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #ifdef CONFIG_PREEMPT_RCU
 	p->rcu_read_lock_nesting = 0;
 	p->rcu_flipctr_idx = 0;
-#endif /* #ifdef CONFIG_PREEMPT_RCU */
+#ifdef CONFIG_PREEMPT_RCU_BOOST
+	p->rcu_prio = MAX_PRIO;
+	p->rcub_rbdp = NULL;
+	p->rcub_state = RCU_BOOST_IDLE;
+	INIT_LIST_HEAD(&p->rcub_entry);
+#endif
+#endif /* CONFIG_PREEMPT_RCU */
 	p->vfork_done = NULL;
 	spin_lock_init(&p->alloc_lock);
 
diff --git a/kernel/rcupreempt-boost.c b/kernel/rcupreempt-boost.c
new file mode 100644
index 0000000..7ba6e07
--- /dev/null
+++ b/kernel/rcupreempt-boost.c
@@ -0,0 +1,549 @@
+/*
+ * Read-Copy Update preempt priority boosting
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * Copyright Red Hat Inc, 2007
+ *
+ * Authors: Steven Rostedt <srostedt@redhat.com>
+ *
+ * Based on the original work by Paul McKenney <paulmck@us.ibm.com>.
+ *
+ */
+#include <linux/sched.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/debugfs.h>
+#include <linux/module.h>
+#include <linux/syscalls.h>
+#include <linux/kthread.h>
+
+DEFINE_SPINLOCK(rcu_boost_wake_lock);
+static int rcu_boost_prio = MAX_PRIO;	/* Prio to set preempted RCU readers */
+static long rcu_boost_counter;		/* used to keep track of who boosted */
+static int rcu_preempt_thread_secs = 3;	/* Seconds between waking rcupreemptd thread */
+
+struct rcu_boost_dat {
+	spinlock_t rbs_lock;		/* Sync changes to this struct */
+	int rbs_prio;			/* CPU copy of rcu_boost_prio  */
+	struct list_head rbs_toboost;	/* Preempted RCU readers       */
+	struct list_head rbs_boosted;	/* RCU readers that have been boosted */
+#ifdef CONFIG_RCU_TRACE
+	/* The rest are for statistics */
+	unsigned long rbs_stat_task_boost_called;
+	unsigned long rbs_stat_task_boosted;
+	unsigned long rbs_stat_boost_called;
+	unsigned long rbs_stat_try_boost;
+	unsigned long rbs_stat_boosted;
+	unsigned long rbs_stat_unboost_called;
+	unsigned long rbs_stat_unboosted;
+	unsigned long rbs_stat_try_boost_readers;
+	unsigned long rbs_stat_boost_readers;
+	unsigned long rbs_stat_try_unboost_readers;
+	unsigned long rbs_stat_unboost_readers;
+	unsigned long rbs_stat_over_taken;
+#endif /* CONFIG_RCU_TRACE */
+};
+
+static DEFINE_PER_CPU(struct rcu_boost_dat, rcu_boost_data);
+#define RCU_BOOST_ME &__get_cpu_var(rcu_boost_data)
+
+#ifdef CONFIG_RCU_TRACE
+
+#define RCUPREEMPT_BOOST_TRACE_BUF_SIZE 4096
+static char rcupreempt_boost_trace_buf[RCUPREEMPT_BOOST_TRACE_BUF_SIZE];
+
+static ssize_t rcuboost_read(struct file *filp, char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	static DEFINE_MUTEX(mutex);
+	int cnt = 0;
+	int cpu;
+	struct rcu_boost_dat *rbd;
+	ssize_t bcount;
+	unsigned long task_boost_called = 0;
+	unsigned long task_boosted = 0;
+	unsigned long boost_called = 0;
+	unsigned long try_boost = 0;
+	unsigned long boosted = 0;
+	unsigned long unboost_called = 0;
+	unsigned long unboosted = 0;
+	unsigned long try_boost_readers = 0;
+	unsigned long boost_readers = 0;
+	unsigned long try_unboost_readers = 0;
+	unsigned long unboost_readers = 0;
+	unsigned long over_taken = 0;
+
+	mutex_lock(&mutex);
+
+	for_each_online_cpu(cpu) {
+		rbd = &per_cpu(rcu_boost_data, cpu);
+
+		task_boost_called += rbd->rbs_stat_task_boost_called;
+		task_boosted += rbd->rbs_stat_task_boosted;
+		boost_called += rbd->rbs_stat_boost_called;
+		try_boost += rbd->rbs_stat_try_boost;
+		boosted += rbd->rbs_stat_boosted;
+		unboost_called += rbd->rbs_stat_unboost_called;
+		unboosted += rbd->rbs_stat_unboosted;
+		try_boost_readers += rbd->rbs_stat_try_boost_readers;
+		boost_readers += rbd->rbs_stat_boost_readers;
+		try_unboost_readers += rbd->rbs_stat_try_boost_readers;
+		unboost_readers += rbd->rbs_stat_boost_readers;
+		over_taken += rbd->rbs_stat_over_taken;
+	}
+
+	cnt += snprintf(&rcupreempt_boost_trace_buf[cnt],
+			RCUPREEMPT_BOOST_TRACE_BUF_SIZE - cnt,
+			"task_boost_called = %ld\n",
+			task_boost_called);
+	cnt += snprintf(&rcupreempt_boost_trace_buf[cnt],
+			RCUPREEMPT_BOOST_TRACE_BUF_SIZE - cnt,
+			"task_boosted = %ld\n",
+			task_boosted);
+	cnt += snprintf(&rcupreempt_boost_trace_buf[cnt],
+			RCUPREEMPT_BOOST_TRACE_BUF_SIZE - cnt,
+			"boost_called = %ld\n",
+			boost_called);
+	cnt += snprintf(&rcupreempt_boost_trace_buf[cnt],
+			RCUPREEMPT_BOOST_TRACE_BUF_SIZE - cnt,
+			"try_boost = %ld\n",
+			try_boost);
+	cnt += snprintf(&rcupreempt_boost_trace_buf[cnt],
+			RCUPREEMPT_BOOST_TRACE_BUF_SIZE - cnt,
+			"boosted = %ld\n",
+			boosted);
+	cnt += snprintf(&rcupreempt_boost_trace_buf[cnt],
+			RCUPREEMPT_BOOST_TRACE_BUF_SIZE - cnt,
+			"unboost_called = %ld\n",
+			unboost_called);
+	cnt += snprintf(&rcupreempt_boost_trace_buf[cnt],
+			RCUPREEMPT_BOOST_TRACE_BUF_SIZE - cnt,
+			"unboosted = %ld\n",
+			unboosted);
+	cnt += snprintf(&rcupreempt_boost_trace_buf[cnt],
+			RCUPREEMPT_BOOST_TRACE_BUF_SIZE - cnt,
+			"try_boost_readers = %ld\n",
+			try_boost_readers);
+	cnt += snprintf(&rcupreempt_boost_trace_buf[cnt],
+			RCUPREEMPT_BOOST_TRACE_BUF_SIZE - cnt,
+			"boost_readers = %ld\n",
+			boost_readers);
+	cnt += snprintf(&rcupreempt_boost_trace_buf[cnt],
+			RCUPREEMPT_BOOST_TRACE_BUF_SIZE - cnt,
+			"try_unboost_readers = %ld\n",
+			try_unboost_readers);
+	cnt += snprintf(&rcupreempt_boost_trace_buf[cnt],
+			RCUPREEMPT_BOOST_TRACE_BUF_SIZE - cnt,
+			"unboost_readers = %ld\n",
+			unboost_readers);
+	cnt += snprintf(&rcupreempt_boost_trace_buf[cnt],
+			RCUPREEMPT_BOOST_TRACE_BUF_SIZE - cnt,
+			"over_taken = %ld\n",
+			over_taken);
+	cnt += snprintf(&rcupreempt_boost_trace_buf[cnt],
+			RCUPREEMPT_BOOST_TRACE_BUF_SIZE - cnt,
+			"rcu_boost_prio = %d\n",
+			rcu_boost_prio);
+	bcount = simple_read_from_buffer(buffer, count, ppos,
+			rcupreempt_boost_trace_buf, strlen(rcupreempt_boost_trace_buf));
+	mutex_unlock(&mutex);
+
+	return bcount;
+}
+
+static struct file_operations rcuboost_fops = {
+	.read = rcuboost_read,
+};
+
+static struct dentry  *rcuboostdir;
+int rcu_trace_boost_create(struct dentry *rcudir)
+{
+	rcuboostdir = debugfs_create_file("rcuboost", 0444, rcudir,
+					  NULL, &rcuboost_fops);
+	if (!rcuboostdir)
+		return 1;
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(rcu_trace_boost_create);
+
+void rcu_trace_boost_destroy(void)
+{
+	if (rcuboostdir)
+		debugfs_remove(rcuboostdir);
+	rcuboostdir = NULL;
+}
+EXPORT_SYMBOL_GPL(rcu_trace_boost_destroy);
+
+#define RCU_BOOST_TRACE_FUNC_DECL(type)			      \
+	static void rcu_trace_boost_##type(struct rcu_boost_dat *rbd)	\
+	{								\
+		rbd->rbs_stat_##type++;					\
+	}
+RCU_BOOST_TRACE_FUNC_DECL(task_boost_called)
+RCU_BOOST_TRACE_FUNC_DECL(task_boosted)
+RCU_BOOST_TRACE_FUNC_DECL(boost_called)
+RCU_BOOST_TRACE_FUNC_DECL(try_boost)
+RCU_BOOST_TRACE_FUNC_DECL(boosted)
+RCU_BOOST_TRACE_FUNC_DECL(unboost_called)
+RCU_BOOST_TRACE_FUNC_DECL(unboosted)
+RCU_BOOST_TRACE_FUNC_DECL(try_boost_readers)
+RCU_BOOST_TRACE_FUNC_DECL(boost_readers)
+RCU_BOOST_TRACE_FUNC_DECL(try_unboost_readers)
+RCU_BOOST_TRACE_FUNC_DECL(unboost_readers)
+RCU_BOOST_TRACE_FUNC_DECL(over_taken)
+#else /* CONFIG_RCU_TRACE */
+/* These were created by the above macro "RCU_BOOST_TRACE_FUNC_DECL" */
+# define rcu_trace_boost_task_boost_called(rbd) do { } while (0)
+# define rcu_trace_boost_task_boosted(rbd) do { } while (0)
+# define rcu_trace_boost_boost_called(rbd) do { } while (0)
+# define rcu_trace_boost_try_boost(rbd) do { } while (0)
+# define rcu_trace_boost_boosted(rbd) do { } while (0)
+# define rcu_trace_boost_unboost_called(rbd) do { } while (0)
+# define rcu_trace_boost_unboosted(rbd) do { } while (0)
+# define rcu_trace_boost_try_boost_readers(rbd) do { } while (0)
+# define rcu_trace_boost_boost_readers(rbd) do { } while (0)
+# define rcu_trace_boost_try_unboost_readers(rbd) do { } while (0)
+# define rcu_trace_boost_unboost_readers(rbd) do { } while (0)
+# define rcu_trace_boost_over_taken(rbd) do { } while (0)
+#endif /* CONFIG_RCU_TRACE */
+
+/*
+ * Helper function to boost a task's prio.
+ */
+static void rcu_boost_task(struct task_struct *task)
+{
+	WARN_ON(!irqs_disabled());
+	WARN_ON_SMP(!spin_is_locked(&task->pi_lock));
+
+	rcu_trace_boost_task_boost_called(RCU_BOOST_ME);
+
+	if (task->rcu_prio < task->prio) {
+		rcu_trace_boost_task_boosted(RCU_BOOST_ME);
+		rt_mutex_setprio(task, task->rcu_prio);
+	}
+}
+
+/**
+ * __rcu_preepmt_boost - Called by sleeping RCU readers.
+ *
+ * When the RCU read-side critical section is preempted
+ * (or schedules out due to RT mutex)
+ * it places itself onto a list to notify that it is sleeping
+ * while holding a RCU read lock. If there is already a
+ * synchronize_rcu happening, then it will increase its
+ * priority (if necessary).
+ */
+void __rcu_preempt_boost(void)
+{
+	struct task_struct *curr = current;
+	struct rcu_boost_dat *rbd;
+	int prio;
+	unsigned long flags;
+
+	WARN_ON(!current->rcu_read_lock_nesting);
+
+	rcu_trace_boost_boost_called(RCU_BOOST_ME);
+
+	/* check to see if we are already boosted */
+	if (unlikely(curr->rcub_rbdp))
+		return;
+
+	/*
+	 * To keep us from preempting between grabing
+	 * the rbd and locking it, we use local_irq_save
+	 */
+	local_irq_save(flags);
+	rbd = &__get_cpu_var(rcu_boost_data);
+	spin_lock(&rbd->rbs_lock);
+
+	spin_lock(&curr->pi_lock);
+
+	curr->rcub_rbdp = rbd;
+
+	rcu_trace_boost_try_boost(rbd);
+
+	prio = rt_mutex_getprio(curr);
+
+	if (list_empty(&curr->rcub_entry))
+		list_add_tail(&curr->rcub_entry, &rbd->rbs_toboost);
+	if (prio <= rbd->rbs_prio)
+		goto out;
+
+	rcu_trace_boost_boosted(curr->rcub_rbdp);
+
+	set_rcu_prio(curr, rbd->rbs_prio);
+	rcu_boost_task(curr);
+
+ out:
+	spin_unlock(&curr->pi_lock);
+	spin_unlock_irqrestore(&rbd->rbs_lock, flags);
+}
+
+/**
+ * __rcu_preempt_unboost - called when releasing the RCU read lock
+ *
+ * When releasing the RCU read lock, a check is made to see if
+ * the task was preempted. If it was, it removes itself from the
+ * RCU data lists and if necessary, sets its priority back to
+ * normal.
+ */
+void __rcu_preempt_unboost(void)
+{
+	struct task_struct *curr = current;
+	struct rcu_boost_dat *rbd;
+	int prio;
+	unsigned long flags;
+
+	rcu_trace_boost_unboost_called(RCU_BOOST_ME);
+
+	/* if not boosted, then ignore */
+	if (likely(!curr->rcub_rbdp))
+		return;
+
+	rbd = curr->rcub_rbdp;
+
+	spin_lock_irqsave(&rbd->rbs_lock, flags);
+	list_del_init(&curr->rcub_entry);
+
+	rcu_trace_boost_unboosted(curr->rcub_rbdp);
+
+	set_rcu_prio(curr, MAX_PRIO);
+
+	spin_lock(&curr->pi_lock);
+	prio = rt_mutex_getprio(curr);
+	rt_mutex_setprio(curr, prio);
+
+	curr->rcub_rbdp = NULL;
+
+	spin_unlock(&curr->pi_lock);
+	spin_unlock_irqrestore(&rbd->rbs_lock, flags);
+}
+
+/*
+ * For each rcu_boost_dat structure, update all the tasks that
+ * are on the lists to the priority of the caller of
+ * synchronize_rcu.
+ */
+static int __rcu_boost_readers(struct rcu_boost_dat *rbd, int prio, unsigned long flags)
+{
+	struct task_struct *curr = current;
+	struct task_struct *p;
+
+	spin_lock(&rbd->rbs_lock);
+
+	rbd->rbs_prio = prio;
+
+	/*
+	 * Move the already boosted readers onto the list and reboost
+	 * them.
+	 */
+	list_splice_init(&rbd->rbs_boosted,
+			 &rbd->rbs_toboost);
+
+	while (!list_empty(&rbd->rbs_toboost)) {
+		p = list_entry(rbd->rbs_toboost.next,
+			       struct task_struct, rcub_entry);
+		list_move_tail(&p->rcub_entry,
+			       &rbd->rbs_boosted);
+		set_rcu_prio(p, prio);
+		spin_lock(&p->pi_lock);
+		rcu_boost_task(p);
+		spin_unlock(&p->pi_lock);
+
+		/*
+		 * Now we release the lock to allow for a higher
+		 * priority task to come in and boost the readers
+		 * even higher. Or simply to let a higher priority
+		 * task to run now.
+		 */
+		spin_unlock(&rbd->rbs_lock);
+		spin_unlock_irqrestore(&rcu_boost_wake_lock, flags);
+
+		cpu_relax();
+		spin_lock_irqsave(&rcu_boost_wake_lock, flags);
+		/*
+		 * Another task may have taken over.
+		 */
+		if (curr->rcu_preempt_counter != rcu_boost_counter) {
+			rcu_trace_boost_over_taken(rbd);
+			return 1;
+		}
+
+		spin_lock(&rbd->rbs_lock);
+	}
+
+	spin_unlock(&rbd->rbs_lock);
+
+	return 0;
+}
+
+/**
+ * rcu_boost_readers - called by synchronize_rcu to boost sleeping RCU readers.
+ *
+ * This function iterates over all the per_cpu rcu_boost_data descriptors
+ * and boosts any sleeping (or slept) RCU readers.
+ */
+void rcu_boost_readers(void)
+{
+	struct task_struct *curr = current;
+	struct rcu_boost_dat *rbd;
+	unsigned long flags;
+	int prio;
+	int cpu;
+	int ret;
+
+	spin_lock_irqsave(&rcu_boost_wake_lock, flags);
+
+	prio = rt_mutex_getprio(curr);
+
+	rcu_trace_boost_try_boost_readers(RCU_BOOST_ME);
+
+	if (prio >= rcu_boost_prio) {
+		/* already boosted */
+		spin_unlock_irqrestore(&rcu_boost_wake_lock, flags);
+		return;
+	}
+
+	rcu_boost_prio = prio;
+
+	rcu_trace_boost_boost_readers(RCU_BOOST_ME);
+
+	/* Flag that we are the one to unboost */
+	curr->rcu_preempt_counter = ++rcu_boost_counter;
+
+	for_each_online_cpu(cpu) {
+		rbd = &per_cpu(rcu_boost_data, cpu);
+		ret = __rcu_boost_readers(rbd, prio, flags);
+		if (ret)
+			break;
+	}
+
+	spin_unlock_irqrestore(&rcu_boost_wake_lock, flags);
+
+}
+
+/**
+ * rcu_unboost_readers - set the boost level back to normal.
+ *
+ * This function DOES NOT change the priority of any RCU reader
+ * that was boosted. The RCU readers do that when they release
+ * the RCU lock. This function only sets the global
+ * rcu_boost_prio to MAX_PRIO so that new RCU readers that sleep
+ * do not increase their priority.
+ */
+void rcu_unboost_readers(void)
+{
+	struct rcu_boost_dat *rbd;
+	unsigned long flags;
+	int cpu;
+
+	spin_lock_irqsave(&rcu_boost_wake_lock, flags);
+
+	rcu_trace_boost_try_unboost_readers(RCU_BOOST_ME);
+
+	if (current->rcu_preempt_counter != rcu_boost_counter)
+		goto out;
+
+	rcu_trace_boost_unboost_readers(RCU_BOOST_ME);
+
+	/*
+	 * We could also put in something that
+	 * would allow other synchronize_rcu callers
+	 * of lower priority that are still waiting
+	 * to boost the prio.
+	 */
+	rcu_boost_prio = MAX_PRIO;
+
+	for_each_online_cpu(cpu) {
+		rbd = &per_cpu(rcu_boost_data, cpu);
+
+		spin_lock(&rbd->rbs_lock);
+		rbd->rbs_prio = rcu_boost_prio;
+		spin_unlock(&rbd->rbs_lock);
+	}
+
+ out:
+	spin_unlock_irqrestore(&rcu_boost_wake_lock, flags);
+}
+
+/*
+ * The krcupreemptd wakes up every "rcu_preempt_thread_secs"
+ * seconds at the minimum priority of 1 to do a
+ * synchronize_rcu. This ensures that grace periods finish
+ * and that we do not starve the system. If there are RT
+ * tasks above priority 1 that are hogging the system and
+ * preventing release of memory, then its the fault of the
+ * system designer running RT tasks too aggressively and the
+ * system is flawed regardless.
+ */
+static int krcupreemptd(void *data)
+{
+	struct sched_param param = { .sched_priority = 1 };
+	int ret;
+	int prio;
+
+	ret = sched_setscheduler(current, SCHED_FIFO, &param);
+	printk("krcupreemptd setsched %d\n", ret);
+	prio = current->prio;
+	printk("  prio = %d\n", prio);
+	set_current_state(TASK_INTERRUPTIBLE);
+
+	while (!kthread_should_stop()) {
+		schedule_timeout(rcu_preempt_thread_secs * HZ);
+
+		__set_current_state(TASK_RUNNING);
+		if (prio != current->prio) {
+			prio = current->prio;
+			printk("krcupreemptd new prio is %d??\n",prio);
+		}
+
+		synchronize_rcu();
+
+		set_current_state(TASK_INTERRUPTIBLE);
+	}
+	__set_current_state(TASK_RUNNING);
+	return 0;
+}
+
+static int __init rcu_preempt_boost_init(void)
+{
+	struct rcu_boost_dat *rbd;
+	struct task_struct *p;
+	int cpu;
+
+	for_each_possible_cpu(cpu) {
+		rbd = &per_cpu(rcu_boost_data, cpu);
+
+		spin_lock_init(&rbd->rbs_lock);
+		rbd->rbs_prio = MAX_PRIO;
+		INIT_LIST_HEAD(&rbd->rbs_toboost);
+		INIT_LIST_HEAD(&rbd->rbs_boosted);
+	}
+
+	p = kthread_create(krcupreemptd, NULL,
+			   "krcupreemptd");
+
+	if (IS_ERR(p)) {
+		printk("krcupreemptd failed\n");
+		return NOTIFY_BAD;
+	}
+	wake_up_process(p);
+
+	return 0;
+}
+
+core_initcall(rcu_preempt_boost_init);
diff --git a/kernel/rcupreempt.c b/kernel/rcupreempt.c
index 2782793..e084103 100644
--- a/kernel/rcupreempt.c
+++ b/kernel/rcupreempt.c
@@ -372,6 +372,8 @@ void __rcu_read_unlock(void)
 
 		ACCESS_ONCE(RCU_DATA_ME()->rcu_flipctr[idx])--;
 		local_irq_restore(flags);
+
+		__rcu_preempt_unboost();
 	}
 }
 EXPORT_SYMBOL_GPL(__rcu_read_unlock);
diff --git a/kernel/rcupreempt_trace.c b/kernel/rcupreempt_trace.c
index 35c2d33..6ef8089 100644
--- a/kernel/rcupreempt_trace.c
+++ b/kernel/rcupreempt_trace.c
@@ -295,8 +295,14 @@ static int rcupreempt_debugfs_init(void)
 						NULL, &rcuctrs_fops);
 	if (!ctrsdir)
 		goto free_out;
+
+	if (!rcu_trace_boost_create(rcudir))
+		goto free_out;
+
 	return 0;
 free_out:
+	if (ctrsdir)
+		debugfs_remove(ctrsdir);
 	if (statdir)
 		debugfs_remove(statdir);
 	if (gpdir)
@@ -322,6 +328,7 @@ static int __init rcupreempt_trace_init(void)
 
 static void __exit rcupreempt_trace_cleanup(void)
 {
+	rcu_trace_boost_destroy();
 	debugfs_remove(statdir);
 	debugfs_remove(gpdir);
 	debugfs_remove(ctrsdir);
diff --git a/kernel/rtmutex.c b/kernel/rtmutex.c
index 6522ae5..cee8b35 100644
--- a/kernel/rtmutex.c
+++ b/kernel/rtmutex.c
@@ -105,11 +105,12 @@ static inline void mark_rt_mutex_waiters(struct rt_mutex *lock)
  */
 int rt_mutex_getprio(struct task_struct *task)
 {
+	int prio = min(task->normal_prio, get_rcu_prio(task));
+
 	if (likely(!task_has_pi_waiters(task)))
-		return task->normal_prio;
+		return prio;
 
-	return min(task_top_pi_waiter(task)->pi_list_entry.prio,
-		   task->normal_prio);
+	return min(task_top_pi_waiter(task)->pi_list_entry.prio, prio);
 }
 
 /*
diff --git a/kernel/sched.c b/kernel/sched.c
index 0e7ba00..9412479 100644
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -4345,6 +4345,8 @@ asmlinkage void __sched schedule(void)
 	struct rq *rq;
 	int cpu;
 
+	rcu_preempt_boost();
+
 need_resched:
 	preempt_disable();
 	cpu = smp_processor_id();
