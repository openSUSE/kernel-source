From 34b1a32790420de1aeb9ea47958f76a51684971b Mon Sep 17 00:00:00 2001
From: Francois Dugast <francois.dugast@intel.com>
Date: Tue, 12 Sep 2023 08:36:35 +0000
Subject: drm/xe: Use Xe assert macros instead of XE_WARN_ON macro
Git-commit: c73acc1eeba5e380a367087cb7b933b946613ee7
Patch-mainline: v6.8-rc1
References: drm-backport-placeholder

The XE_WARN_ON macro maps to WARN_ON which is not justified
in many cases where only a simple debug check is needed.
Replace the use of the XE_WARN_ON macro with the new xe_assert
macros which relies on drm_*. This takes a struct drm_device
argument, which is one of the main changes in this commit. The
other main change is that the condition is reversed, as with
XE_WARN_ON a message is displayed if the condition is true,
whereas with xe_assert it is if the condition is false.

v2:
- Rebase
- Keep WARN splats in xe_wopcm.c (Matt Roper)

v3:
- Rebase

Signed-off-by: Francois Dugast <francois.dugast@intel.com>
Reviewed-by: Rodrigo Vivi <rodrigo.vivi@intel.com>
Signed-off-by: Rodrigo Vivi <rodrigo.vivi@intel.com>
Acked-by: Patrik Jakobsson <pjakobsson@suse.de>
---
 drivers/gpu/drm/xe/xe_bb.c                  |    8 +--
 drivers/gpu/drm/xe/xe_bo.c                  |   64 ++++++++++++++-------------
 drivers/gpu/drm/xe/xe_bo_evict.c            |    4 -
 drivers/gpu/drm/xe/xe_device.c              |    6 +-
 drivers/gpu/drm/xe/xe_exec.c                |    2 
 drivers/gpu/drm/xe/xe_execlist.c            |   12 +++--
 drivers/gpu/drm/xe/xe_force_wake.c          |    4 -
 drivers/gpu/drm/xe/xe_force_wake.h          |    6 +-
 drivers/gpu/drm/xe/xe_ggtt.c                |   14 +++---
 drivers/gpu/drm/xe/xe_gt.c                  |    1 
 drivers/gpu/drm/xe/xe_gt_clock.c            |    2 
 drivers/gpu/drm/xe/xe_gt_debugfs.c          |    2 
 drivers/gpu/drm/xe/xe_gt_tlb_invalidation.c |   12 ++---
 drivers/gpu/drm/xe/xe_guc.c                 |   24 +++++-----
 drivers/gpu/drm/xe/xe_guc_ads.c             |   22 +++++----
 drivers/gpu/drm/xe/xe_guc_ct.c              |   29 ++++++------
 drivers/gpu/drm/xe/xe_guc_log.c             |    4 -
 drivers/gpu/drm/xe/xe_guc_pc.c              |    2 
 drivers/gpu/drm/xe/xe_guc_submit.c          |   65 +++++++++++++++++-----------
 drivers/gpu/drm/xe/xe_huc.c                 |    3 -
 drivers/gpu/drm/xe/xe_hw_engine.c           |   11 ++--
 drivers/gpu/drm/xe/xe_lrc.c                 |    9 ++-
 drivers/gpu/drm/xe/xe_migrate.c             |   31 ++++++-------
 drivers/gpu/drm/xe/xe_mocs.c                |    2 
 drivers/gpu/drm/xe/xe_pt.c                  |   14 +++---
 drivers/gpu/drm/xe/xe_ring_ops.c            |   13 +++--
 drivers/gpu/drm/xe/xe_sched_job.c           |    2 
 drivers/gpu/drm/xe/xe_uc.c                  |    2 
 drivers/gpu/drm/xe/xe_uc_fw.c               |   16 +++---
 drivers/gpu/drm/xe/xe_vm.c                  |   48 ++++++++++----------
 drivers/gpu/drm/xe/xe_vm.h                  |    2 
 31 files changed, 235 insertions(+), 201 deletions(-)

--- a/drivers/gpu/drm/xe/xe_bb.c
+++ b/drivers/gpu/drm/xe/xe_bb.c
@@ -66,7 +66,7 @@
 
 	bb->cs[bb->len++] = MI_BATCH_BUFFER_END;
 
-	WARN_ON(bb->len * 4 + bb_prefetch(q->gt) > size);
+	xe_gt_assert(q->gt, bb->len * 4 + bb_prefetch(q->gt) <= size);
 
 	xe_sa_bo_flush_write(bb->bo);
 
@@ -84,8 +84,8 @@
 		4 * second_idx,
 	};
 
-	XE_WARN_ON(second_idx > bb->len);
-	XE_WARN_ON(!(q->vm->flags & XE_VM_FLAG_MIGRATION));
+	xe_gt_assert(q->gt, second_idx <= bb->len);
+	xe_gt_assert(q->gt, q->vm->flags & XE_VM_FLAG_MIGRATION);
 
 	return __xe_bb_create_job(q, bb, addr);
 }
@@ -95,7 +95,7 @@
 {
 	u64 addr = xe_sa_bo_gpu_addr(bb->bo);
 
-	XE_WARN_ON(q->vm && q->vm->flags & XE_VM_FLAG_MIGRATION);
+	xe_gt_assert(q->gt, !(q->vm && q->vm->flags & XE_VM_FLAG_MIGRATION));
 	return __xe_bb_create_job(q, bb, &addr);
 }
 
--- a/drivers/gpu/drm/xe/xe_bo.c
+++ b/drivers/gpu/drm/xe/xe_bo.c
@@ -103,7 +103,7 @@
 static struct xe_tile *
 mem_type_to_tile(struct xe_device *xe, u32 mem_type)
 {
-	XE_WARN_ON(mem_type != XE_PL_STOLEN && !mem_type_is_vram(mem_type));
+	xe_assert(xe, mem_type == XE_PL_STOLEN || mem_type_is_vram(mem_type));
 
 	return &xe->tiles[mem_type == XE_PL_STOLEN ? 0 : (mem_type - XE_PL_VRAM0)];
 }
@@ -142,7 +142,7 @@
 	struct ttm_place place = { .mem_type = mem_type };
 	u64 io_size = tile->mem.vram.io_size;
 
-	XE_WARN_ON(!tile->mem.vram.usable_size);
+	xe_assert(xe, tile->mem.vram.usable_size);
 
 	/*
 	 * For eviction / restore on suspend / resume objects
@@ -544,10 +544,11 @@
 	struct dma_buf_attachment *attach = ttm_bo->base.import_attach;
 	struct xe_ttm_tt *xe_tt = container_of(ttm_bo->ttm, struct xe_ttm_tt,
 					       ttm);
+	struct xe_device *xe = ttm_to_xe_device(ttm_bo->bdev);
 	struct sg_table *sg;
 
-	XE_WARN_ON(!attach);
-	XE_WARN_ON(!ttm_bo->ttm);
+	xe_assert(xe, attach);
+	xe_assert(xe, ttm_bo->ttm);
 
 	if (new_res->mem_type == XE_PL_SYSTEM)
 		goto out;
@@ -709,8 +710,8 @@
 	else if (mem_type_is_vram(old_mem_type))
 		tile = mem_type_to_tile(xe, old_mem_type);
 
-	XE_WARN_ON(!tile);
-	XE_WARN_ON(!tile->migrate);
+	xe_assert(xe, tile);
+	xe_tile_assert(tile, tile->migrate);
 
 	trace_xe_bo_move(bo);
 	xe_device_mem_access_get(xe);
@@ -740,7 +741,7 @@
 					goto out;
 				}
 
-				XE_WARN_ON(new_mem->start !=
+				xe_assert(xe, new_mem->start ==
 					  bo->placements->fpfn);
 
 				iosys_map_set_vaddr_iomem(&bo->vmap, new_addr);
@@ -939,9 +940,10 @@
  */
 static bool xe_ttm_bo_lock_in_destructor(struct ttm_buffer_object *ttm_bo)
 {
+	struct xe_device *xe = ttm_to_xe_device(ttm_bo->bdev);
 	bool locked;
 
-	XE_WARN_ON(kref_read(&ttm_bo->kref));
+	xe_assert(xe, !kref_read(&ttm_bo->kref));
 
 	/*
 	 * We can typically only race with TTM trylocking under the
@@ -952,7 +954,7 @@
 	spin_lock(&ttm_bo->bdev->lru_lock);
 	locked = dma_resv_trylock(ttm_bo->base.resv);
 	spin_unlock(&ttm_bo->bdev->lru_lock);
-	XE_WARN_ON(!locked);
+	xe_assert(xe, locked);
 
 	return locked;
 }
@@ -968,7 +970,7 @@
 		return;
 
 	bo = ttm_to_xe_bo(ttm_bo);
-	XE_WARN_ON(bo->created && kref_read(&ttm_bo->base.refcount));
+	xe_assert(xe_bo_device(bo), !(bo->created && kref_read(&ttm_bo->base.refcount)));
 
 	/*
 	 * Corner case where TTM fails to allocate memory and this BOs resv
@@ -1041,12 +1043,13 @@
 static void xe_ttm_bo_destroy(struct ttm_buffer_object *ttm_bo)
 {
 	struct xe_bo *bo = ttm_to_xe_bo(ttm_bo);
+	struct xe_device *xe = ttm_to_xe_device(ttm_bo->bdev);
 
 	if (bo->ttm.base.import_attach)
 		drm_prime_gem_destroy(&bo->ttm.base, NULL);
 	drm_gem_object_release(&bo->ttm.base);
 
-	WARN_ON(!list_empty(&bo->vmas));
+	xe_assert(xe, list_empty(&bo->vmas));
 
 	if (bo->ggtt_node.size)
 		xe_ggtt_remove_bo(bo->tile->mem.ggtt, bo);
@@ -1082,7 +1085,7 @@
 	struct xe_bo *bo = gem_to_xe_bo(obj);
 
 	if (bo->vm && !xe_vm_in_fault_mode(bo->vm)) {
-		XE_WARN_ON(!xe_bo_is_user(bo));
+		xe_assert(xe_bo_device(bo), xe_bo_is_user(bo));
 
 		xe_bo_lock(bo, false);
 		ttm_bo_set_bulk_move(&bo->ttm, NULL);
@@ -1198,7 +1201,7 @@
 	int err;
 
 	/* Only kernel objects should set GT */
-	XE_WARN_ON(tile && type != ttm_bo_type_kernel);
+	xe_assert(xe, !tile || type == ttm_bo_type_kernel);
 
 	if (XE_WARN_ON(!size)) {
 		xe_bo_free(bo);
@@ -1354,7 +1357,7 @@
 		if (!tile && flags & XE_BO_CREATE_STOLEN_BIT)
 			tile = xe_device_get_root_tile(xe);
 
-		XE_WARN_ON(!tile);
+		xe_assert(xe, tile);
 
 		if (flags & XE_BO_CREATE_STOLEN_BIT &&
 		    flags & XE_BO_FIXED_PLACEMENT_BIT) {
@@ -1485,8 +1488,8 @@
 	struct xe_device *xe = xe_bo_device(bo);
 	int err;
 
-	XE_WARN_ON(bo->vm);
-	XE_WARN_ON(!xe_bo_is_user(bo));
+	xe_assert(xe, !bo->vm);
+	xe_assert(xe, xe_bo_is_user(bo));
 
 	if (!xe_bo_is_pinned(bo)) {
 		err = xe_bo_validate(bo, NULL, false);
@@ -1518,20 +1521,20 @@
 	int err;
 
 	/* We currently don't expect user BO to be pinned */
-	XE_WARN_ON(xe_bo_is_user(bo));
+	xe_assert(xe, !xe_bo_is_user(bo));
 
 	/* Pinned object must be in GGTT or have pinned flag */
-	XE_WARN_ON(!(bo->flags & (XE_BO_CREATE_PINNED_BIT |
-				 XE_BO_CREATE_GGTT_BIT)));
+	xe_assert(xe, bo->flags & (XE_BO_CREATE_PINNED_BIT |
+				   XE_BO_CREATE_GGTT_BIT));
 
 	/*
 	 * No reason we can't support pinning imported dma-bufs we just don't
 	 * expect to pin an imported dma-buf.
 	 */
-	XE_WARN_ON(bo->ttm.base.import_attach);
+	xe_assert(xe, !bo->ttm.base.import_attach);
 
 	/* We only expect at most 1 pin */
-	XE_WARN_ON(xe_bo_is_pinned(bo));
+	xe_assert(xe, !xe_bo_is_pinned(bo));
 
 	err = xe_bo_validate(bo, NULL, false);
 	if (err)
@@ -1547,7 +1550,7 @@
 		struct ttm_place *place = &(bo->placements[0]);
 
 		if (mem_type_is_vram(place->mem_type)) {
-			XE_WARN_ON(!(place->flags & TTM_PL_FLAG_CONTIGUOUS));
+			xe_assert(xe, place->flags & TTM_PL_FLAG_CONTIGUOUS);
 
 			place->fpfn = (xe_bo_addr(bo, 0, PAGE_SIZE) -
 				       vram_region_gpu_offset(bo->ttm.resource)) >> PAGE_SHIFT;
@@ -1584,9 +1587,9 @@
 {
 	struct xe_device *xe = xe_bo_device(bo);
 
-	XE_WARN_ON(bo->vm);
-	XE_WARN_ON(!xe_bo_is_pinned(bo));
-	XE_WARN_ON(!xe_bo_is_user(bo));
+	xe_assert(xe, !bo->vm);
+	xe_assert(xe, xe_bo_is_pinned(bo));
+	xe_assert(xe, xe_bo_is_user(bo));
 
 	if (bo->ttm.pin_count == 1 && !list_empty(&bo->pinned_link)) {
 		spin_lock(&xe->pinned.lock);
@@ -1607,15 +1610,15 @@
 {
 	struct xe_device *xe = xe_bo_device(bo);
 
-	XE_WARN_ON(bo->ttm.base.import_attach);
-	XE_WARN_ON(!xe_bo_is_pinned(bo));
+	xe_assert(xe, !bo->ttm.base.import_attach);
+	xe_assert(xe, xe_bo_is_pinned(bo));
 
 	if (IS_DGFX(xe) && !(IS_ENABLED(CONFIG_DRM_XE_DEBUG) &&
 	    bo->flags & XE_BO_INTERNAL_TEST)) {
 		struct ttm_place *place = &(bo->placements[0]);
 
 		if (mem_type_is_vram(place->mem_type)) {
-			XE_WARN_ON(list_empty(&bo->pinned_link));
+			xe_assert(xe, !list_empty(&bo->pinned_link));
 
 			spin_lock(&xe->pinned.lock);
 			list_del_init(&bo->pinned_link);
@@ -1676,15 +1679,16 @@
  */
 dma_addr_t __xe_bo_addr(struct xe_bo *bo, u64 offset, size_t page_size)
 {
+	struct xe_device *xe = xe_bo_device(bo);
 	struct xe_res_cursor cur;
 	u64 page;
 
-	XE_WARN_ON(page_size > PAGE_SIZE);
+	xe_assert(xe, page_size <= PAGE_SIZE);
 	page = offset >> PAGE_SHIFT;
 	offset &= (PAGE_SIZE - 1);
 
 	if (!xe_bo_is_vram(bo) && !xe_bo_is_stolen(bo)) {
-		XE_WARN_ON(!bo->ttm.ttm);
+		xe_assert(xe, bo->ttm.ttm);
 
 		xe_res_first_sg(xe_bo_get_sg(bo), page << PAGE_SHIFT,
 				page_size, &cur);
--- a/drivers/gpu/drm/xe/xe_bo_evict.c
+++ b/drivers/gpu/drm/xe/xe_bo_evict.c
@@ -158,8 +158,8 @@
 		 * We expect validate to trigger a move VRAM and our move code
 		 * should setup the iosys map.
 		 */
-		XE_WARN_ON(iosys_map_is_null(&bo->vmap));
-		XE_WARN_ON(!xe_bo_is_vram(bo));
+		xe_assert(xe, !iosys_map_is_null(&bo->vmap));
+		xe_assert(xe, xe_bo_is_vram(bo));
 
 		xe_bo_put(bo);
 
--- a/drivers/gpu/drm/xe/xe_device.c
+++ b/drivers/gpu/drm/xe/xe_device.c
@@ -394,7 +394,7 @@
 	if (active) {
 		int ref = atomic_inc_return(&xe->mem_access.ref);
 
-		XE_WARN_ON(ref == S32_MAX);
+		xe_assert(xe, ref != S32_MAX);
 	}
 
 	return active;
@@ -436,7 +436,7 @@
 	xe_pm_runtime_get(xe);
 	ref = atomic_inc_return(&xe->mem_access.ref);
 
-	XE_WARN_ON(ref == S32_MAX);
+	xe_assert(xe, ref != S32_MAX);
 
 }
 
@@ -450,5 +450,5 @@
 	ref = atomic_dec_return(&xe->mem_access.ref);
 	xe_pm_runtime_put(xe);
 
-	XE_WARN_ON(ref < 0);
+	xe_assert(xe, ref >= 0);
 }
--- a/drivers/gpu/drm/xe/xe_exec.c
+++ b/drivers/gpu/drm/xe/xe_exec.c
@@ -117,7 +117,7 @@
 	 * to a location where the GPU can access it).
 	 */
 	list_for_each_entry(vma, &vm->rebind_list, combined_links.rebind) {
-		XE_WARN_ON(xe_vma_is_null(vma));
+		xe_assert(vm->xe, !xe_vma_is_null(vma));
 
 		if (xe_vma_is_userptr(vma))
 			continue;
--- a/drivers/gpu/drm/xe/xe_execlist.c
+++ b/drivers/gpu/drm/xe/xe_execlist.c
@@ -12,6 +12,7 @@
 #include "regs/xe_gt_regs.h"
 #include "regs/xe_lrc_layout.h"
 #include "regs/xe_regs.h"
+#include "xe_assert.h"
 #include "xe_bo.h"
 #include "xe_device.h"
 #include "xe_exec_queue.h"
@@ -50,10 +51,10 @@
 	lrc_desc = xe_lrc_descriptor(lrc);
 
 	if (GRAPHICS_VERx100(xe) >= 1250) {
-		XE_WARN_ON(!FIELD_FIT(XEHP_SW_CTX_ID, ctx_id));
+		xe_gt_assert(hwe->gt, FIELD_FIT(XEHP_SW_CTX_ID, ctx_id));
 		lrc_desc |= FIELD_PREP(XEHP_SW_CTX_ID, ctx_id);
 	} else {
-		XE_WARN_ON(!FIELD_FIT(GEN11_SW_CTX_ID, ctx_id));
+		xe_gt_assert(hwe->gt, FIELD_FIT(GEN11_SW_CTX_ID, ctx_id));
 		lrc_desc |= FIELD_PREP(GEN11_SW_CTX_ID, ctx_id);
 	}
 
@@ -321,7 +322,7 @@
 	struct xe_device *xe = gt_to_xe(q->gt);
 	int err;
 
-	XE_WARN_ON(xe_device_guc_submission_enabled(xe));
+	xe_assert(xe, !xe_device_guc_submission_enabled(xe));
 
 	drm_info(&xe->drm, "Enabling execlist submission (GuC submission disabled)\n");
 
@@ -367,9 +368,10 @@
 		container_of(w, struct xe_execlist_exec_queue, fini_async);
 	struct xe_exec_queue *q = ee->q;
 	struct xe_execlist_exec_queue *exl = q->execlist;
+	struct xe_device *xe = gt_to_xe(q->gt);
 	unsigned long flags;
 
-	XE_WARN_ON(xe_device_guc_submission_enabled(gt_to_xe(q->gt)));
+	xe_assert(xe, !xe_device_guc_submission_enabled(xe));
 
 	spin_lock_irqsave(&exl->port->lock, flags);
 	if (WARN_ON(exl->active_priority != XE_EXEC_QUEUE_PRIORITY_UNSET))
@@ -377,7 +379,7 @@
 	spin_unlock_irqrestore(&exl->port->lock, flags);
 
 	if (q->flags & EXEC_QUEUE_FLAG_PERSISTENT)
-		xe_device_remove_persistent_exec_queues(gt_to_xe(q->gt), q);
+		xe_device_remove_persistent_exec_queues(xe, q);
 	drm_sched_entity_fini(&exl->entity);
 	drm_sched_fini(&exl->sched);
 	kfree(exl);
--- a/drivers/gpu/drm/xe/xe_force_wake.c
+++ b/drivers/gpu/drm/xe/xe_force_wake.c
@@ -45,7 +45,7 @@
 	mutex_init(&fw->lock);
 
 	/* Assuming gen11+ so assert this assumption is correct */
-	XE_WARN_ON(GRAPHICS_VER(gt_to_xe(gt)) < 11);
+	xe_gt_assert(gt, GRAPHICS_VER(gt_to_xe(gt)) >= 11);
 
 	if (xe->info.graphics_verx100 >= 1270) {
 		domain_init(&fw->domains[XE_FW_DOMAIN_ID_GT],
@@ -67,7 +67,7 @@
 	int i, j;
 
 	/* Assuming gen11+ so assert this assumption is correct */
-	XE_WARN_ON(GRAPHICS_VER(gt_to_xe(gt)) < 11);
+	xe_gt_assert(gt, GRAPHICS_VER(gt_to_xe(gt)) >= 11);
 
 	if (!xe_gt_is_media_type(gt))
 		domain_init(&fw->domains[XE_FW_DOMAIN_ID_RENDER],
--- a/drivers/gpu/drm/xe/xe_force_wake.h
+++ b/drivers/gpu/drm/xe/xe_force_wake.h
@@ -6,8 +6,8 @@
 #ifndef _XE_FORCE_WAKE_H_
 #define _XE_FORCE_WAKE_H_
 
+#include "xe_assert.h"
 #include "xe_force_wake_types.h"
-#include "xe_macros.h"
 
 struct xe_gt;
 
@@ -24,7 +24,7 @@
 xe_force_wake_ref(struct xe_force_wake *fw,
 		  enum xe_force_wake_domains domain)
 {
-	XE_WARN_ON(!domain);
+	xe_gt_assert(fw->gt, domain);
 	return fw->domains[ffs(domain) - 1].ref;
 }
 
@@ -32,7 +32,7 @@
 xe_force_wake_assert_held(struct xe_force_wake *fw,
 			  enum xe_force_wake_domains domain)
 {
-	XE_WARN_ON(!(fw->awake_domains & domain));
+	xe_gt_assert(fw->gt, fw->awake_domains & domain);
 }
 
 #endif
--- a/drivers/gpu/drm/xe/xe_ggtt.c
+++ b/drivers/gpu/drm/xe/xe_ggtt.c
@@ -58,8 +58,8 @@
 
 void xe_ggtt_set_pte(struct xe_ggtt *ggtt, u64 addr, u64 pte)
 {
-	XE_WARN_ON(addr & XE_PTE_MASK);
-	XE_WARN_ON(addr >= ggtt->size);
+	xe_tile_assert(ggtt->tile, !(addr & XE_PTE_MASK));
+	xe_tile_assert(ggtt->tile, addr < ggtt->size);
 
 	writeq(pte, &ggtt->gsm[addr >> XE_PTE_SHIFT]);
 }
@@ -69,7 +69,7 @@
 	u64 end = start + size - 1;
 	u64 scratch_pte;
 
-	XE_WARN_ON(start >= end);
+	xe_tile_assert(ggtt->tile, start < end);
 
 	if (ggtt->scratch)
 		scratch_pte = xe_ggtt_pte_encode(ggtt->scratch, 0);
@@ -230,7 +230,7 @@
 		int seqno;
 
 		seqno = xe_gt_tlb_invalidation_guc(gt);
-		XE_WARN_ON(seqno <= 0);
+		xe_gt_assert(gt, seqno > 0);
 		if (seqno > 0)
 			xe_gt_tlb_invalidation_wait(gt, seqno);
 	} else if (xe_device_guc_submission_enabled(gt_to_xe(gt))) {
@@ -266,7 +266,7 @@
 	for (addr = 0; addr < ggtt->size; addr += XE_PAGE_SIZE) {
 		unsigned int i = addr / XE_PAGE_SIZE;
 
-		XE_WARN_ON(addr > U32_MAX);
+		xe_tile_assert(ggtt->tile, addr <= U32_MAX);
 		if (ggtt->gsm[i] == scratch_pte)
 			continue;
 
@@ -315,7 +315,7 @@
 
 	if (XE_WARN_ON(bo->ggtt_node.size)) {
 		/* Someone's already inserted this BO in the GGTT */
-		XE_WARN_ON(bo->ggtt_node.size != bo->size);
+		xe_tile_assert(ggtt->tile, bo->ggtt_node.size == bo->size);
 		return 0;
 	}
 
@@ -378,7 +378,7 @@
 		return;
 
 	/* This BO is not currently in the GGTT */
-	XE_WARN_ON(bo->ggtt_node.size != bo->size);
+	xe_tile_assert(ggtt->tile, bo->ggtt_node.size == bo->size);
 
 	xe_ggtt_remove_node(ggtt, &bo->ggtt_node);
 }
--- a/drivers/gpu/drm/xe/xe_gt.c
+++ b/drivers/gpu/drm/xe/xe_gt.c
@@ -11,6 +11,7 @@
 #include <drm/xe_drm.h>
 
 #include "regs/xe_gt_regs.h"
+#include "xe_assert.h"
 #include "xe_bb.h"
 #include "xe_bo.h"
 #include "xe_device.h"
--- a/drivers/gpu/drm/xe/xe_gt_clock.c
+++ b/drivers/gpu/drm/xe/xe_gt_clock.c
@@ -58,7 +58,7 @@
 	u32 freq = 0;
 
 	/* Assuming gen11+ so assert this assumption is correct */
-	XE_WARN_ON(GRAPHICS_VER(gt_to_xe(gt)) < 11);
+	xe_gt_assert(gt, GRAPHICS_VER(gt_to_xe(gt)) >= 11);
 
 	if (ctc_reg & CTC_SOURCE_DIVIDE_LOGIC) {
 		freq = read_reference_ts_freq(gt);
--- a/drivers/gpu/drm/xe/xe_gt_debugfs.c
+++ b/drivers/gpu/drm/xe/xe_gt_debugfs.c
@@ -158,7 +158,7 @@
 	char name[8];
 	int i;
 
-	XE_WARN_ON(!minor->debugfs_root);
+	xe_gt_assert(gt, minor->debugfs_root);
 
 	sprintf(name, "gt%d", gt->info.id);
 	root = debugfs_create_dir(name, minor->debugfs_root);
--- a/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.c
+++ b/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.c
@@ -250,7 +250,7 @@
 	u32 action[MAX_TLB_INVALIDATION_LEN];
 	int len = 0;
 
-	XE_WARN_ON(!vma);
+	xe_gt_assert(gt, vma);
 
 	action[len++] = XE_GUC_ACTION_TLB_INVALIDATION;
 	action[len++] = 0; /* seqno, replaced in send_tlb_invalidation */
@@ -288,10 +288,10 @@
 			start = ALIGN_DOWN(xe_vma_start(vma), length);
 		}
 
-		XE_WARN_ON(length < SZ_4K);
-		XE_WARN_ON(!is_power_of_2(length));
-		XE_WARN_ON(length & GENMASK(ilog2(SZ_16M) - 1, ilog2(SZ_2M) + 1));
-		XE_WARN_ON(!IS_ALIGNED(start, length));
+		xe_gt_assert(gt, length >= SZ_4K);
+		xe_gt_assert(gt, is_power_of_2(length));
+		xe_gt_assert(gt, !(length & GENMASK(ilog2(SZ_16M) - 1, ilog2(SZ_2M) + 1)));
+		xe_gt_assert(gt, IS_ALIGNED(start, length));
 
 		action[len++] = MAKE_INVAL_OP(XE_GUC_TLB_INVAL_PAGE_SELECTIVE);
 		action[len++] = xe_vma_vm(vma)->usm.asid;
@@ -300,7 +300,7 @@
 		action[len++] = ilog2(length) - ilog2(SZ_4K);
 	}
 
-	XE_WARN_ON(len > MAX_TLB_INVALIDATION_LEN);
+	xe_gt_assert(gt, len <= MAX_TLB_INVALIDATION_LEN);
 
 	return send_tlb_invalidation(&gt->uc.guc, fence, action, len);
 }
--- a/drivers/gpu/drm/xe/xe_guc.c
+++ b/drivers/gpu/drm/xe/xe_guc.c
@@ -44,11 +44,12 @@
 static u32 guc_bo_ggtt_addr(struct xe_guc *guc,
 			    struct xe_bo *bo)
 {
+	struct xe_device *xe = guc_to_xe(guc);
 	u32 addr = xe_bo_ggtt_addr(bo);
 
-	XE_WARN_ON(addr < xe_wopcm_size(guc_to_xe(guc)));
-	XE_WARN_ON(addr >= GUC_GGTT_TOP);
-	XE_WARN_ON(bo->size > GUC_GGTT_TOP - addr);
+	xe_assert(xe, addr >= xe_wopcm_size(guc_to_xe(guc)));
+	xe_assert(xe, addr < GUC_GGTT_TOP);
+	xe_assert(xe, bo->size <= GUC_GGTT_TOP - addr);
 
 	return addr;
 }
@@ -629,13 +630,13 @@
 
 	BUILD_BUG_ON(VF_SW_FLAG_COUNT != MED_VF_SW_FLAG_COUNT);
 
-	XE_WARN_ON(guc->ct.enabled);
-	XE_WARN_ON(!len);
-	XE_WARN_ON(len > VF_SW_FLAG_COUNT);
-	XE_WARN_ON(len > MED_VF_SW_FLAG_COUNT);
-	XE_WARN_ON(FIELD_GET(GUC_HXG_MSG_0_ORIGIN, request[0]) !=
+	xe_assert(xe, !guc->ct.enabled);
+	xe_assert(xe, len);
+	xe_assert(xe, len <= VF_SW_FLAG_COUNT);
+	xe_assert(xe, len <= MED_VF_SW_FLAG_COUNT);
+	xe_assert(xe, FIELD_GET(GUC_HXG_MSG_0_ORIGIN, request[0]) ==
 		  GUC_HXG_ORIGIN_HOST);
-	XE_WARN_ON(FIELD_GET(GUC_HXG_MSG_0_TYPE, request[0]) !=
+	xe_assert(xe, FIELD_GET(GUC_HXG_MSG_0_TYPE, request[0]) ==
 		  GUC_HXG_TYPE_REQUEST);
 
 retry:
@@ -727,6 +728,7 @@
 
 static int guc_self_cfg(struct xe_guc *guc, u16 key, u16 len, u64 val)
 {
+	struct xe_device *xe = guc_to_xe(guc);
 	u32 request[HOST2GUC_SELF_CFG_REQUEST_MSG_LEN] = {
 		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
 		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
@@ -741,8 +743,8 @@
 	};
 	int ret;
 
-	XE_WARN_ON(len > 2);
-	XE_WARN_ON(len == 1 && upper_32_bits(val));
+	xe_assert(xe, len <= 2);
+	xe_assert(xe, len != 1 || !upper_32_bits(val));
 
 	/* Self config must go over MMIO */
 	ret = xe_guc_mmio_send(guc, request, ARRAY_SIZE(request));
--- a/drivers/gpu/drm/xe/xe_guc_ads.c
+++ b/drivers/gpu/drm/xe/xe_guc_ads.c
@@ -118,7 +118,9 @@
 
 static size_t guc_ads_regset_size(struct xe_guc_ads *ads)
 {
-	XE_WARN_ON(!ads->regset_size);
+	struct xe_device *xe = ads_to_xe(ads);
+
+	xe_assert(xe, ads->regset_size);
 
 	return ads->regset_size;
 }
@@ -309,14 +311,14 @@
 	struct xe_gt *gt = ads_to_gt(ads);
 	u32 prev_regset_size = ads->regset_size;
 
-	XE_WARN_ON(!ads->bo);
+	xe_gt_assert(gt, ads->bo);
 
 	ads->golden_lrc_size = calculate_golden_lrc_size(ads);
 	ads->regset_size = calculate_regset_size(gt);
 
-	XE_WARN_ON(ads->golden_lrc_size +
-		   (ads->regset_size - prev_regset_size) >
-		   MAX_GOLDEN_LRC_SIZE);
+	xe_gt_assert(gt, ads->golden_lrc_size +
+		     (ads->regset_size - prev_regset_size) <=
+		     MAX_GOLDEN_LRC_SIZE);
 
 	return 0;
 }
@@ -517,7 +519,7 @@
 		regset_used += count * sizeof(struct guc_mmio_reg);
 	}
 
-	XE_WARN_ON(regset_used > ads->regset_size);
+	xe_gt_assert(gt, regset_used <= ads->regset_size);
 }
 
 static void guc_um_init_params(struct xe_guc_ads *ads)
@@ -572,7 +574,7 @@
 			offsetof(struct __guc_ads_blob, system_info));
 	u32 base = xe_bo_ggtt_addr(ads->bo);
 
-	XE_WARN_ON(!ads->bo);
+	xe_gt_assert(gt, ads->bo);
 
 	xe_map_memset(ads_to_xe(ads), ads_to_map(ads), 0, 0, ads->bo->size);
 	guc_policies_init(ads);
@@ -596,7 +598,7 @@
 			offsetof(struct __guc_ads_blob, system_info));
 	u32 base = xe_bo_ggtt_addr(ads->bo);
 
-	XE_WARN_ON(!ads->bo);
+	xe_gt_assert(gt, ads->bo);
 
 	xe_map_memset(ads_to_xe(ads), ads_to_map(ads), 0, 0, ads->bo->size);
 	guc_policies_init(ads);
@@ -643,7 +645,7 @@
 				   engine_enabled_masks[guc_class]))
 			continue;
 
-		XE_WARN_ON(!gt->default_lrc[class]);
+		xe_gt_assert(gt, gt->default_lrc[class]);
 
 		real_size = xe_lrc_size(xe, class);
 		alloc_size = PAGE_ALIGN(real_size);
@@ -672,7 +674,7 @@
 		offset += alloc_size;
 	}
 
-	XE_WARN_ON(total_size != ads->golden_lrc_size);
+	xe_gt_assert(gt, total_size == ads->golden_lrc_size);
 }
 
 void xe_guc_ads_populate_post_load(struct xe_guc_ads *ads)
--- a/drivers/gpu/drm/xe/xe_guc_ct.c
+++ b/drivers/gpu/drm/xe/xe_guc_ct.c
@@ -135,7 +135,7 @@
 	struct xe_bo *bo;
 	int err;
 
-	XE_WARN_ON(guc_ct_size() % PAGE_SIZE);
+	xe_assert(xe, !(guc_ct_size() % PAGE_SIZE));
 
 	mutex_init(&ct->lock);
 	spin_lock_init(&ct->fast_lock);
@@ -283,7 +283,7 @@
 	struct xe_device *xe = ct_to_xe(ct);
 	int err;
 
-	XE_WARN_ON(ct->enabled);
+	xe_assert(xe, !ct->enabled);
 
 	guc_ct_ctb_h2g_init(xe, &ct->ctbs.h2g, &ct->bo->vmap);
 	guc_ct_ctb_g2h_init(xe, &ct->ctbs.g2h, &ct->bo->vmap);
@@ -376,7 +376,7 @@
 
 static void __g2h_reserve_space(struct xe_guc_ct *ct, u32 g2h_len, u32 num_g2h)
 {
-	XE_WARN_ON(g2h_len > ct->ctbs.g2h.info.space);
+	xe_assert(ct_to_xe(ct), g2h_len <= ct->ctbs.g2h.info.space);
 
 	if (g2h_len) {
 		lockdep_assert_held(&ct->fast_lock);
@@ -389,8 +389,8 @@
 static void __g2h_release_space(struct xe_guc_ct *ct, u32 g2h_len)
 {
 	lockdep_assert_held(&ct->fast_lock);
-	XE_WARN_ON(ct->ctbs.g2h.info.space + g2h_len >
-		   ct->ctbs.g2h.info.size - ct->ctbs.g2h.info.resv_space);
+	xe_assert(ct_to_xe(ct), ct->ctbs.g2h.info.space + g2h_len <=
+		  ct->ctbs.g2h.info.size - ct->ctbs.g2h.info.resv_space);
 
 	ct->ctbs.g2h.info.space += g2h_len;
 	--ct->g2h_outstanding;
@@ -419,8 +419,8 @@
 	full_len = len + GUC_CTB_HDR_LEN;
 
 	lockdep_assert_held(&ct->lock);
-	XE_WARN_ON(full_len > (GUC_CTB_MSG_MAX_LEN - GUC_CTB_HDR_LEN));
-	XE_WARN_ON(tail > h2g->info.size);
+	xe_assert(xe, full_len <= (GUC_CTB_MSG_MAX_LEN - GUC_CTB_HDR_LEN));
+	xe_assert(xe, tail <= h2g->info.size);
 
 	/* Command will wrap, zero fill (NOPs), return and check credits again */
 	if (tail + full_len > h2g->info.size) {
@@ -476,12 +476,13 @@
 				u32 len, u32 g2h_len, u32 num_g2h,
 				struct g2h_fence *g2h_fence)
 {
+	struct xe_device *xe = ct_to_xe(ct);
 	int ret;
 
-	XE_WARN_ON(g2h_len && g2h_fence);
-	XE_WARN_ON(num_g2h && g2h_fence);
-	XE_WARN_ON(g2h_len && !num_g2h);
-	XE_WARN_ON(!g2h_len && num_g2h);
+	xe_assert(xe, !g2h_len || !g2h_fence);
+	xe_assert(xe, !num_g2h || !g2h_fence);
+	xe_assert(xe, !g2h_len || num_g2h);
+	xe_assert(xe, g2h_len || !num_g2h);
 	lockdep_assert_held(&ct->lock);
 
 	if (unlikely(ct->ctbs.h2g.info.broken)) {
@@ -552,7 +553,7 @@
 	unsigned int sleep_period_ms = 1;
 	int ret;
 
-	XE_WARN_ON(g2h_len && g2h_fence);
+	xe_assert(ct_to_xe(ct), !g2h_len || !g2h_fence);
 	lockdep_assert_held(&ct->lock);
 	xe_device_assert_mem_access(ct_to_xe(ct));
 
@@ -622,7 +623,7 @@
 {
 	int ret;
 
-	XE_WARN_ON(g2h_len && g2h_fence);
+	xe_assert(ct_to_xe(ct), !g2h_len || !g2h_fence);
 
 	mutex_lock(&ct->lock);
 	ret = guc_ct_send_locked(ct, action, len, g2h_len, num_g2h, g2h_fence);
@@ -798,7 +799,7 @@
 		return 0;
 	}
 
-	XE_WARN_ON(fence != g2h_fence->seqno);
+	xe_assert(xe, fence == g2h_fence->seqno);
 
 	if (type == GUC_HXG_TYPE_RESPONSE_FAILURE) {
 		g2h_fence->fail = true;
--- a/drivers/gpu/drm/xe/xe_guc_log.c
+++ b/drivers/gpu/drm/xe/xe_guc_log.c
@@ -55,12 +55,12 @@
 	size_t size;
 	int i, j;
 
-	XE_WARN_ON(!log->bo);
+	xe_assert(xe, log->bo);
 
 	size = log->bo->size;
 
 #define DW_PER_READ		128
-	XE_WARN_ON(size % (DW_PER_READ * sizeof(u32)));
+	xe_assert(xe, !(size % (DW_PER_READ * sizeof(u32))));
 	for (i = 0; i < size / sizeof(u32); i += DW_PER_READ) {
 		u32 read[DW_PER_READ];
 
--- a/drivers/gpu/drm/xe/xe_guc_pc.c
+++ b/drivers/gpu/drm/xe/xe_guc_pc.c
@@ -816,7 +816,7 @@
 	u32 size = PAGE_ALIGN(sizeof(struct slpc_shared_data));
 	int ret;
 
-	XE_WARN_ON(!xe_device_guc_submission_enabled(xe));
+	xe_gt_assert(gt, xe_device_guc_submission_enabled(xe));
 
 	xe_device_mem_access_get(pc_to_xe(pc));
 
--- a/drivers/gpu/drm/xe/xe_guc_submit.c
+++ b/drivers/gpu/drm/xe/xe_guc_submit.c
@@ -14,6 +14,7 @@
 #include <drm/drm_managed.h>
 
 #include "regs/xe_lrc_layout.h"
+#include "xe_assert.h"
 #include "xe_devcoredump.h"
 #include "xe_device.h"
 #include "xe_exec_queue.h"
@@ -354,11 +355,12 @@
 static void init_policies(struct xe_guc *guc, struct xe_exec_queue *q)
 {
 	struct exec_queue_policy policy;
+	struct xe_device *xe = guc_to_xe(guc);
 	enum xe_exec_queue_priority prio = q->priority;
 	u32 timeslice_us = q->sched_props.timeslice_us;
 	u32 preempt_timeout_us = q->sched_props.preempt_timeout_us;
 
-	XE_WARN_ON(!exec_queue_registered(q));
+	xe_assert(xe, exec_queue_registered(q));
 
 	__guc_exec_queue_policy_start_klv(&policy, q->guc->id);
 	__guc_exec_queue_policy_add_priority(&policy, xe_exec_queue_prio_to_guc[prio]);
@@ -392,11 +394,12 @@
 				   struct guc_ctxt_registration_info *info)
 {
 #define MAX_MLRC_REG_SIZE      (13 + XE_HW_ENGINE_MAX_INSTANCE * 2)
+	struct xe_device *xe = guc_to_xe(guc);
 	u32 action[MAX_MLRC_REG_SIZE];
 	int len = 0;
 	int i;
 
-	XE_WARN_ON(!xe_exec_queue_is_parallel(q));
+	xe_assert(xe, xe_exec_queue_is_parallel(q));
 
 	action[len++] = XE_GUC_ACTION_REGISTER_CONTEXT_MULTI_LRC;
 	action[len++] = info->flags;
@@ -419,7 +422,7 @@
 		action[len++] = upper_32_bits(xe_lrc_descriptor(lrc));
 	}
 
-	XE_WARN_ON(len > MAX_MLRC_REG_SIZE);
+	xe_assert(xe, len <= MAX_MLRC_REG_SIZE);
 #undef MAX_MLRC_REG_SIZE
 
 	xe_guc_ct_send(&guc->ct, action, len, 0, 0);
@@ -453,7 +456,7 @@
 	struct xe_lrc *lrc = q->lrc;
 	struct guc_ctxt_registration_info info;
 
-	XE_WARN_ON(exec_queue_registered(q));
+	xe_assert(xe, !exec_queue_registered(q));
 
 	memset(&info, 0, sizeof(info));
 	info.context_idx = q->guc->id;
@@ -543,7 +546,7 @@
 	if (wq_wait_for_space(q, wq_space_until_wrap(q)))
 		return -ENODEV;
 
-	XE_WARN_ON(!FIELD_FIT(WQ_LEN_MASK, len_dw));
+	xe_assert(xe, FIELD_FIT(WQ_LEN_MASK, len_dw));
 
 	parallel_write(xe, map, wq[q->guc->wqi_tail / sizeof(u32)],
 		       FIELD_PREP(WQ_TYPE_MASK, WQ_TYPE_NOOP) |
@@ -583,13 +586,13 @@
 		wqi[i++] = lrc->ring.tail / sizeof(u64);
 	}
 
-	XE_WARN_ON(i != wqi_size / sizeof(u32));
+	xe_assert(xe, i == wqi_size / sizeof(u32));
 
 	iosys_map_incr(&map, offsetof(struct guc_submit_parallel_scratch,
 				      wq[q->guc->wqi_tail / sizeof(u32)]));
 	xe_map_memcpy_to(xe, &map, 0, wqi, wqi_size);
 	q->guc->wqi_tail += wqi_size;
-	XE_WARN_ON(q->guc->wqi_tail > WQ_SIZE);
+	xe_assert(xe, q->guc->wqi_tail <= WQ_SIZE);
 
 	xe_device_wmb(xe);
 
@@ -601,6 +604,7 @@
 static void submit_exec_queue(struct xe_exec_queue *q)
 {
 	struct xe_guc *guc = exec_queue_to_guc(q);
+	struct xe_device *xe = guc_to_xe(guc);
 	struct xe_lrc *lrc = q->lrc;
 	u32 action[3];
 	u32 g2h_len = 0;
@@ -608,7 +612,7 @@
 	int len = 0;
 	bool extra_submit = false;
 
-	XE_WARN_ON(!exec_queue_registered(q));
+	xe_assert(xe, exec_queue_registered(q));
 
 	if (xe_exec_queue_is_parallel(q))
 		wq_item_append(q);
@@ -654,10 +658,12 @@
 {
 	struct xe_sched_job *job = to_xe_sched_job(drm_job);
 	struct xe_exec_queue *q = job->q;
+	struct xe_guc *guc = exec_queue_to_guc(q);
+	struct xe_device *xe = guc_to_xe(guc);
 	bool lr = xe_exec_queue_is_lr(q);
 
-	XE_WARN_ON((exec_queue_destroyed(q) || exec_queue_pending_disable(q)) &&
-		   !exec_queue_banned(q) && !exec_queue_suspended(q));
+	xe_assert(xe, !(exec_queue_destroyed(q) || exec_queue_pending_disable(q)) ||
+		  exec_queue_banned(q) || exec_queue_suspended(q));
 
 	trace_xe_sched_job_run(job);
 
@@ -799,7 +805,7 @@
 	struct xe_device *xe = guc_to_xe(guc);
 	struct xe_gpu_scheduler *sched = &ge->sched;
 
-	XE_WARN_ON(!xe_exec_queue_is_lr(q));
+	xe_assert(xe, xe_exec_queue_is_lr(q));
 	trace_xe_exec_queue_lr_cleanup(q);
 
 	/* Kill the run_job / process_msg entry points */
@@ -853,8 +859,8 @@
 	int i = 0;
 
 	if (!test_bit(DMA_FENCE_FLAG_SIGNALED_BIT, &job->fence->flags)) {
-		XE_WARN_ON(q->flags & EXEC_QUEUE_FLAG_KERNEL);
-		XE_WARN_ON(q->flags & EXEC_QUEUE_FLAG_VM && !exec_queue_killed(q));
+		xe_assert(xe, !(q->flags & EXEC_QUEUE_FLAG_KERNEL));
+		xe_assert(xe, !(q->flags & EXEC_QUEUE_FLAG_VM && !exec_queue_killed(q)));
 
 		drm_notice(&xe->drm, "Timedout job: seqno=%u, guc_id=%d, flags=0x%lx",
 			   xe_sched_job_seqno(job), q->guc->id, q->flags);
@@ -990,8 +996,9 @@
 {
 	struct xe_exec_queue *q = msg->private_data;
 	struct xe_guc *guc = exec_queue_to_guc(q);
+	struct xe_device *xe = guc_to_xe(guc);
 
-	XE_WARN_ON(q->flags & EXEC_QUEUE_FLAG_PERMANENT);
+	xe_assert(xe, !(q->flags & EXEC_QUEUE_FLAG_PERMANENT));
 	trace_xe_exec_queue_cleanup_entity(q);
 
 	if (exec_queue_registered(q))
@@ -1018,10 +1025,11 @@
 static void suspend_fence_signal(struct xe_exec_queue *q)
 {
 	struct xe_guc *guc = exec_queue_to_guc(q);
+	struct xe_device *xe = guc_to_xe(guc);
 
-	XE_WARN_ON(!exec_queue_suspended(q) && !exec_queue_killed(q) &&
-		   !guc_read_stopped(guc));
-	XE_WARN_ON(!q->guc->suspend_pending);
+	xe_assert(xe, exec_queue_suspended(q) || exec_queue_killed(q) ||
+		  guc_read_stopped(guc));
+	xe_assert(xe, q->guc->suspend_pending);
 
 	q->guc->suspend_pending = false;
 	smp_wmb();
@@ -1125,11 +1133,12 @@
 {
 	struct xe_gpu_scheduler *sched;
 	struct xe_guc *guc = exec_queue_to_guc(q);
+	struct xe_device *xe = guc_to_xe(guc);
 	struct xe_guc_exec_queue *ge;
 	long timeout;
 	int err;
 
-	XE_WARN_ON(!xe_device_guc_submission_enabled(guc_to_xe(guc)));
+	xe_assert(xe, xe_device_guc_submission_enabled(guc_to_xe(guc)));
 
 	ge = kzalloc(sizeof(*ge), GFP_KERNEL);
 	if (!ge)
@@ -1275,10 +1284,12 @@
 static int guc_exec_queue_set_job_timeout(struct xe_exec_queue *q, u32 job_timeout_ms)
 {
 	struct xe_gpu_scheduler *sched = &q->guc->sched;
+	struct xe_guc *guc = exec_queue_to_guc(q);
+	struct xe_device *xe = guc_to_xe(guc);
 
-	XE_WARN_ON(exec_queue_registered(q));
-	XE_WARN_ON(exec_queue_banned(q));
-	XE_WARN_ON(exec_queue_killed(q));
+	xe_assert(xe, !exec_queue_registered(q));
+	xe_assert(xe, !exec_queue_banned(q));
+	xe_assert(xe, !exec_queue_killed(q));
 
 	sched->base.timeout = job_timeout_ms;
 
@@ -1309,8 +1320,10 @@
 static void guc_exec_queue_resume(struct xe_exec_queue *q)
 {
 	struct xe_sched_msg *msg = q->guc->static_msgs + STATIC_MSG_RESUME;
+	struct xe_guc *guc = exec_queue_to_guc(q);
+	struct xe_device *xe = guc_to_xe(guc);
 
-	XE_WARN_ON(q->guc->suspend_pending);
+	xe_assert(xe, !q->guc->suspend_pending);
 
 	guc_exec_queue_add_msg(q, msg, RESUME);
 }
@@ -1405,8 +1418,9 @@
 {
 	struct xe_exec_queue *q;
 	unsigned long index;
+	struct xe_device *xe = guc_to_xe(guc);
 
-	XE_WARN_ON(guc_read_stopped(guc) != 1);
+	xe_assert(xe, guc_read_stopped(guc) == 1);
 
 	mutex_lock(&guc->submission_state.lock);
 
@@ -1443,8 +1457,9 @@
 {
 	struct xe_exec_queue *q;
 	unsigned long index;
+	struct xe_device *xe = guc_to_xe(guc);
 
-	XE_WARN_ON(guc_read_stopped(guc) != 1);
+	xe_assert(xe, guc_read_stopped(guc) == 1);
 
 	mutex_lock(&guc->submission_state.lock);
 	atomic_dec(&guc->submission_state.stopped);
@@ -1474,7 +1489,7 @@
 		return NULL;
 	}
 
-	XE_WARN_ON(q->guc->id != guc_id);
+	xe_assert(xe, q->guc->id == guc_id);
 
 	return q;
 }
--- a/drivers/gpu/drm/xe/xe_huc.c
+++ b/drivers/gpu/drm/xe/xe_huc.c
@@ -6,6 +6,7 @@
 #include "xe_huc.h"
 
 #include "regs/xe_guc_regs.h"
+#include "xe_assert.h"
 #include "xe_bo.h"
 #include "xe_device.h"
 #include "xe_force_wake.h"
@@ -72,7 +73,7 @@
 	if (xe_uc_fw_is_disabled(&huc->fw))
 		return 0;
 
-	XE_WARN_ON(xe_uc_fw_is_running(&huc->fw));
+	xe_assert(xe, !xe_uc_fw_is_running(&huc->fw));
 
 	if (!xe_uc_fw_is_loaded(&huc->fw))
 		return -ENOEXEC;
--- a/drivers/gpu/drm/xe/xe_hw_engine.c
+++ b/drivers/gpu/drm/xe/xe_hw_engine.c
@@ -10,6 +10,7 @@
 #include "regs/xe_engine_regs.h"
 #include "regs/xe_gt_regs.h"
 #include "regs/xe_regs.h"
+#include "xe_assert.h"
 #include "xe_bo.h"
 #include "xe_device.h"
 #include "xe_execlist.h"
@@ -244,7 +245,7 @@
 static void hw_engine_mmio_write32(struct xe_hw_engine *hwe, struct xe_reg reg,
 				   u32 val)
 {
-	XE_WARN_ON(reg.addr & hwe->mmio_base);
+	xe_gt_assert(hwe->gt, !(reg.addr & hwe->mmio_base));
 	xe_force_wake_assert_held(gt_to_fw(hwe->gt), hwe->domain);
 
 	reg.addr += hwe->mmio_base;
@@ -254,7 +255,7 @@
 
 static u32 hw_engine_mmio_read32(struct xe_hw_engine *hwe, struct xe_reg reg)
 {
-	XE_WARN_ON(reg.addr & hwe->mmio_base);
+	xe_gt_assert(hwe->gt, !(reg.addr & hwe->mmio_base));
 	xe_force_wake_assert_held(gt_to_fw(hwe->gt), hwe->domain);
 
 	reg.addr += hwe->mmio_base;
@@ -374,7 +375,7 @@
 
 	info = &engine_infos[id];
 
-	XE_WARN_ON(hwe->gt);
+	xe_gt_assert(gt, !hwe->gt);
 
 	hwe->gt = gt;
 	hwe->class = info->class;
@@ -415,8 +416,8 @@
 	struct xe_tile *tile = gt_to_tile(gt);
 	int err;
 
-	XE_WARN_ON(id >= ARRAY_SIZE(engine_infos) || !engine_infos[id].name);
-	XE_WARN_ON(!(gt->info.engine_mask & BIT(id)));
+	xe_gt_assert(gt, id < ARRAY_SIZE(engine_infos) && engine_infos[id].name);
+	xe_gt_assert(gt, gt->info.engine_mask & BIT(id));
 
 	xe_reg_sr_apply_mmio(&hwe->reg_sr, gt);
 	xe_reg_sr_apply_whitelist(hwe);
--- a/drivers/gpu/drm/xe/xe_lrc.c
+++ b/drivers/gpu/drm/xe/xe_lrc.c
@@ -116,7 +116,7 @@
 		*regs |= MI_LRI_LRM_CS_MMIO;
 		regs++;
 
-		XE_WARN_ON(!count);
+		xe_gt_assert(hwe->gt, count);
 		do {
 			u32 offset = 0;
 			u8 v;
@@ -608,7 +608,7 @@
 { \
 	struct iosys_map map = lrc->bo->vmap; \
 \
-	XE_WARN_ON(iosys_map_is_null(&map)); \
+	xe_assert(lrc_to_xe(lrc), !iosys_map_is_null(&map));  \
 	iosys_map_incr(&map, __xe_lrc_##elem##_offset(lrc)); \
 	return map; \
 } \
@@ -827,16 +827,17 @@
 
 void xe_lrc_write_ring(struct xe_lrc *lrc, const void *data, size_t size)
 {
+	struct xe_device *xe = lrc_to_xe(lrc);
 	struct iosys_map ring;
 	u32 rhs;
 	size_t aligned_size;
 
-	XE_WARN_ON(!IS_ALIGNED(size, 4));
+	xe_assert(xe, IS_ALIGNED(size, 4));
 	aligned_size = ALIGN(size, 8);
 
 	ring = __xe_lrc_ring_map(lrc);
 
-	XE_WARN_ON(lrc->ring.tail >= lrc->ring.size);
+	xe_assert(xe, lrc->ring.tail < lrc->ring.size);
 	rhs = lrc->ring.size - lrc->ring.tail;
 	if (size > rhs) {
 		__xe_lrc_write_ring(lrc, ring, data, rhs);
--- a/drivers/gpu/drm/xe/xe_migrate.c
+++ b/drivers/gpu/drm/xe/xe_migrate.c
@@ -15,6 +15,7 @@
 #include "generated/xe_wa_oob.h"
 #include "regs/xe_gpu_commands.h"
 #include "tests/xe_test.h"
+#include "xe_assert.h"
 #include "xe_bb.h"
 #include "xe_bo.h"
 #include "xe_exec_queue.h"
@@ -172,7 +173,7 @@
 	BUILD_BUG_ON(!(NUM_KERNEL_PDE & 1));
 
 	/* Need to be sure everything fits in the first PT, or create more */
-	XE_WARN_ON(m->batch_base_ofs + batch->size >= SZ_2M);
+	xe_tile_assert(tile, m->batch_base_ofs + batch->size < SZ_2M);
 
 	bo = xe_bo_create_pin_map(vm->xe, tile, vm,
 				  num_entries * XE_PAGE_SIZE,
@@ -206,7 +207,7 @@
 	}
 
 	if (!IS_DGFX(xe)) {
-		XE_WARN_ON(xe->info.supports_usm);
+		xe_tile_assert(tile, !xe->info.supports_usm);
 
 		/* Write out batch too */
 		m->batch_base_ofs = NUM_PT_SLOTS * XE_PAGE_SIZE;
@@ -487,7 +488,7 @@
 				/* Is this a 64K PTE entry? */
 				if ((m->q->vm->flags & XE_VM_FLAG_64K) &&
 				    !(cur_ofs & (16 * 8 - 1))) {
-					XE_WARN_ON(!IS_ALIGNED(addr, SZ_64K));
+					xe_tile_assert(m->tile, IS_ALIGNED(addr, SZ_64K));
 					addr |= XE_PTE_PS64;
 				}
 
@@ -516,7 +517,7 @@
 
 	num_ccs_blks = DIV_ROUND_UP(xe_device_ccs_bytes(gt_to_xe(gt), size),
 				    NUM_CCS_BYTES_PER_BLOCK);
-	XE_WARN_ON(num_ccs_blks > NUM_CCS_BLKS_PER_XFER);
+	xe_gt_assert(gt, num_ccs_blks <= NUM_CCS_BLKS_PER_XFER);
 	*cs++ = XY_CTRL_SURF_COPY_BLT |
 		(src_is_indirect ? 0x0 : 0x1) << SRC_ACCESS_TYPE_SHIFT |
 		(dst_is_indirect ? 0x0 : 0x1) << DST_ACCESS_TYPE_SHIFT |
@@ -536,9 +537,9 @@
 		      u64 src_ofs, u64 dst_ofs, unsigned int size,
 		      unsigned int pitch)
 {
-	XE_WARN_ON(size / pitch > S16_MAX);
-	XE_WARN_ON(pitch / 4 > S16_MAX);
-	XE_WARN_ON(pitch > U16_MAX);
+	xe_gt_assert(gt, size / pitch <= S16_MAX);
+	xe_gt_assert(gt, pitch / 4 <= S16_MAX);
+	xe_gt_assert(gt, pitch <= U16_MAX);
 
 	bb->cs[bb->len++] = XY_FAST_COPY_BLT_CMD | (10 - 2);
 	bb->cs[bb->len++] = XY_FAST_COPY_BLT_DEPTH_32 | pitch;
@@ -598,7 +599,7 @@
 		 * At the moment, we don't support copying CCS metadata from
 		 * system to system.
 		 */
-		XE_WARN_ON(!src_is_vram && !dst_is_vram);
+		xe_gt_assert(gt, src_is_vram || dst_is_vram);
 
 		emit_copy_ccs(gt, bb, dst_ofs, dst_is_vram, src_ofs,
 			      src_is_vram, dst_size);
@@ -810,7 +811,7 @@
 	*cs++ = upper_32_bits(src_ofs);
 	*cs++ = FIELD_PREP(PVC_MS_MOCS_INDEX_MASK, mocs);
 
-	XE_WARN_ON(cs - bb->cs != len + bb->len);
+	xe_gt_assert(gt, cs - bb->cs == len + bb->len);
 
 	bb->len += len;
 }
@@ -848,7 +849,7 @@
 		*cs++ = 0;
 	}
 
-	XE_WARN_ON(cs - bb->cs != len + bb->len);
+	xe_gt_assert(gt, cs - bb->cs == len + bb->len);
 
 	bb->len += len;
 }
@@ -1021,9 +1022,9 @@
 	 * PDE. This requires a BO that is almost vm->size big.
 	 *
 	 * This shouldn't be possible in practice.. might change when 16K
-	 * pages are used. Hence the XE_WARN_ON.
+	 * pages are used. Hence the assert.
 	 */
-	XE_WARN_ON(update->qwords > 0x1ff);
+	xe_tile_assert(tile, update->qwords <= 0x1ff);
 	if (!ppgtt_ofs) {
 		ppgtt_ofs = xe_migrate_vram_ofs(xe_bo_addr(update->pt_bo, 0,
 							   XE_PAGE_SIZE));
@@ -1213,7 +1214,7 @@
 	 * Worst case: Sum(2 * (each lower level page size) + (top level page size))
 	 * Should be reasonably bound..
 	 */
-	XE_WARN_ON(batch_size >= SZ_128K);
+	xe_tile_assert(tile, batch_size < SZ_128K);
 
 	bb = xe_bb_new(gt, batch_size, !q && xe->info.supports_usm);
 	if (IS_ERR(bb))
@@ -1223,7 +1224,7 @@
 	if (!IS_DGFX(xe)) {
 		ppgtt_ofs = NUM_KERNEL_PDE - 1;
 		if (q) {
-			XE_WARN_ON(num_updates > NUM_VMUSA_WRITES_PER_UNIT);
+			xe_tile_assert(tile, num_updates <= NUM_VMUSA_WRITES_PER_UNIT);
 
 			sa_bo = drm_suballoc_new(&m->vm_update_sa, 1,
 						 GFP_KERNEL, true, 0);
@@ -1252,7 +1253,7 @@
 		for (i = 0; i < num_updates; i++) {
 			struct xe_bo *pt_bo = updates[i].pt_bo;
 
-			XE_WARN_ON(pt_bo->size != SZ_4K);
+			xe_tile_assert(tile, pt_bo->size == SZ_4K);
 
 			addr = xe_pte_encode(pt_bo, 0, XE_CACHE_WB, 0);
 			bb->cs[bb->len++] = lower_32_bits(addr);
--- a/drivers/gpu/drm/xe/xe_mocs.c
+++ b/drivers/gpu/drm/xe/xe_mocs.c
@@ -463,7 +463,7 @@
 	 * is still 0 at this point, we'll assume that it was omitted by
 	 * mistake in the switch statement above.
 	 */
-	XE_WARN_ON(info->unused_entries_index == 0);
+	xe_assert(xe, info->unused_entries_index != 0);
 
 	if (XE_WARN_ON(info->size > info->n_entries)) {
 		info->table = NULL;
--- a/drivers/gpu/drm/xe/xe_pt.c
+++ b/drivers/gpu/drm/xe/xe_pt.c
@@ -196,7 +196,7 @@
 	pt->level = level;
 	pt->base.dir = level ? &as_xe_pt_dir(pt)->dir : NULL;
 
-	XE_WARN_ON(level > XE_VM_MAX_LEVEL);
+	xe_tile_assert(tile, level <= XE_VM_MAX_LEVEL);
 
 	return pt;
 
@@ -1004,7 +1004,7 @@
 	*num_entries = 0;
 	err = xe_pt_stage_bind(tile, vma, entries, num_entries);
 	if (!err)
-		XE_WARN_ON(!*num_entries);
+		xe_tile_assert(tile, *num_entries);
 	else /* abort! */
 		xe_pt_abort_bind(vma, entries, *num_entries);
 
@@ -1026,7 +1026,7 @@
 		u64 end;
 		u64 start;
 
-		XE_WARN_ON(entry->pt->is_compact);
+		xe_assert(xe, !entry->pt->is_compact);
 		start = entry->ofs * page_size;
 		end = start + page_size * entry->qwords;
 		vm_dbg(&xe->drm,
@@ -1276,7 +1276,7 @@
 		dma_fence_put(&ifence->base.base);	/* Creation ref */
 	}
 
-	XE_WARN_ON(ret && ret != -ENOENT);
+	xe_gt_assert(gt, !ret || ret == -ENOENT);
 
 	return ret && ret != -ENOENT ? ret : 0;
 }
@@ -1356,7 +1356,7 @@
 	err = xe_pt_prepare_bind(tile, vma, entries, &num_entries, rebind);
 	if (err)
 		goto err;
-	XE_WARN_ON(num_entries > ARRAY_SIZE(entries));
+	xe_tile_assert(tile, num_entries <= ARRAY_SIZE(entries));
 
 	xe_vm_dbg_print_entries(tile_to_xe(tile), entries, num_entries);
 	xe_pt_calc_rfence_interval(vma, &bind_pt_update, entries,
@@ -1707,7 +1707,7 @@
 	       xe_vma_start(vma), xe_vma_end(vma) - 1, q);
 
 	num_entries = xe_pt_stage_unbind(tile, vma, entries);
-	XE_WARN_ON(num_entries > ARRAY_SIZE(entries));
+	xe_tile_assert(tile, num_entries <= ARRAY_SIZE(entries));
 
 	xe_vm_dbg_print_entries(tile_to_xe(tile), entries, num_entries);
 	xe_pt_calc_rfence_interval(vma, &unbind_pt_update, entries,
@@ -1773,7 +1773,7 @@
 		list_del_init(&vma->combined_links.rebind);
 
 	if (unbind_pt_update.locked) {
-		XE_WARN_ON(!xe_vma_is_userptr(vma));
+		xe_tile_assert(tile, xe_vma_is_userptr(vma));
 
 		if (!vma->tile_present) {
 			spin_lock(&vm->userptr.invalidated_lock);
--- a/drivers/gpu/drm/xe/xe_ring_ops.c
+++ b/drivers/gpu/drm/xe/xe_ring_ops.c
@@ -212,6 +212,7 @@
 	u32 dw[MAX_JOB_SIZE_DW], i = 0;
 	u32 ppgtt_flag = get_ppgtt_flag(job);
 	struct xe_vm *vm = job->q->vm;
+	struct xe_gt *gt = job->q->gt;
 
 	if (vm && vm->batch_invalidate_tlb) {
 		dw[i++] = preparser_disable(true);
@@ -234,7 +235,7 @@
 
 	i = emit_user_interrupt(dw, i);
 
-	XE_WARN_ON(i > MAX_JOB_SIZE_DW);
+	xe_gt_assert(gt, i <= MAX_JOB_SIZE_DW);
 
 	xe_lrc_write_ring(lrc, dw, i * sizeof(*dw));
 }
@@ -294,7 +295,7 @@
 
 	i = emit_user_interrupt(dw, i);
 
-	XE_WARN_ON(i > MAX_JOB_SIZE_DW);
+	xe_gt_assert(gt, i <= MAX_JOB_SIZE_DW);
 
 	xe_lrc_write_ring(lrc, dw, i * sizeof(*dw));
 }
@@ -342,7 +343,7 @@
 
 	i = emit_user_interrupt(dw, i);
 
-	XE_WARN_ON(i > MAX_JOB_SIZE_DW);
+	xe_gt_assert(gt, i <= MAX_JOB_SIZE_DW);
 
 	xe_lrc_write_ring(lrc, dw, i * sizeof(*dw));
 }
@@ -372,14 +373,16 @@
 
 	i = emit_user_interrupt(dw, i);
 
-	XE_WARN_ON(i > MAX_JOB_SIZE_DW);
+	xe_gt_assert(job->q->gt, i <= MAX_JOB_SIZE_DW);
 
 	xe_lrc_write_ring(lrc, dw, i * sizeof(*dw));
 }
 
 static void emit_job_gen12_gsc(struct xe_sched_job *job)
 {
-	XE_WARN_ON(job->q->width > 1); /* no parallel submission for GSCCS */
+	struct xe_gt *gt = job->q->gt;
+
+	xe_gt_assert(gt, job->q->width <= 1); /* no parallel submission for GSCCS */
 
 	__emit_job_gen12_simple(job, job->q->lrc,
 				job->batch_addr[0],
--- a/drivers/gpu/drm/xe/xe_sched_job.c
+++ b/drivers/gpu/drm/xe/xe_sched_job.c
@@ -143,7 +143,7 @@
 
 		/* Sanity check */
 		for (j = 0; j < q->width; ++j)
-			XE_WARN_ON(cf->base.seqno != fences[j]->seqno);
+			xe_assert(job_to_xe(job), cf->base.seqno == fences[j]->seqno);
 
 		job->fence = &cf->base;
 	}
--- a/drivers/gpu/drm/xe/xe_uc.c
+++ b/drivers/gpu/drm/xe/xe_uc.c
@@ -162,7 +162,7 @@
 
 	/* We don't fail the driver load if HuC fails to auth, but let's warn */
 	ret = xe_huc_auth(&uc->huc);
-	XE_WARN_ON(ret);
+	xe_gt_assert(uc_to_gt(uc), !ret);
 
 	return 0;
 }
--- a/drivers/gpu/drm/xe/xe_uc_fw.c
+++ b/drivers/gpu/drm/xe/xe_uc_fw.c
@@ -195,7 +195,7 @@
 	u32 count;
 	int i;
 
-	XE_WARN_ON(uc_fw->type >= ARRAY_SIZE(blobs_all));
+	xe_assert(xe, uc_fw->type < ARRAY_SIZE(blobs_all));
 	entries = blobs_all[uc_fw->type].entries;
 	count = blobs_all[uc_fw->type].count;
 
@@ -224,8 +224,8 @@
 	struct xe_device *xe = uc_fw_to_xe(uc_fw);
 	u32 size = min_t(u32, uc_fw->rsa_size, max_len);
 
-	XE_WARN_ON(size % 4);
-	XE_WARN_ON(!xe_uc_fw_is_available(uc_fw));
+	xe_assert(xe, !(size % 4));
+	xe_assert(xe, xe_uc_fw_is_available(uc_fw));
 
 	xe_map_memcpy_from(xe, dst, &uc_fw->bo->vmap,
 			   xe_uc_fw_rsa_offset(uc_fw), size);
@@ -249,8 +249,8 @@
 	struct xe_gt *gt = uc_fw_to_gt(uc_fw);
 	struct xe_guc *guc = &gt->uc.guc;
 
-	XE_WARN_ON(uc_fw->type != XE_UC_FW_TYPE_GUC);
-	XE_WARN_ON(uc_fw->major_ver_found < 70);
+	xe_gt_assert(gt, uc_fw->type == XE_UC_FW_TYPE_GUC);
+	xe_gt_assert(gt, uc_fw->major_ver_found >= 70);
 
 	if (uc_fw->major_ver_found > 70 || uc_fw->minor_ver_found >= 6) {
 		/* v70.6.0 adds CSS header support */
@@ -336,8 +336,8 @@
 	 * before we're looked at the HW caps to see if we have uc support
 	 */
 	BUILD_BUG_ON(XE_UC_FIRMWARE_UNINITIALIZED);
-	XE_WARN_ON(uc_fw->status);
-	XE_WARN_ON(uc_fw->path);
+	xe_assert(xe, !uc_fw->status);
+	xe_assert(xe, !uc_fw->path);
 
 	uc_fw_auto_select(xe, uc_fw);
 	xe_uc_fw_change_status(uc_fw, uc_fw->path ? *uc_fw->path ?
@@ -504,7 +504,7 @@
 	int err;
 
 	/* make sure the status was cleared the last time we reset the uc */
-	XE_WARN_ON(xe_uc_fw_is_loaded(uc_fw));
+	xe_assert(xe, !xe_uc_fw_is_loaded(uc_fw));
 
 	if (!xe_uc_fw_is_loadable(uc_fw))
 		return -ENOEXEC;
--- a/drivers/gpu/drm/xe/xe_vm.c
+++ b/drivers/gpu/drm/xe/xe_vm.c
@@ -71,7 +71,7 @@
 	bool read_only = xe_vma_read_only(vma);
 
 	lockdep_assert_held(&vm->lock);
-	XE_WARN_ON(!xe_vma_is_userptr(vma));
+	xe_assert(xe, xe_vma_is_userptr(vma));
 retry:
 	if (vma->gpuva.flags & XE_VMA_DESTROYED)
 		return 0;
@@ -260,7 +260,7 @@
 		struct dma_fence *fence;
 
 		link = list->next;
-		XE_WARN_ON(link == list);
+		xe_assert(vm->xe, link != list);
 
 		fence = xe_preempt_fence_arm(to_preempt_fence_from_link(link),
 					     q, q->compute.context,
@@ -338,7 +338,7 @@
 	int err;
 	bool wait;
 
-	XE_WARN_ON(!xe_vm_in_compute_mode(vm));
+	xe_assert(vm->xe, xe_vm_in_compute_mode(vm));
 
 	down_write(&vm->lock);
 	drm_exec_init(&exec, DRM_EXEC_INTERRUPTIBLE_WAIT, 0);
@@ -573,7 +573,7 @@
 	long wait;
 	int __maybe_unused tries = 0;
 
-	XE_WARN_ON(!xe_vm_in_compute_mode(vm));
+	xe_assert(vm->xe, xe_vm_in_compute_mode(vm));
 	trace_xe_vm_rebind_worker_enter(vm);
 
 	down_write(&vm->lock);
@@ -698,7 +698,7 @@
 	struct dma_fence *fence;
 	long err;
 
-	XE_WARN_ON(!xe_vma_is_userptr(vma));
+	xe_assert(vm->xe, xe_vma_is_userptr(vma));
 	trace_xe_vma_userptr_invalidate(vma);
 
 	if (!mmu_notifier_range_blockable(range))
@@ -839,7 +839,7 @@
 	xe_vm_assert_held(vm);
 	list_for_each_entry_safe(vma, next, &vm->rebind_list,
 				 combined_links.rebind) {
-		XE_WARN_ON(!vma->tile_present);
+		xe_assert(vm->xe, vma->tile_present);
 
 		list_del_init(&vma->combined_links.rebind);
 		dma_fence_put(fence);
@@ -867,8 +867,8 @@
 	struct xe_tile *tile;
 	u8 id;
 
-	XE_WARN_ON(start >= end);
-	XE_WARN_ON(end >= vm->size);
+	xe_assert(vm->xe, start < end);
+	xe_assert(vm->xe, end < vm->size);
 
 	if (!bo && !is_null)	/* userptr */
 		vma = kzalloc(sizeof(*vma), GFP_KERNEL);
@@ -1064,10 +1064,10 @@
 	struct xe_vm *vm = xe_vma_vm(vma);
 
 	lockdep_assert_held_write(&vm->lock);
-	XE_WARN_ON(!list_empty(&vma->combined_links.destroy));
+	xe_assert(vm->xe, list_empty(&vma->combined_links.destroy));
 
 	if (xe_vma_is_userptr(vma)) {
-		XE_WARN_ON(!(vma->gpuva.flags & XE_VMA_DESTROYED));
+		xe_assert(vm->xe, vma->gpuva.flags & XE_VMA_DESTROYED);
 
 		spin_lock(&vm->userptr.invalidated_lock);
 		list_del(&vma->userptr.invalidate_link);
@@ -1160,7 +1160,7 @@
 	if (xe_vm_is_closed_or_banned(vm))
 		return NULL;
 
-	XE_WARN_ON(start + range > vm->size);
+	xe_assert(vm->xe, start + range <= vm->size);
 
 	gpuva = drm_gpuva_find_first(&vm->gpuvm, start, range);
 
@@ -1171,7 +1171,7 @@
 {
 	int err;
 
-	XE_WARN_ON(xe_vma_vm(vma) != vm);
+	xe_assert(vm->xe, xe_vma_vm(vma) == vm);
 	lockdep_assert_held(&vm->lock);
 
 	err = drm_gpuva_insert(&vm->gpuvm, &vma->gpuva);
@@ -1182,7 +1182,7 @@
 
 static void xe_vm_remove_vma(struct xe_vm *vm, struct xe_vma *vma)
 {
-	XE_WARN_ON(xe_vma_vm(vma) != vm);
+	xe_assert(vm->xe, xe_vma_vm(vma) == vm);
 	lockdep_assert_held(&vm->lock);
 
 	drm_gpuva_remove(&vma->gpuva);
@@ -1428,7 +1428,7 @@
 	struct drm_gpuva *gpuva, *next;
 	u8 id;
 
-	XE_WARN_ON(vm->preempt.num_exec_queues);
+	xe_assert(xe, !vm->preempt.num_exec_queues);
 
 	xe_vm_close(vm);
 	flush_async_ops(vm);
@@ -1505,7 +1505,7 @@
 	if (vm->async_ops.error_capture.addr)
 		wake_up_all(&vm->async_ops.error_capture.wq);
 
-	XE_WARN_ON(!list_empty(&vm->extobj.list));
+	xe_assert(xe, list_empty(&vm->extobj.list));
 	up_write(&vm->lock);
 
 	mutex_lock(&xe->usm.lock);
@@ -1531,7 +1531,7 @@
 	void *lookup;
 
 	/* xe_vm_close_and_put was not called? */
-	XE_WARN_ON(vm->size);
+	xe_assert(xe, !vm->size);
 
 	if (!(vm->flags & XE_VM_FLAG_MIGRATION)) {
 		xe_device_mem_access_put(xe);
@@ -1539,7 +1539,7 @@
 		if (xe->info.has_asid) {
 			mutex_lock(&xe->usm.lock);
 			lookup = xa_erase(&xe->usm.asid_to_vm, vm->usm.asid);
-			XE_WARN_ON(lookup != vm);
+			xe_assert(xe, lookup == vm);
 			mutex_unlock(&xe->usm.lock);
 		}
 	}
@@ -1802,7 +1802,7 @@
 		struct async_op_fence *afence =
 			container_of(fence, struct async_op_fence, fence);
 
-		XE_WARN_ON(xe_vm_no_dma_fences(afence->vm));
+		xe_assert(afence->vm->xe, !xe_vm_no_dma_fences(afence->vm));
 
 		smp_rmb();
 		return wait_event_interruptible(afence->wq, afence->started);
@@ -1828,7 +1828,7 @@
 	} else {
 		int i;
 
-		XE_WARN_ON(!xe_vm_in_fault_mode(vm));
+		xe_assert(vm->xe, xe_vm_in_fault_mode(vm));
 
 		fence = dma_fence_get_stub();
 		if (last_op) {
@@ -2110,7 +2110,7 @@
 {
 	int err;
 
-	XE_WARN_ON(region > ARRAY_SIZE(region_to_mem_type));
+	xe_assert(vm->xe, region <= ARRAY_SIZE(region_to_mem_type));
 
 	if (!xe_vma_has_no_bo(vma)) {
 		err = xe_bo_migrate(xe_vma_bo(vma), region_to_mem_type[region]);
@@ -2309,7 +2309,7 @@
 		}
 		break;
 	case XE_VM_BIND_OP_UNMAP_ALL:
-		XE_WARN_ON(!bo);
+		xe_assert(vm->xe, bo);
 
 		err = xe_bo_lock(bo, true);
 		if (err)
@@ -2506,7 +2506,7 @@
 		struct xe_vma_op *op = gpuva_op_to_vma_op(__op);
 		bool first = list_empty(ops_list);
 
-		XE_WARN_ON(!first && !async);
+		xe_assert(vm->xe, first || async);
 
 		INIT_LIST_HEAD(&op->link);
 		list_add_tail(&op->link, ops_list);
@@ -3468,8 +3468,8 @@
 	u8 id;
 	int ret;
 
-	XE_WARN_ON(!xe_vm_in_fault_mode(xe_vma_vm(vma)));
-	XE_WARN_ON(xe_vma_is_null(vma));
+	xe_assert(xe, xe_vm_in_fault_mode(xe_vma_vm(vma)));
+	xe_assert(xe, !xe_vma_is_null(vma));
 	trace_xe_vma_usm_invalidate(vma);
 
 	/* Check that we don't race with page-table updates */
--- a/drivers/gpu/drm/xe/xe_vm.h
+++ b/drivers/gpu/drm/xe/xe_vm.h
@@ -182,7 +182,7 @@
 
 static inline void xe_vm_queue_rebind_worker(struct xe_vm *vm)
 {
-	XE_WARN_ON(!xe_vm_in_compute_mode(vm));
+	xe_assert(vm->xe, xe_vm_in_compute_mode(vm));
 	queue_work(vm->xe->ordered_wq, &vm->preempt.rebind_work);
 }
 
