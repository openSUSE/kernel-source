Subject: add affinity_load_balancing sysctl
From: kraxel@suse.de
References: 176738

Add a sysctl to tweak how the kernel initially schedules threads to cpus.

By default the kernel tries to keep threads on the local cpu (and local
node on NUMA machines).  Depending on the application this may not deliver
the best performance, especially applications with a large working set for
each thread tend to perform better when being scheduled to different nodes
because they can use caches of multiple nodes then.

With this sysctl enabled the kernel will spread threads over the cpus given
and doesn't try to keep them local.

usage:
  - set sysctl kernel.affinity_load_balancing = 1
  - use taskset or numactl to specify which cpus your task should be
    scheduled on.

Index: linux-2.6.16/kernel/sysctl.c
===================================================================
--- linux-2.6.16.orig/kernel/sysctl.c
+++ linux-2.6.16/kernel/sysctl.c
@@ -233,6 +233,8 @@ static ctl_table root_table[] = {
 	{ .ctl_name = 0 }
 };
 
+extern int affinity_load_balancing;
+
 static ctl_table kern_table[] = {
 	{
 		.ctl_name	= KERN_OSTYPE,
@@ -721,6 +723,16 @@ static ctl_table kern_table[] = {
 		.mode		= 0644,
 		.proc_handler	= &proc_dointvec,
 	},
+#ifdef CONFIG_SMP
+	{
+		.ctl_name	= -2,
+		.procname	= "affinity_load_balancing",
+		.data		= &affinity_load_balancing,
+		.maxlen		= sizeof(affinity_load_balancing),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+#endif
 	{ .ctl_name = 0 }
 };
 
Index: linux-2.6.16/kernel/sched.c
===================================================================
--- linux-2.6.16.orig/kernel/sched.c
+++ linux-2.6.16/kernel/sched.c
@@ -1020,6 +1020,28 @@ find_idlest_cpu(struct sched_group *grou
 	return idlest;
 }
 
+static int
+find_idlest_cpu_nodomain(struct task_struct *p, int this_cpu)
+{
+	cpumask_t tmp;
+	unsigned long load, min_load = ULONG_MAX;
+	int idlest = -1;
+	int i;
+
+	/* Traverse only the allowed CPUs */
+	cpus_and(tmp, cpu_online_map, p->cpus_allowed);
+
+	for_each_cpu_mask(i, tmp) {
+		load = target_load(i, 1);
+
+		if (load < min_load) {
+			min_load = load;
+			idlest = i;
+		}
+	}
+	return idlest;
+}
+
 /*
  * sched_balance_self: balance the current task (running on cpu) in domains
  * that have the 'flag' flag set. In practice, this is SD_BALANCE_FORK and
@@ -1031,11 +1053,17 @@ find_idlest_cpu(struct sched_group *grou
  *
  * preempt must be disabled.
  */
+
+int affinity_load_balancing = 0;
+
 static int sched_balance_self(int cpu, int flag)
 {
 	struct task_struct *t = current;
 	struct sched_domain *tmp, *sd = NULL;
 
+	if (affinity_load_balancing && !cpus_full(t->cpus_allowed))
+		return find_idlest_cpu_nodomain(t, cpu);
+
 	for_each_domain(cpu, tmp)
 		if (tmp->flags & flag)
 			sd = tmp;
