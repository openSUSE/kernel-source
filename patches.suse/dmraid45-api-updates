From: Jeff Mahoney <jeffm@suse.com>
Subject: dmraid45: dm_dirty_log_create API fix
Patch-mainline: not yet, depends on patches.suse/dm-raid45_2.6.27_20081027.patch

 2.6.33 added an optional callback to dm_dirty_log_create for flush
 operations. Eventually raid45 should have one but until then, this is
 to allow it to build.

Signed-off-by: Jeff Mahoney <jeffm@suse.com>

---
 drivers/md/dm-raid45.c |   55 +++++++++----------------------------------------
 1 file changed, 11 insertions(+), 44 deletions(-)

--- a/drivers/md/dm-raid45.c
+++ b/drivers/md/dm-raid45.c
@@ -197,10 +197,6 @@ enum chunk_flags {
 	CHUNK_UPTODATE,		/* Chunk pages are uptodate. */
 };
 
-#if READ != 0 || WRITE != 1
-#error dm-raid45: READ/WRITE != 0/1 used as index!!!
-#endif
-
 enum bl_type {
 	WRITE_QUEUED = WRITE + 1,
 	WRITE_MERGED,
@@ -937,7 +933,7 @@ static int raid_dev_lookup(struct raid_s
  * Stripe hash functions
  */
 /* Initialize/destroy stripe hash. */
-static int hash_init(struct stripe_hash *hash, unsigned stripes)
+static int dm_raid_hash_init(struct stripe_hash *hash, unsigned stripes)
 {
 	unsigned buckets = roundup_pow_of_two(stripes >> 1);
 	static unsigned hash_primes[] = {
@@ -1019,7 +1015,7 @@ static int sc_hash_resize(struct stripe_
 		int r;
 		struct stripe_hash hash;
 
-		r = hash_init(&hash, atomic_read(&sc->stripes));
+		r = dm_raid_hash_init(&hash, atomic_read(&sc->stripes));
 		if (r)
 			return r;
 
@@ -1568,7 +1564,6 @@ static int sc_init(struct raid_set *rs,
 	disk = dm_disk(md);
 	snprintf(sc->kc.name, sizeof(sc->kc.name), "%s-%d.%d", TARGET,
 		 disk->first_minor, atomic_inc_return(&_stripe_sc_nr));
-	dm_put(md);
 	sc->kc.cache = kmem_cache_create(sc->kc.name, stripe_size(rs),
 					 0, 0, NULL);
 	if (!sc->kc.cache)
@@ -1590,18 +1585,13 @@ static int sc_init(struct raid_set *rs,
 		return PTR_ERR(rec->mem_cache_client);
 
 	/* Create dm-io client context for IO stripes. */
-	sc->dm_io_client =
-		dm_io_client_create((stripes > 32 ? 32 : stripes) *
-				    rs->set.raid_devs *
-				    chunk_pages(rs->set.io_size));
+	sc->dm_io_client = dm_io_client_create();
 	if (IS_ERR(sc->dm_io_client))
 		return PTR_ERR(sc->dm_io_client);
 
 	/* FIXME: intermingeled with stripe cache initialization. */
 	/* Create dm-io client context for recovery stripes. */
-	rec->dm_io_client =
-		dm_io_client_create(rstripes * rs->set.raid_devs *
-				    chunk_pages(rec->io_size));
+	rec->dm_io_client = dm_io_client_create();
 	if (IS_ERR(rec->dm_io_client))
 		return PTR_ERR(rec->dm_io_client);
 
@@ -3278,7 +3268,7 @@ static void do_ios(struct raid_set *rs,
 		 * the input queue unless all work queues are empty
 		 * and the stripe cache is inactive.
 		 */
-		if (unlikely(bio_empty_barrier(bio))) {
+		if (bio->bi_rw & REQ_FLUSH) {
 			/* REMOVEME: statistics. */
 			atomic_inc(rs->stats + S_BARRIER);
 			if (delay ||
@@ -3331,18 +3321,6 @@ static void do_ios(struct raid_set *rs,
 	bio_list_merge_head(ios, &reject);
 }
 
-/* Unplug: let any queued io role on the sets devices. */
-static void do_unplug(struct raid_set *rs)
-{
-	struct raid_dev *dev = rs->dev + rs->set.raid_devs;
-
-	while (dev-- > rs->dev) {
-		/* Only call any device unplug function, if io got queued. */
-		if (TestClearDevIoQueued(dev))
-			blk_unplug(bdev_get_queue(dev->dev->bdev));
-	}
-}
-
 /* Send an event in case we're getting too busy. */
 static void do_busy_event(struct raid_set *rs)
 {
@@ -3399,8 +3377,6 @@ static void do_raid(struct work_struct *
 
 	/* Try to recover regions. */
 	r = do_recovery(rs);
-	if (r)
-		do_unplug(rs);	/* Unplug the sets device queues. */
 
 	/* Quickly grab all new ios queued and add them to the work list. */
 	mutex_lock(&rs->io.in_lock);
@@ -3412,8 +3388,6 @@ static void do_raid(struct work_struct *
 		do_ios(rs, ios); /* Got ios to work into the cache. */
 
 	r = do_flush(rs);		/* Flush any stripes on io list. */
-	if (r)
-		do_unplug(rs);		/* Unplug the sets device queues. */
 
 	do_busy_event(rs);	/* Check if we got too busy. */
 }
@@ -3599,7 +3573,7 @@ context_alloc(struct raid_type *raid_typ
 	 */
 	ti_len = ti->len;
 	ti->len = sectors_per_dev;
-	dl = dm_dirty_log_create(argv[0], ti, dl_parms, argv + 2);
+	dl = dm_dirty_log_create(argv[0], ti, NULL, dl_parms, argv + 2);
 	ti->len = ti_len;
 	if (!dl)
 		goto bad_dirty_log;
@@ -3811,9 +3785,8 @@ DMINFO("rs->set.sectors_per_dev=%llu", (
 			TI_ERR("Invalid RAID device offset parameter");
 
 		dev->start = tmp;
-		r = dm_get_device(ti, argv[0], dev->start,
-				  rs->set.sectors_per_dev,
-				  dm_table_get_mode(ti->table), &dev->dev);
+		r = dm_get_device(ti, argv[0], dm_table_get_mode(ti->table),
+				  &dev->dev);
 		if (r)
 			TI_ERR_RET("RAID device lookup failure", r);
 
@@ -3983,8 +3956,6 @@ static void rs_set_read_ahead(struct rai
 			q->backing_dev_info.ra_pages = ra_pages;
 		}
 	}
-
-	dm_put(md);
 }
 
 /* Set congested function. */
@@ -3996,7 +3967,6 @@ static void rs_set_congested_fn(struct r
 	/* Set congested function and data. */
 	bdi->congested_fn = rs_congested;
 	bdi->congested_data = rs;
-	dm_put(md);
 }
 
 /*
@@ -4224,8 +4194,7 @@ static void raid_dtr(struct dm_target *t
 }
 
 /* Raid mapping function. */
-static int raid_map(struct dm_target *ti, struct bio *bio,
-		    union map_info *map_context)
+static int raid_map(struct dm_target *ti, struct bio *bio)
 {
 	/* I don't want to waste stripe cache capacity. */
 	if (bio_rw(bio) == READA)
@@ -4369,8 +4338,8 @@ static void raid_devel_stats(struct dm_t
 	*size = sz;
 }
 
-static int raid_status(struct dm_target *ti, status_type_t type,
-		       unsigned status_flags, char *result, unsigned maxlen)
+static void raid_status(struct dm_target *ti, status_type_t type,
+			unsigned status_flags, char *result, unsigned maxlen)
 {
 	unsigned p, sz = 0;
 	char buf[BDEVNAME_SIZE];
@@ -4435,8 +4404,6 @@ static int raid_status(struct dm_target
 			       format_dev_t(buf, rs->dev[p].dev->bdev->bd_dev),
 			       (unsigned long long) rs->dev[p].start);
 	}
-
-	return 0;
 }
 
 /*
