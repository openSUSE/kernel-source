From: Carlos López <clopez@suse.de>
Patch-mainline: never, kabi
References: git-fixes
Subject: [PATCH] kABI fix for KVM: x86: Snapshot the host's DEBUGCTL
 in common x86 (git-fixes)

Backport of mainline commit fb71c7959356 adds a new member to
kvm_vcpu_arch, which breaks kABI (due to it being embedded in kvm_vcpu).
Split the new member into two holes in the struct and add the required
logic to retrieve the original value.

Add build-time checks to ensure that the new fields fall into struct
holes. Opportunistically add a check for a previously introduced
field (is_amd_compatible) in a hole in the same struct for a similar
kABI fix.

Signed-off-by: Carlos López <clopez@suse.de>
Reviewed-by: Jan Kara <jack@suse.cz>
---
 arch/x86/include/asm/kvm_host.h |  330 +++++++++++++++++++++++++++++++++++++++-
 arch/x86/kvm/vmx/vmx.c          |    7
 arch/x86/kvm/x86.c              |    5
 arch/x86/include/asm/kvm_host.h |  330 +++++++++++++++++++++++++++++++++++++++-
 arch/x86/kvm/vmx/vmx.c          |    7 
 arch/x86/kvm/x86.c              |    5 
 3 files changed, 338 insertions(+), 4 deletions(-)

--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -16,6 +16,7 @@
 #include <linux/irq_work.h>
 #include <linux/irq.h>
 #include <linux/workqueue.h>
+#include <linux/build_bug.h>
 
 #include <linux/kvm.h>
 #include <linux/kvm_para.h>
@@ -761,11 +762,16 @@ struct kvm_vcpu_arch {
 	u32 host_pkru;
 	u32 pkru;
 	u32 hflags;
+#ifndef __GENKSYMS__
+	u32 host_debugctl_lo;
+#endif
 	u64 efer;
-	u64 host_debugctl;
 	u64 apic_base;
 	struct kvm_lapic *apic;    /* kernel irqchip context */
 	bool load_eoi_exitmap_pending;
+#ifndef __GENKSYMS__
+	u32 host_debugctl_hi;
+#endif
 	DECLARE_BITMAP(ioapic_handled_vectors, 256);
 	unsigned long apic_attention;
 	int32_t apic_arb_prio;
@@ -1052,6 +1058,328 @@ struct kvm_vcpu_arch {
 #endif
 };
 
+struct __orig_kvm_vcpu_arch {
+	/*
+	 * rip and regs accesses must go through
+	 * kvm_{register,rip}_{read,write} functions.
+	 */
+	unsigned long regs[NR_VCPU_REGS];
+	u32 regs_avail;
+	u32 regs_dirty;
+
+	unsigned long cr0;
+	unsigned long cr0_guest_owned_bits;
+	unsigned long cr2;
+	unsigned long cr3;
+	unsigned long cr4;
+	unsigned long cr4_guest_owned_bits;
+	unsigned long cr4_guest_rsvd_bits;
+	unsigned long cr8;
+	u32 host_pkru;
+	u32 pkru;
+	u32 hflags;
+	u64 efer;
+	u64 apic_base;
+	struct kvm_lapic *apic;    /* kernel irqchip context */
+	bool load_eoi_exitmap_pending;
+	DECLARE_BITMAP(ioapic_handled_vectors, 256);
+	unsigned long apic_attention;
+	int32_t apic_arb_prio;
+	int mp_state;
+	u64 ia32_misc_enable_msr;
+	u64 smbase;
+	u64 smi_count;
+	bool at_instruction_boundary;
+	bool tpr_access_reporting;
+	bool xfd_no_write_intercept;
+	u64 ia32_xss;
+	u64 microcode_version;
+	u64 arch_capabilities;
+	u64 perf_capabilities;
+
+	/*
+	 * Paging state of the vcpu
+	 *
+	 * If the vcpu runs in guest mode with two level paging this still saves
+	 * the paging mode of the l1 guest. This context is always used to
+	 * handle faults.
+	 */
+	struct kvm_mmu *mmu;
+
+	/* Non-nested MMU for L1 */
+	struct kvm_mmu root_mmu;
+
+	/* L1 MMU when running nested */
+	struct kvm_mmu guest_mmu;
+
+	/*
+	 * Paging state of an L2 guest (used for nested npt)
+	 *
+	 * This context will save all necessary information to walk page tables
+	 * of an L2 guest. This context is only initialized for page table
+	 * walking and not for faulting since we never handle l2 page faults on
+	 * the host.
+	 */
+	struct kvm_mmu nested_mmu;
+
+	/*
+	 * Pointer to the mmu context currently used for
+	 * gva_to_gpa translations.
+	 */
+	struct kvm_mmu *walk_mmu;
+
+	struct kvm_mmu_memory_cache mmu_pte_list_desc_cache;
+	struct kvm_mmu_memory_cache mmu_shadow_page_cache;
+	struct kvm_mmu_memory_cache mmu_shadowed_info_cache;
+	struct kvm_mmu_memory_cache mmu_page_header_cache;
+	struct kvm_mmu_memory_cache mmu_private_spt_cache;
+
+	/*
+	 * QEMU userspace and the guest each have their own FPU state.
+	 * In vcpu_run, we switch between the user and guest FPU contexts.
+	 * While running a VCPU, the VCPU thread will have the guest FPU
+	 * context.
+	 *
+	 * Note that while the PKRU state lives inside the fpu registers,
+	 * it is switched out separately at VMENTER and VMEXIT time. The
+	 * "guest_fpstate" state here contains the guest FPU context, with the
+	 * host PRKU bits.
+	 */
+	struct fpu_guest guest_fpu;
+
+	u64 xcr0;
+	u64 guest_supported_xcr0;
+
+	struct kvm_pio_request pio;
+	void *pio_data;
+	void *sev_pio_data;
+	unsigned sev_pio_count;
+
+	u8 event_exit_inst_len;
+
+	bool exception_from_userspace;
+
+	/* Exceptions to be injected to the guest. */
+	struct kvm_queued_exception exception;
+	/* Exception VM-Exits to be synthesized to L1. */
+	struct kvm_queued_exception exception_vmexit;
+
+	struct __orig_kvm_queued_interrupt {
+		bool injected;
+		bool soft;
+		u8 nr;
+	} interrupt;
+
+	int halt_request; /* real mode on Intel only */
+
+	int cpuid_nent;
+	struct kvm_cpuid_entry2 *cpuid_entries;
+	struct kvm_hypervisor_cpuid kvm_cpuid;
+
+	/*
+	 * FIXME: Drop this macro and use KVM_NR_GOVERNED_FEATURES directly
+	 * when "struct kvm_vcpu_arch" is no longer defined in an
+	 * arch/x86/include/asm header.  The max is mostly arbitrary, i.e.
+	 * can be increased as necessary.
+	 */
+#define KVM_MAX_NR_GOVERNED_FEATURES BITS_PER_LONG
+
+	/*
+	 * Track whether or not the guest is allowed to use features that are
+	 * governed by KVM, where "governed" means KVM needs to manage state
+	 * and/or explicitly enable the feature in hardware.  Typically, but
+	 * not always, governed features can be used by the guest if and only
+	 * if both KVM and userspace want to expose the feature to the guest.
+	 */
+	struct {
+		DECLARE_BITMAP(enabled, KVM_MAX_NR_GOVERNED_FEATURES);
+	} governed_features;
+
+	u64 reserved_gpa_bits;
+	int maxphyaddr;
+
+	/* emulate context */
+
+	struct x86_emulate_ctxt *emulate_ctxt;
+	bool emulate_regs_need_sync_to_vcpu;
+	bool emulate_regs_need_sync_from_vcpu;
+	int (*complete_userspace_io)(struct kvm_vcpu *vcpu);
+
+	gpa_t time;
+	struct pvclock_vcpu_time_info hv_clock;
+	unsigned int hw_tsc_khz;
+	struct gfn_to_pfn_cache pv_time;
+	/* set guest stopped flag in pvclock flags field */
+	bool pvclock_set_guest_stopped_request;
+
+	struct {
+		u8 preempted;
+		u64 msr_val;
+		u64 last_steal;
+		struct gfn_to_hva_cache cache;
+	} st;
+
+	u64 l1_tsc_offset;
+	u64 tsc_offset; /* current tsc offset */
+	u64 last_guest_tsc;
+	u64 last_host_tsc;
+	u64 tsc_offset_adjustment;
+	u64 this_tsc_nsec;
+	u64 this_tsc_write;
+	u64 this_tsc_generation;
+	bool tsc_catchup;
+	bool tsc_always_catchup;
+	s8 virtual_tsc_shift;
+	u32 virtual_tsc_mult;
+	u32 virtual_tsc_khz;
+	s64 ia32_tsc_adjust_msr;
+	u64 msr_ia32_power_ctl;
+	u64 l1_tsc_scaling_ratio;
+	u64 tsc_scaling_ratio; /* current scaling ratio */
+
+	atomic_t nmi_queued;  /* unprocessed asynchronous NMIs */
+	/* Number of NMIs pending injection, not including hardware vNMIs. */
+	unsigned int nmi_pending;
+	bool nmi_injected;    /* Trying to inject an NMI this entry */
+	bool smi_pending;    /* SMI queued after currently running handler */
+	u8 handling_intr_from_guest;
+
+	struct kvm_mtrr mtrr_state;
+	u64 pat;
+
+	unsigned switch_db_regs;
+	unsigned long db[KVM_NR_DB_REGS];
+	unsigned long dr6;
+	unsigned long dr7;
+	unsigned long eff_db[KVM_NR_DB_REGS];
+	unsigned long guest_debug_dr7;
+	u64 msr_platform_info;
+	u64 msr_misc_features_enables;
+
+	u64 mcg_cap;
+	u64 mcg_status;
+	u64 mcg_ctl;
+	u64 mcg_ext_ctl;
+	u64 *mce_banks;
+	u64 *mci_ctl2_banks;
+
+	/* Cache MMIO info */
+	u64 mmio_gva;
+	unsigned mmio_access;
+	gfn_t mmio_gfn;
+	u64 mmio_gen;
+
+	struct kvm_pmu pmu;
+
+	/* used for guest single stepping over the given code position */
+	unsigned long singlestep_rip;
+
+#ifdef CONFIG_KVM_HYPERV
+	bool hyperv_enabled;
+	struct kvm_vcpu_hv *hyperv;
+#endif
+#ifdef CONFIG_KVM_XEN
+	struct kvm_vcpu_xen xen;
+#endif
+	cpumask_var_t wbinvd_dirty_mask;
+
+	unsigned long last_retry_eip;
+	unsigned long last_retry_addr;
+
+	struct {
+		bool halted;
+		gfn_t gfns[ASYNC_PF_PER_VCPU];
+		struct gfn_to_hva_cache data;
+		u64 msr_en_val; /* MSR_KVM_ASYNC_PF_EN */
+		u64 msr_int_val; /* MSR_KVM_ASYNC_PF_INT */
+		u16 vec;
+		u32 id;
+		bool send_user_only;
+		u32 host_apf_flags;
+		bool delivery_as_pf_vmexit;
+		bool pageready_pending;
+	} apf;
+
+	/* OSVW MSRs (AMD only) */
+	struct {
+		u64 length;
+		u64 status;
+	} osvw;
+
+	struct {
+		u64 msr_val;
+		struct gfn_to_hva_cache data;
+	} pv_eoi;
+
+	u64 msr_kvm_poll_control;
+
+	/* set at EPT violation at this point */
+	unsigned long exit_qualification;
+
+	/* pv related host specific info */
+	struct {
+		bool pv_unhalted;
+	} pv;
+
+	int pending_ioapic_eoi;
+	int pending_external_vector;
+
+	/* be preempted when it's in kernel-mode(cpl=0) */
+	bool preempted_in_kernel;
+
+	/* Flush the L1 Data cache for L1TF mitigation on VMENTER */
+	bool l1tf_flush_l1d;
+
+	/* Host CPU on which VM-entry was most recently attempted */
+	int last_vmentry_cpu;
+
+	/* AMD MSRC001_0015 Hardware Configuration */
+	u64 msr_hwcr;
+
+	/* pv related cpuid info */
+	struct {
+		/*
+		 * value of the eax register in the KVM_CPUID_FEATURES CPUID
+		 * leaf.
+		 */
+		u32 features;
+
+		/*
+		 * indicates whether pv emulation should be disabled if features
+		 * are not present in the guest's cpuid
+		 */
+		bool enforce;
+	} pv_cpuid;
+
+	/* Protected Guests */
+	bool guest_state_protected;
+
+	/*
+	 * Set when PDPTS were loaded directly by the userspace without
+	 * reading the guest memory
+	 */
+	bool pdptrs_from_userspace;
+
+#if IS_ENABLED(CONFIG_HYPERV)
+	hpa_t hv_root_tdp;
+#endif
+};
+
+suse_kabi_static_assert(sizeof(struct kvm_vcpu_arch) ==
+                        sizeof(struct __orig_kvm_vcpu_arch));
+suse_kabi_static_assert(offsetof(struct kvm_vcpu_arch, hflags) ==
+                        offsetof(struct __orig_kvm_vcpu_arch, hflags));
+suse_kabi_static_assert(offsetof(struct kvm_vcpu_arch, efer) ==
+                        offsetof(struct __orig_kvm_vcpu_arch, efer));
+suse_kabi_static_assert(offsetof(struct kvm_vcpu_arch, load_eoi_exitmap_pending) ==
+                        offsetof(struct __orig_kvm_vcpu_arch, load_eoi_exitmap_pending));
+suse_kabi_static_assert(offsetof(struct kvm_vcpu_arch, ioapic_handled_vectors) ==
+                        offsetof(struct __orig_kvm_vcpu_arch, ioapic_handled_vectors));
+suse_kabi_static_assert(offsetof(struct kvm_vcpu_arch, exception_from_userspace) ==
+                        offsetof(struct __orig_kvm_vcpu_arch, exception_from_userspace));
+suse_kabi_static_assert(offsetof(struct kvm_vcpu_arch, exception) ==
+                        offsetof(struct __orig_kvm_vcpu_arch, exception));
+
 struct kvm_lpage_info {
 	int disallow_lpage;
 };
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -7350,6 +7350,7 @@ static fastpath_t vmx_vcpu_run(struct kv
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	unsigned long cr3, cr4;
+	u64 host_debugctl;
 
 	/* Record the guest's net vcpu time for enforced NMI injections. */
 	if (unlikely(!enable_vnmi &&
@@ -7446,8 +7447,10 @@ static fastpath_t vmx_vcpu_run(struct kv
 	}
 
 	/* MSR_IA32_DEBUGCTLMSR is zeroed on vmexit. Restore it if needed */
-	if (vcpu->arch.host_debugctl)
-		update_debugctlmsr(vcpu->arch.host_debugctl);
+	host_debugctl = vcpu->arch.host_debugctl_lo |
+	            	((u64) vcpu->arch.host_debugctl_hi << 32);
+	if (host_debugctl)
+		update_debugctlmsr(host_debugctl);
 
 #ifndef CONFIG_X86_64
 	/*
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -10769,6 +10769,7 @@ static int vcpu_enter_guest(struct kvm_v
 		dm_request_for_irq_injection(vcpu) &&
 		kvm_cpu_accept_dm_intr(vcpu);
 	fastpath_t exit_fastpath;
+	u64 host_debugctl;
 
 	bool req_immediate_exit = false;
 
@@ -11028,7 +11029,9 @@ static int vcpu_enter_guest(struct kvm_v
 		set_debugreg(0, 7);
 	}
 
-	vcpu->arch.host_debugctl = get_debugctlmsr();
+	host_debugctl = get_debugctlmsr();
+	vcpu->arch.host_debugctl_lo = host_debugctl;
+	vcpu->arch.host_debugctl_hi = host_debugctl >> 32;
 
 	guest_timing_enter_irqoff();
 
