From fa66abf5bbf293f612e368cfe8a5465c8e05cb9e Mon Sep 17 00:00:00 2001
From: Mel Gorman <mgorman@techsingularity.net>
Date: Mon, 17 Nov 2025 13:29:45 +0000
Subject: [PATCH] sched/fair: Proportional newidle balance -KABI

References: bsc#1248792
Patch-mainline: Never, kABI

Upstream reported regressions on their database workloads due to:

      155213a2aed4 ("sched/fair: Bump sd->max_newidle_lb_cost when newidle balance fails")

The same patch was merged to SLE as it addressed a range of known
issues. Unfortunately it also regresses some database loads both
upstream and as reported by bsc#1248792 and the upstream fix breaks
KABI.

The affected structure is internal to the scheduler and no kernel module
should be accessing or modifying it. The main problem would be if a
kprobe was built as a binary against the GA kernel source and while
unlikely, it is possible.

To mitigate the risk, the KABI fix moves the new fields into existing
holes that should be ignored by any prebuilt external modules.

Signed-off-by: Mel Gorman <mgorman@suse.de>
Reviewed-by: Petr Pavlu <petr.pavlu@suse.com>
---
 include/linux/sched/topology.h |  121 ++++++++++++++++++++++++++++++++++++++++-
 1 file changed, 118 insertions(+), 3 deletion(-)

diff --git a/include/linux/sched/topology.h b/include/linux/sched/topology.h
index 4c88e57ef0986..572a285112d26 100644
--- a/include/linux/sched/topology.h
+++ b/include/linux/sched/topology.h
@@ -100,15 +100,29 @@ struct sched_domain {
 	int flags;			/* See SD_* */
 	int level;
 
+#ifndef __GENKSYMS__
+	/*
+	 * The newidle_* fields break KABI but not in any way that should
+	 * matter.  For KABI purposes, the location of the fields is slightly
+	 * changed so any module that accesses the offsets would have been
+	 * examining schedstat counters. It is unlikely this will cause any
+	 * functional error. There is a potential impact on binary kprobe
+	 * modules getting unexpected values but it is unlikely as probes and
+	 * tracing programs are generally built against a specific kernel
+	 * source or offsets are probed at runtime.
+	 *
+	 * Locations within struct are to fit in existing holes as suggested
+	 * by Petr Pavlu.
+	 */
+	unsigned int newidle_call;
+#endif
+
 	/* Runtime fields. */
 	unsigned long last_balance;	/* init to jiffies. units in jiffies */
 	unsigned int balance_interval;	/* initialise to 1. units in ms. */
 	unsigned int nr_balance_failed; /* initialise to 0 */
 
 	/* idle_balance() stats */
-	unsigned int newidle_call;
-	unsigned int newidle_success;
-	unsigned int newidle_ratio;
 	u64 max_newidle_lb_cost;
 	unsigned long last_decay_max_lb_cost;
 
@@ -146,6 +160,9 @@ struct sched_domain {
 	unsigned int ttwu_move_affine;
 	unsigned int ttwu_move_balance;
 #endif
+#ifndef __GENKSYMS__
+	unsigned int newidle_success;
+#endif
 	char *name;
 	union {
 		void *private;		/* used during construction */
@@ -154,6 +171,9 @@ struct sched_domain {
 	struct sched_domain_shared *shared;
 
 	unsigned int span_weight;
+#ifndef __GENKSYMS__
+	unsigned int newidle_ratio;
+#endif
 	/*
 	 * Span of all CPUs in this domain.
 	 *
@@ -164,6 +164,101 @@ struct sched_domain {
 	unsigned long span[];
 };
 
+struct __orig_sched_domain {
+	/* These fields must be setup */
+	struct sched_domain __rcu *parent;	/* top domain must be null terminated */
+	struct sched_domain __rcu *child;	/* bottom domain must be null terminated */
+	struct sched_group *groups;	/* the balancing groups of the domain */
+	unsigned long min_interval;	/* Minimum balance interval ms */
+	unsigned long max_interval;	/* Maximum balance interval ms */
+	unsigned int busy_factor;	/* less balancing by factor if busy */
+	unsigned int imbalance_pct;	/* No balance until over watermark */
+	unsigned int cache_nice_tries;	/* Leave cache hot tasks for # tries */
+	unsigned int imb_numa_nr;	/* Nr running tasks that allows a NUMA imbalance */
+
+	int nohz_idle;			/* NOHZ IDLE status */
+	int flags;			/* See SD_* */
+	int level;
+
+	/* Runtime fields. */
+	unsigned long last_balance;	/* init to jiffies. units in jiffies */
+	unsigned int balance_interval;	/* initialise to 1. units in ms. */
+	unsigned int nr_balance_failed; /* initialise to 0 */
+
+	/* idle_balance() stats */
+	u64 max_newidle_lb_cost;
+	unsigned long last_decay_max_lb_cost;
+
+#ifdef CONFIG_SCHEDSTATS
+	/* sched_balance_rq() stats */
+	unsigned int lb_count[CPU_MAX_IDLE_TYPES];
+	unsigned int lb_failed[CPU_MAX_IDLE_TYPES];
+	unsigned int lb_balanced[CPU_MAX_IDLE_TYPES];
+	unsigned int lb_imbalance_load[CPU_MAX_IDLE_TYPES];
+	unsigned int lb_imbalance_util[CPU_MAX_IDLE_TYPES];
+	unsigned int lb_imbalance_task[CPU_MAX_IDLE_TYPES];
+	unsigned int lb_imbalance_misfit[CPU_MAX_IDLE_TYPES];
+	unsigned int lb_gained[CPU_MAX_IDLE_TYPES];
+	unsigned int lb_hot_gained[CPU_MAX_IDLE_TYPES];
+	unsigned int lb_nobusyg[CPU_MAX_IDLE_TYPES];
+	unsigned int lb_nobusyq[CPU_MAX_IDLE_TYPES];
+
+	/* Active load balancing */
+	unsigned int alb_count;
+	unsigned int alb_failed;
+	unsigned int alb_pushed;
+
+	/* SD_BALANCE_EXEC stats */
+	unsigned int sbe_count;
+	unsigned int sbe_balanced;
+	unsigned int sbe_pushed;
+
+	/* SD_BALANCE_FORK stats */
+	unsigned int sbf_count;
+	unsigned int sbf_balanced;
+	unsigned int sbf_pushed;
+
+	/* try_to_wake_up() stats */
+	unsigned int ttwu_wake_remote;
+	unsigned int ttwu_move_affine;
+	unsigned int ttwu_move_balance;
+#endif
+	char *name;
+	union {
+		void *private;		/* used during construction */
+		struct rcu_head rcu;	/* used during destruction */
+	};
+	struct sched_domain_shared *shared;
+
+	unsigned int span_weight;
+	/*
+	 * Span of all CPUs in this domain.
+	 *
+	 * NOTE: this field is variable length. (Allocated dynamically
+	 * by attaching extra space to the end of the structure,
+	 * depending on how many CPUs the kernel has booted up with)
+	 */
+	unsigned long span[];
+};
+
+/* newidle_call */
+suse_kabi_static_assert(offsetof(struct sched_domain, level) ==
+                        offsetof(struct __orig_sched_domain, level));
+suse_kabi_static_assert(offsetof(struct sched_domain, last_balance) ==
+                        offsetof(struct __orig_sched_domain, last_balance));
+
+/* newidle_success */
+suse_kabi_static_assert(offsetof(struct sched_domain, ttwu_move_balance) ==
+                        offsetof(struct __orig_sched_domain, ttwu_move_balance));
+suse_kabi_static_assert(offsetof(struct sched_domain, name) ==
+                        offsetof(struct __orig_sched_domain, name));
+
+/* newidle_ratio */
+suse_kabi_static_assert(offsetof(struct sched_domain, span_weight) ==
+                        offsetof(struct __orig_sched_domain, span_weight));
+suse_kabi_static_assert(offsetof(struct sched_domain, span) ==
+                        offsetof(struct __orig_sched_domain, span));
+
 static inline struct cpumask *sched_domain_span(struct sched_domain *sd)
 {
 	return to_cpumask(sd->span);
