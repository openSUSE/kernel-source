From: Shung-Hsi Yu <shung-hsi.yu@suse.com>
Date: Thu Nov 27 02:20:34 PM CST 2025
Subject: kABI workaround for bpf: Enforce expected_attach_type for tailcall compatibility
Patch-mainline: Never, kABI workaround
References: CVE-2025-40123 bsc#1253365

Backport of upstream commit 4540aed51b12 ("bpf: Enforce expected_attach_type
for tailcall compatibility") break kABI by introducing a 'enum bpf_attach_type
expected_attach_type' field into 'struct bpf_map.owner'. Fortunately there is a
6-byte hole after 'struct bpf_map.owner', and the new enum fits right in
because it just takes 2 bytes. So simply wrap the new field in __GENKSYMS__,
and use suse_kabi_static_assert() to ensure there is a suitable hole across all
architectures.

Signed-off-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>
Reviewed-by: Jan Kara <jack@suse.cz>
---
 include/linux/bpf.h |   58 ++++++++++++++++++++++++++++++++++++++++++++++++++++
 1 file changed, 58 insertions(+)

--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@ -30,6 +30,7 @@
 #include <linux/static_call.h>
 #include <linux/memcontrol.h>
 #include <linux/cfi.h>
+#include <linux/build_bug.h>
 
 struct bpf_verifier_env;
 struct bpf_verifier_log;
@@ -299,7 +300,62 @@ struct bpf_map {
 		enum bpf_prog_type type;
 		bool jited;
 		bool xdp_has_frags;
+#ifndef __GENKSYMS__
 		enum bpf_attach_type expected_attach_type;
+#endif
+	} owner;
+	bool bypass_spec_v1;
+	bool frozen; /* write-once; write-protected by freeze_mutex */
+	bool free_after_mult_rcu_gp;
+	bool free_after_rcu_gp;
+	atomic64_t sleepable_refcnt;
+	s64 __percpu *elem_count;
+	void *suse_kabi_padding;
+};
+
+struct __orig_bpf_map {
+	const struct bpf_map_ops *ops;
+	struct bpf_map *inner_map_meta;
+#ifdef CONFIG_SECURITY
+	void *security;
+#endif
+	enum bpf_map_type map_type;
+	u32 key_size;
+	u32 value_size;
+	u32 max_entries;
+	u64 map_extra; /* any per-map-type extra fields */
+	u32 map_flags;
+	u32 id;
+	struct btf_record *record;
+	int numa_node;
+	u32 btf_key_type_id;
+	u32 btf_value_type_id;
+	u32 btf_vmlinux_value_type_id;
+	struct btf *btf;
+#ifdef CONFIG_MEMCG
+	struct obj_cgroup *objcg;
+#endif
+	char name[BPF_OBJ_NAME_LEN];
+	struct mutex freeze_mutex;
+	atomic64_t refcnt;
+	atomic64_t usercnt;
+	/* rcu is used before freeing and work is only used during freeing */
+	union {
+		struct work_struct work;
+		struct rcu_head rcu;
+	};
+	atomic64_t writecnt;
+	/* 'Ownership' of program-containing map is claimed by the first program
+	 * that is going to use this map or by the first program which FD is
+	 * stored in the map to make sure that all callers and callees have the
+	 * same prog type, JITed flag and xdp_has_frags flag.
+	 */
+	struct {
+		const struct btf_type *attach_func_proto;
+		spinlock_t lock;
+		enum bpf_prog_type type;
+		bool jited;
+		bool xdp_has_frags;
 	} owner;
 	bool bypass_spec_v1;
 	bool frozen; /* write-once; write-protected by freeze_mutex */
@@ -310,6 +366,8 @@ struct bpf_map {
 	void *suse_kabi_padding;
 };
 
+suse_kabi_static_assert(offsetof(struct bpf_map, bypass_spec_v1) == offsetof(struct __orig_bpf_map, bypass_spec_v1));
+
 static inline const char *btf_field_type_name(enum btf_field_type type)
 {
 	switch (type) {
