From: Shung-Hsi Yu <shung-hsi.yu@suse.com>
Subject: kABI: bpf: verifier kABI workaround
Patch-mainline: never, kabi
References: bsc#1225903

Workaround kABI breakage in bpf_func_state, bpf_verifier_state, and bpf_insn_aux_data:

- Commit bb124da69c47d ("bpf: keep track of max number of bpf_loop callback
  iterations") added a callback_depth field in "struct bpf_func_state" and
  break kABI, workaround the breakage by moving the callback_depth field to the
  end of the structure as it is only accessed through pointers.

  However, copy_func_state() in kernel/bpf/verifier.c has to be updated as well
  so it will be copied along with other fields before the acquired_refs field.

- Commit ab5cfac139ab8 ("bpf: verify callbacks as if they are called unknown
  number of times") and commit 2a0992829ea38 ("bpf: correct loop detection for
  iterators convergence") added several fields in struct bpf_verifier_state.
  with the exception of "bool used_as_loop_entry", which fit into the padding,
  move the new fields to a new structure, struct suse_bpf_verifier_state_extra,
  to workaround kABI breakage.

  This new structure is allocated in the same chunk of memory as struct
  bpf_verifier_state{,_list,_stack_elem}, and comes before them. This required
  changing various k*alloc/kfree call sites. To make things easier, the struct
  suse_bpf_verifier_state_wrapper and the extra() helper was introduced.

- Commit ab5cfac139ab8 ("bpf: verify callbacks as if they are called unknown
  number of times") adds a calls_callback field to struct bpf_insn_aux_data as
  the last field. Simply hiding it from kernel symbol generation is enough
  since it is already the last field and struct bpf_insn_aux_data is accessed
  through pointers.

- Commit 45b5623f2d72 ("bpf: rearrange bpf_func_state fields to save a bit of
  memory") rearranged field in bpf_func_state, simply move the back to where
  they were.

---
 include/linux/bpf_verifier.h |  139 ++++++++++++++++++++++++++++++++++---------
 kernel/bpf/verifier.c        |   79 +++++++++++++-----------
 2 files changed, 155 insertions(+), 63 deletions(-)

--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -302,24 +302,17 @@ struct bpf_func_state {
 	 * void foo(void) { bpf_timer_set_callback(,foo); }
 	 */
 	u32 async_entry_cnt;
-	struct bpf_retval_range callback_ret_range;
 	bool in_callback_fn;
+	struct bpf_retval_range callback_ret_range;
 	bool in_async_callback_fn;
-	/* For callback calling functions that limit number of possible
-	 * callback executions (e.g. bpf_loop) keeps track of current
-	 * simulated iteration number.
-	 * Value in frame N refers to number of times callback with frame
-	 * N+1 was simulated, e.g. for the following call:
-	 *
-	 *   bpf_loop(..., fn, ...); | suppose current frame is N
-	 *                           | fn would be simulated in frame N+1
-	 *                           | number of simulations is tracked in frame N
-	 */
-	u32 callback_depth;
 
 	/* The following fields should be last. See copy_func_state() */
 	int acquired_refs;
 	struct bpf_reference_state *refs;
+	/* Size of the current stack, in bytes. The stack state is tracked below, in
+	 * `stack`. allocated_stack is always a multiple of BPF_REG_SIZE.
+	 */
+	int allocated_stack;
 	/* The state of the stack. Each element of the array describes BPF_REG_SIZE
 	 * (i.e. 8) bytes worth of stack memory.
 	 * stack[0] represents bytes [*(r10-8)..*(r10-1)]
@@ -328,10 +321,24 @@ struct bpf_func_state {
 	 * stack[allocated_stack/8 - 1] represents [*(r10-allocated_stack)..*(r10-allocated_stack+7)]
 	 */
 	struct bpf_stack_state *stack;
-	/* Size of the current stack, in bytes. The stack state is tracked below, in
-	 * `stack`. allocated_stack is always a multiple of BPF_REG_SIZE.
+
+	/* syu: fields that after this has to be copied manully in
+	 * copy_func_state() to keep the code working while preserving kABI at
+	 * the same time.
 	 */
-	int allocated_stack;
+#ifndef __GENKSYMS__
+	/* For callback calling functions that limit number of possible
+	 * callback executions (e.g. bpf_loop) keeps track of current
+	 * simulated iteration number.
+	 * Value in frame N refers to number of times callback with frame
+	 * N+1 was simulated, e.g. for the following call:
+	 *
+	 *   bpf_loop(..., fn, ...); | suppose current frame is N
+	 *                           | fn would be simulated in frame N+1
+	 *                           | number of simulations is tracked in frame N
+	 */
+	u32 callback_depth;
+#endif /* __GENKSYMS__ */
 };
 
 #define MAX_CALL_FRAMES 8
@@ -354,14 +361,26 @@ enum {
 static_assert(INSN_F_FRAMENO_MASK + 1 >= MAX_CALL_FRAMES);
 static_assert(INSN_F_SPI_MASK + 1 >= MAX_BPF_STACK / 8);
 
+/* Original definition that is no longer used but kept to preserve kABI */
+struct bpf_idx_pair {
+	u32 prev_idx;
+	u32 idx;
+};
+
 struct bpf_jmp_history_entry {
+	u32 prev_idx;
 	u32 idx;
-	/* insn idx can't be bigger than 1 million */
-	u32 prev_idx : 22;
-	/* special flags, e.g., whether insn is doing register stack spill/load */
-	u32 flags : 10;
+	u32 flags;
 };
 
+/* Make sure "u32 prev_idx" location stays the same */
+static_assert(offsetof(struct bpf_jmp_history_entry, prev_idx) ==
+		offsetof(struct bpf_idx_pair, prev_idx));
+
+/* Make sure "u32 idx" location stays the same */
+static_assert(offsetof(struct bpf_jmp_history_entry, idx) ==
+		offsetof(struct bpf_idx_pair, idx));
+
 /* Maximum number of register states that can exist at once */
 #define BPF_ID_MAP_SIZE ((MAX_BPF_REG + MAX_BPF_STACK / BPF_REG_SIZE) * MAX_CALL_FRAMES)
 struct bpf_verifier_state {
@@ -420,15 +439,36 @@ struct bpf_verifier_state {
 	struct bpf_active_lock active_lock;
 	bool speculative;
 	bool active_rcu_lock;
+#ifndef __GENKSYMS__
 	/* If this state was ever pointed-to by other state's loop_entry field
 	 * this flag would be set to true. Used to avoid freeing such states
 	 * while they are still in use.
 	 */
 	bool used_as_loop_entry;
+#endif /* __GENKSYMS__ */
 
 	/* first and last insn idx of this verifier state */
 	u32 first_insn_idx;
 	u32 last_insn_idx;
+	/* jmp history recorded from first to last.
+	 * backtracking is using it to go from last to first.
+	 * For most states jmp_history_cnt is [0-3].
+	 * For loops can go up to ~40.
+	 */
+#ifndef __GENKSYMS__
+	struct bpf_jmp_history_entry *jmp_history;
+#else
+	struct bpf_idx_pair *jmp_history;
+#endif
+	u32 jmp_history_cnt;
+};
+
+/* Used for kABI workaround. This must be position right before
+ * bpf_verifier_state in memory.
+ */
+struct suse_bpf_verifier_state_extra {
+	u32 dfs_depth;
+	u32 callback_unroll_depth;
 	/* If this state is a part of states loop this field points to some
 	 * parent of this state such that:
 	 * - it is also a member of the same states loop;
@@ -439,17 +479,53 @@ struct bpf_verifier_state {
 	 * See get_loop_entry() for more information.
 	 */
 	struct bpf_verifier_state *loop_entry;
-	/* jmp history recorded from first to last.
-	 * backtracking is using it to go from last to first.
-	 * For most states jmp_history_cnt is [0-3].
-	 * For loops can go up to ~40.
-	 */
-	struct bpf_jmp_history_entry *jmp_history;
-	u32 jmp_history_cnt;
-	u32 dfs_depth;
-	u32 callback_unroll_depth;
 };
 
+/* Used for kABI workaround. Make accessing suse_bpf_verifier_state_extra from
+ * bpf_verifier_state pointer easier.
+ */
+struct suse_bpf_verifier_state_wrapper {
+	struct suse_bpf_verifier_state_extra extra;
+	struct bpf_verifier_state state;
+};
+
+/* kABI workarund. Get pointer to struct suse_bpf_verifier_state_extra from a
+ * pointer to struct suse_bpf_verifier_state. This works because we ensure all
+ * allocation of bpf_verifier_state (as well as bpf_verifier_stack_elem and
+ * bpf_verifier_state_list that embeds it) has struct bpf_verifier_state_extra
+ * right before it.
+ */
+static inline struct suse_bpf_verifier_state_extra *extra(struct bpf_verifier_state *state)
+{
+	struct suse_bpf_verifier_state_wrapper *wrapper;
+	wrapper = container_of(state, struct suse_bpf_verifier_state_wrapper, state);
+	return &wrapper->extra;
+}
+
+/* The original state of "struct bpf_verifier_state" before kABI changes */
+struct __orig_bpf_verifier_state {
+	struct bpf_func_state *frame[MAX_CALL_FRAMES];
+	struct bpf_verifier_state *parent;
+	u32 branches;
+	u32 insn_idx;
+	u32 curframe;
+	struct bpf_active_lock active_lock;
+	bool speculative;
+	bool active_rcu_lock;
+	u32 first_insn_idx;
+	u32 last_insn_idx;
+ 	struct bpf_idx_pair *jmp_history;
+ 	u32 jmp_history_cnt;
+};
+
+/* Make sure "bool used_as_loop_entry" field does not affect memory layout */
+static_assert(offsetof(struct bpf_verifier_state, first_insn_idx) ==
+		offsetof(struct __orig_bpf_verifier_state, first_insn_idx));
+
+/* Make sure size of "struct bpf_verifier_state" stays the same */
+static_assert(sizeof(struct bpf_verifier_state) ==
+		sizeof(struct __orig_bpf_verifier_state));
+
 #define bpf_get_spilled_reg(slot, frame)				\
 	(((slot < frame->allocated_stack / BPF_REG_SIZE) &&		\
 	  (frame->stack[slot].slot_type[0] == STACK_SPILL))		\
@@ -555,10 +631,12 @@ struct bpf_insn_aux_data {
 	 * this instruction, regardless of any heuristics
 	 */
 	bool force_checkpoint;
+#ifndef __GENKSYMS__
 	/* true if instruction is a call to a helper function that
 	 * accepts callback function as a parameter.
 	 */
 	bool calls_callback;
+#endif /* __GENKSYMS__ */
 };
 
 #define MAX_USED_MAPS 64 /* max number of maps accessed by one eBPF program */
@@ -682,7 +760,6 @@ struct bpf_verifier_env {
 		int cur_stack;
 	} cfg;
 	struct backtrack_state bt;
-	struct bpf_jmp_history_entry *cur_hist_ent;
 	u32 pass_cnt; /* number of times do_check() was called */
 	u32 subprog_cnt;
 	/* number of instructions analyzed by the verifier */
@@ -715,7 +792,11 @@ struct bpf_verifier_env {
 	 * e.g., in reg_type_str() to generate reg_type string
 	 */
 	char tmp_str_buf[TMP_STR_BUF_LEN];
+#ifndef __GENKSYMS__
+	struct bpf_jmp_history_entry *cur_hist_ent;
+#else
 	void *suse_kabi_padding;
+#endif
 };
 
 __printf(2, 0) void bpf_verifier_vlog(struct bpf_verifier_log *log,
--- a/kernel/bpf/verifier.c
+++ b/kernel/bpf/verifier.c
@@ -1744,7 +1744,7 @@ static void free_verifier_state(struct b
 	}
 	clear_jmp_history(state);
 	if (free_self)
-		kfree(state);
+		kfree(extra(state));
 }
 
 /* copy verifier state from src to dst growing dst stack space
@@ -1756,6 +1756,7 @@ static int copy_func_state(struct bpf_fu
 	int err;
 
 	memcpy(dst, src, offsetof(struct bpf_func_state, acquired_refs));
+	dst->callback_depth = src->callback_depth;
 	err = copy_reference_state(dst, src);
 	if (err)
 		return err;
@@ -1765,6 +1766,7 @@ static int copy_func_state(struct bpf_fu
 static int copy_verifier_state(struct bpf_verifier_state *dst_state,
 			       const struct bpf_verifier_state *src)
 {
+	const struct suse_bpf_verifier_state_extra *src_extra = &container_of(src, const struct suse_bpf_verifier_state_wrapper, state)->extra;
 	struct bpf_func_state *dst;
 	int i, err;
 
@@ -1789,10 +1791,10 @@ static int copy_verifier_state(struct bp
 	dst_state->parent = src->parent;
 	dst_state->first_insn_idx = src->first_insn_idx;
 	dst_state->last_insn_idx = src->last_insn_idx;
-	dst_state->dfs_depth = src->dfs_depth;
-	dst_state->callback_unroll_depth = src->callback_unroll_depth;
+	extra(dst_state)->dfs_depth = src_extra->dfs_depth;
+	extra(dst_state)->callback_unroll_depth = src_extra->callback_unroll_depth;
 	dst_state->used_as_loop_entry = src->used_as_loop_entry;
-	dst_state->loop_entry = src->loop_entry;
+	extra(dst_state)->loop_entry = src_extra->loop_entry;
 	for (i = 0; i <= src->curframe; i++) {
 		dst = dst_state->frame[i];
 		if (!dst) {
@@ -1959,16 +1961,16 @@ static bool same_callsites(struct bpf_ve
  */
 static struct bpf_verifier_state *get_loop_entry(struct bpf_verifier_state *st)
 {
-	struct bpf_verifier_state *topmost = st->loop_entry, *old;
+	struct bpf_verifier_state *topmost = extra(st)->loop_entry, *old;
 
-	while (topmost && topmost->loop_entry && topmost != topmost->loop_entry)
-		topmost = topmost->loop_entry;
+	while (topmost && extra(topmost)->loop_entry && topmost != extra(topmost)->loop_entry)
+		topmost = extra(topmost)->loop_entry;
 	/* Update loop entries for intermediate states to avoid this
 	 * traversal in future get_loop_entry() calls.
 	 */
-	while (st && st->loop_entry != topmost) {
-		old = st->loop_entry;
-		st->loop_entry = topmost;
+	while (st && extra(st)->loop_entry != topmost) {
+		old = extra(st)->loop_entry;
+		extra(st)->loop_entry = topmost;
 		st = old;
 	}
 	return topmost;
@@ -1986,8 +1988,8 @@ static void update_loop_entry(struct bpf
 	 * hence 'cur' and 'hdr' are not in the same loop and there is
 	 * no need to update cur->loop_entry.
 	 */
-	if (hdr1->branches && hdr1->dfs_depth <= cur1->dfs_depth) {
-		cur->loop_entry = hdr;
+	if (hdr1->branches && extra(hdr1)->dfs_depth <= extra(cur1)->dfs_depth) {
+		extra(cur)->loop_entry = hdr;
 		hdr->used_as_loop_entry = true;
 	}
 }
@@ -2002,8 +2004,8 @@ static void update_branch_counts(struct
 		 * turned out that st is a part of some loop.
 		 * This is a part of 'case A' in get_loop_entry() comment.
 		 */
-		if (br == 0 && st->parent && st->loop_entry)
-			update_loop_entry(st->parent, st->loop_entry);
+		if (br == 0 && st->parent && extra(st)->loop_entry)
+			update_loop_entry(st->parent, extra(st)->loop_entry);
 
 		/* WARN_ON(br > 1) technically makes sense here,
 		 * but see comment in push_stack(), hence:
@@ -2040,7 +2042,7 @@ static int pop_stack(struct bpf_verifier
 		*prev_insn_idx = head->prev_insn_idx;
 	elem = head->next;
 	free_verifier_state(&head->st, false);
-	kfree(head);
+	kfree(extra(&head->st));
 	env->head = elem;
 	env->stack_size--;
 	return 0;
@@ -2050,14 +2052,17 @@ static struct bpf_verifier_state *push_s
 					     int insn_idx, int prev_insn_idx,
 					     bool speculative)
 {
+	struct suse_bpf_verifier_state_wrapper *elem_wrapper;
 	struct bpf_verifier_state *cur = env->cur_state;
 	struct bpf_verifier_stack_elem *elem;
 	int err;
 
-	elem = kzalloc(sizeof(struct bpf_verifier_stack_elem), GFP_KERNEL);
-	if (!elem)
+
+	elem_wrapper = kzalloc(sizeof(struct suse_bpf_verifier_state_extra) + sizeof(struct bpf_verifier_stack_elem), GFP_KERNEL);
+	if (!elem_wrapper)
 		goto err;
 
+	elem = (struct bpf_verifier_stack_elem *) &elem_wrapper->state;
 	elem->insn_idx = insn_idx;
 	elem->prev_insn_idx = prev_insn_idx;
 	elem->next = env->head;
@@ -2613,13 +2618,15 @@ static struct bpf_verifier_state *push_a
 						int insn_idx, int prev_insn_idx,
 						int subprog)
 {
+	struct suse_bpf_verifier_state_wrapper *elem_wrapper;
 	struct bpf_verifier_stack_elem *elem;
 	struct bpf_func_state *frame;
 
-	elem = kzalloc(sizeof(struct bpf_verifier_stack_elem), GFP_KERNEL);
-	if (!elem)
+	elem_wrapper = kzalloc(sizeof(struct suse_bpf_verifier_state_extra) + sizeof(struct bpf_verifier_stack_elem), GFP_KERNEL);
+	if (!elem_wrapper)
 		goto err;
 
+	elem = (struct bpf_verifier_stack_elem *) &elem_wrapper->state;
 	elem->insn_idx = insn_idx;
 	elem->prev_insn_idx = prev_insn_idx;
 	elem->next = env->head;
@@ -7891,7 +7898,7 @@ static struct bpf_verifier_state *find_p
 		 */
 		st = &sl->state;
 		if (st->insn_idx == insn_idx && st->branches && same_callsites(st, cur) &&
-		    st->dfs_depth < cur->dfs_depth)
+		    extra(st)->dfs_depth < extra(cur)->dfs_depth)
 			return st;
 	}
 
@@ -9405,7 +9412,7 @@ static int push_callback_call(struct bpf
 	if (err)
 		return err;
 
-	callback_state->callback_unroll_depth++;
+	extra(callback_state)->callback_unroll_depth++;
 	callback_state->frame[callback_state->curframe - 1]->callback_depth++;
 	caller->callback_depth = 0;
 	return 0;
@@ -16706,6 +16713,7 @@ static bool iter_active_depths_differ(st
 
 static int is_state_visited(struct bpf_verifier_env *env, int insn_idx)
 {
+	struct suse_bpf_verifier_state_wrapper *new_sl_wrapper;
 	struct bpf_verifier_state_list *new_sl;
 	struct bpf_verifier_state_list *sl, **pprev;
 	struct bpf_verifier_state *cur = env->cur_state, *new, *loop_entry;
@@ -16823,7 +16831,7 @@ static int is_state_visited(struct bpf_v
 			if (states_maybe_looping(&sl->state, cur) &&
 			    states_equal(env, &sl->state, cur, false) &&
 			    !iter_active_depths_differ(&sl->state, cur) &&
-			    sl->state.callback_unroll_depth == cur->callback_unroll_depth) {
+			    extra(&sl->state)->callback_unroll_depth == extra(cur)->callback_unroll_depth) {
 				verbose_linfo(env, insn_idx, "; ");
 				verbose(env, "infinite loop detected at insn %d\n", insn_idx);
 				verbose(env, "cur state:");
@@ -16938,7 +16946,7 @@ miss:
 					  "BUG live_done but branches_to_explore %d\n",
 					  br);
 				free_verifier_state(&sl->state, false);
-				kfree(sl);
+				kfree(extra(&sl->state));
 				env->peak_states--;
 			} else {
 				/* cannot free this state, since parentage chain may
@@ -16974,9 +16982,10 @@ next:
 	 * When looping the sl->state.branches will be > 0 and this state
 	 * will not be considered for equivalence until branches == 0.
 	 */
-	new_sl = kzalloc(sizeof(struct bpf_verifier_state_list), GFP_KERNEL);
-	if (!new_sl)
+	new_sl_wrapper = kzalloc(sizeof(struct suse_bpf_verifier_state_extra) + sizeof(struct bpf_verifier_state_list), GFP_KERNEL);
+	if (!new_sl_wrapper)
 		return -ENOMEM;
+	new_sl = (struct bpf_verifier_state_list *) &new_sl_wrapper->state;
 	env->total_states++;
 	env->peak_states++;
 	env->prev_jmps_processed = env->jmps_processed;
@@ -16991,7 +17000,7 @@ next:
 	err = copy_verifier_state(new, cur);
 	if (err) {
 		free_verifier_state(new, false);
-		kfree(new_sl);
+		kfree(extra(&new_sl->state));
 		return err;
 	}
 	new->insn_idx = insn_idx;
@@ -17000,7 +17009,7 @@ next:
 
 	cur->parent = new;
 	cur->first_insn_idx = insn_idx;
-	cur->dfs_depth = new->dfs_depth + 1;
+	extra(cur)->dfs_depth = extra(new)->dfs_depth + 1;
 	clear_jmp_history(cur);
 	new_sl->next = *explored_state(env, insn_idx);
 	*explored_state(env, insn_idx) = new_sl;
@@ -17418,8 +17427,8 @@ process_bpf_exit:
 						return err;
 					break;
 				} else {
-					if (WARN_ON_ONCE(env->cur_state->loop_entry))
-						env->cur_state->loop_entry = NULL;
+					if (WARN_ON_ONCE(extra(env->cur_state)->loop_entry))
+						extra(env->cur_state)->loop_entry = NULL;
 					do_print_state = true;
 					continue;
 				}
@@ -19799,7 +19808,7 @@ static void free_states(struct bpf_verif
 	while (sl) {
 		sln = sl->next;
 		free_verifier_state(&sl->state, false);
-		kfree(sl);
+		kfree(extra(&sl->state));
 		sl = sln;
 	}
 	env->free_list = NULL;
@@ -19813,7 +19822,7 @@ static void free_states(struct bpf_verif
 		while (sl) {
 			sln = sl->next;
 			free_verifier_state(&sl->state, false);
-			kfree(sl);
+			kfree(extra(&sl->state));
 			sl = sln;
 		}
 		env->explored_states[i] = NULL;
@@ -19822,6 +19831,7 @@ static void free_states(struct bpf_verif
 
 static int do_check_common(struct bpf_verifier_env *env, int subprog)
 {
+	struct suse_bpf_verifier_state_wrapper *state_wrapper;
 	bool pop_log = !(env->log.level & BPF_LOG_LEVEL2);
 	struct bpf_verifier_state *state;
 	struct bpf_reg_state *regs;
@@ -19830,15 +19840,16 @@ static int do_check_common(struct bpf_ve
 	env->prev_linfo = NULL;
 	env->pass_cnt++;
 
-	state = kzalloc(sizeof(struct bpf_verifier_state), GFP_KERNEL);
-	if (!state)
+	state_wrapper = kzalloc(sizeof(struct suse_bpf_verifier_state_wrapper), GFP_KERNEL);
+	if (!state_wrapper)
 		return -ENOMEM;
+	state = &state_wrapper->state;
 	state->curframe = 0;
 	state->speculative = false;
 	state->branches = 1;
 	state->frame[0] = kzalloc(sizeof(struct bpf_func_state), GFP_KERNEL);
 	if (!state->frame[0]) {
-		kfree(state);
+		kfree(state_wrapper);
 		return -ENOMEM;
 	}
 	env->cur_state = state;
