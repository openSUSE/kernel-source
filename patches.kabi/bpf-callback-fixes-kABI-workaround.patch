From: Shung-Hsi Yu <shung-hsi.yu@suse.com>
Subject: kABI: bpf: callback fixes kABI workaround
Patch-mainline: never, kabi
References: bsc#1225903

- Commit bb124da69c47d ("bpf: keep track of max number of bpf_loop callback
  iterations") added a callback_depth field in "struct bpf_func_state" and
  break kABI, workaround the breakage by moving the callback_depth field to the
  end of the structure as it is only accessed through pointers.

  However, copy_func_state() in kernel/bpf/verifier.c has to be updated as well
  so it will be copied along with other fields before the acquired_refs field.

- Commit ab5cfac139ab8 ("bpf: verify callbacks as if they are called unknown
  number of times") and commit 2a0992829ea38 ("bpf: correct loop detection for
  iterators convergence") added several fields in struct bpf_verifier_state.
  with the exception of "bool used_as_loop_entry", which fit into the padding,
  move the new fields to a new structure, struct suse_bpf_verifier_state_extra,
  to workaround kABI breakage.

  This new structure is allocated in the same chunk of memory as struct
  bpf_verifier_state{,_list,_stack_elem}, and comes before them. This required
  changing various k*alloc/kfree call sites. To make things easier, the struct
  suse_bpf_verifier_state_wrapper and the extra() helper was introduced.

---
 include/linux/bpf_verifier.h |   66 +++++++++++++++++++++++---------
 kernel/bpf/verifier.c        |   87 ++++++++++++++++++++++++-------------------
 2 files changed, 97 insertions(+), 56 deletions(-)

--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -255,17 +255,6 @@ struct bpf_func_state {
 	u32 async_entry_cnt;
 	bool in_callback_fn;
 	bool in_async_callback_fn;
-	/* For callback calling functions that limit number of possible
-	 * callback executions (e.g. bpf_loop) keeps track of current
-	 * simulated iteration number.
-	 * Value in frame N refers to number of times callback with frame
-	 * N+1 was simulated, e.g. for the following call:
-	 *
-	 *   bpf_loop(..., fn, ...); | suppose current frame is N
-	 *                           | fn would be simulated in frame N+1
-	 *                           | number of simulations is tracked in frame N
-	 */
-	u32 callback_depth;
 
 	/* The following fields should be last. See copy_func_state() */
 	int acquired_refs;
@@ -282,6 +271,19 @@ struct bpf_func_state {
 	 * stack[allocated_stack/8 - 1] represents [*(r10-allocated_stack)..*(r10-allocated_stack+7)]
 	 */
 	struct bpf_stack_state *stack;
+#ifndef __GENKSYMS__
+	/* For callback calling functions that limit number of possible
+	 * callback executions (e.g. bpf_loop) keeps track of current
+	 * simulated iteration number.
+	 * Value in frame N refers to number of times callback with frame
+	 * N+1 was simulated, e.g. for the following call:
+	 *
+	 *   bpf_loop(..., fn, ...); | suppose current frame is N
+	 *                           | fn would be simulated in frame N+1
+	 *                           | number of simulations is tracked in frame N
+	 */
+	u32 callback_depth;
+#endif /* __GENKSYMS__ */
 };
 
 #define MAX_CALL_FRAMES 8
@@ -369,11 +371,6 @@ struct bpf_verifier_state {
 	u32 curframe;
 	u32 active_spin_lock;
 	bool speculative;
-	/* If this state was ever pointed-to by other state's loop_entry field
-	 * this flag would be set to true. Used to avoid freeing such states
-	 * while they are still in use.
-	 */
-	bool used_as_loop_entry;
 
 	/* first and last insn idx of this verifier state */
 	u32 first_insn_idx;
@@ -387,7 +384,6 @@ struct bpf_verifier_state {
 	 * State loops might appear because of open coded iterators logic.
 	 * See get_loop_entry() for more information.
 	 */
-	struct bpf_verifier_state *loop_entry;
 	/* jmp history recorded from first to last.
 	 * backtracking is using it to go from last to first.
 	 * For most states jmp_history_cnt is [0-3].
@@ -395,10 +391,44 @@ struct bpf_verifier_state {
 	 */
 	struct bpf_jmp_history_entry *jmp_history;
 	u32 jmp_history_cnt;
+};
+
+/* Used for kABI workaround. This must be position right before
+ * bpf_verifier_state in memory.
+ */
+struct suse_bpf_verifier_state_extra {
+	/* If this state was ever pointed-to by other state's loop_entry field
+	 * this flag would be set to true. Used to avoid freeing such states
+	 * while they are still in use.
+	 */
+	bool used_as_loop_entry;
+	struct bpf_verifier_state *loop_entry;
 	u32 dfs_depth;
 	u32 callback_unroll_depth;
 };
 
+/* Used for kABI workaround. Make accessing suse_bpf_verifier_state_extra from
+ * bpf_verifier_state pointer easier.
+ */
+struct suse_bpf_verifier_state_wrapper {
+	struct suse_bpf_verifier_state_extra extra;
+	struct bpf_verifier_state state;
+};
+
+/* kABI workarund. Get pointer to struct bpf_verifier_state_extra from a
+ * pointer to struct bpf_verifier_state. This works because we ensure all
+ * allocation of bpf_verifier_state (as well as bpf_verifier_stack_elem and
+ * bpf_verifier_state_list that embeds it) has struct
+ * suse_bpf_verifier_state_extra right before it.
+ */
+static inline struct suse_bpf_verifier_state_extra *extra(struct bpf_verifier_state *state)
+{
+	struct suse_bpf_verifier_state_wrapper *wrapper;
+	wrapper = container_of(state, struct suse_bpf_verifier_state_wrapper, state);
+	return &wrapper->extra;
+}
+
+
 #define bpf_get_spilled_reg(slot, frame)				\
 	(((slot < frame->allocated_stack / BPF_REG_SIZE) &&		\
 	  (frame->stack[slot].slot_type[0] == STACK_SPILL))		\
@@ -619,7 +649,6 @@ struct bpf_verifier_env {
 		int *insn_stack;
 		int cur_stack;
 	} cfg;
-	struct backtrack_state bt;
 	struct bpf_jmp_history_entry *cur_hist_ent;
 	u32 pass_cnt; /* number of times do_check() was called */
 	u32 subprog_cnt;
@@ -658,6 +687,7 @@ struct bpf_verifier_env {
 		struct bpf_idmap idmap_scratch;
 		struct bpf_idset idset_scratch;
 	};
+	struct backtrack_state bt;
 #else
 	char type_str_buf[TYPE_STR_BUF_LEN_OLD];
 #endif /* __GENKSYMS__ */
--- a/kernel/bpf/verifier.c
+++ b/kernel/bpf/verifier.c
@@ -1363,8 +1363,12 @@ static void free_verifier_state(struct b
 		state->frame[i] = NULL;
 	}
 	clear_jmp_history(state);
-	if (free_self)
-		kfree(state);
+	if (free_self) {
+		/* kABI workarund. Free from struct
+		 * suse_bpf_verifier_state_extra that always comes before
+		 * struct bpf_verifier_state. */
+		kfree(extra(state));
+	}
 }
 
 /* copy verifier state from src to dst growing dst stack space
@@ -1385,6 +1389,7 @@ static int copy_func_state(struct bpf_fu
 static int copy_verifier_state(struct bpf_verifier_state *dst_state,
 			       const struct bpf_verifier_state *src)
 {
+	const struct suse_bpf_verifier_state_extra *src_extra = &container_of(src, const struct suse_bpf_verifier_state_wrapper, state)->extra;
 	struct bpf_func_state *dst;
 	int i, err;
 
@@ -1407,9 +1412,9 @@ static int copy_verifier_state(struct bp
 	dst_state->parent = src->parent;
 	dst_state->first_insn_idx = src->first_insn_idx;
 	dst_state->last_insn_idx = src->last_insn_idx;
-	dst_state->dfs_depth = src->dfs_depth;
-	dst_state->callback_unroll_depth = src->callback_unroll_depth;
-	dst_state->used_as_loop_entry = src->used_as_loop_entry;
+	extra(dst_state)->dfs_depth = src_extra->dfs_depth;
+	extra(dst_state)->callback_unroll_depth = src_extra->callback_unroll_depth;
+	extra(dst_state)->used_as_loop_entry = src_extra->used_as_loop_entry;
 	for (i = 0; i <= src->curframe; i++) {
 		dst = dst_state->frame[i];
 		if (!dst) {
@@ -1576,16 +1581,16 @@ static bool same_callsites(struct bpf_ve
  */
 static struct bpf_verifier_state *get_loop_entry(struct bpf_verifier_state *st)
 {
-	struct bpf_verifier_state *topmost = st->loop_entry, *old;
+	struct bpf_verifier_state *topmost = extra(st)->loop_entry, *old;
 
-	while (topmost && topmost->loop_entry && topmost != topmost->loop_entry)
-		topmost = topmost->loop_entry;
+	while (topmost && extra(topmost)->loop_entry && topmost != extra(topmost)->loop_entry)
+		topmost = extra(topmost)->loop_entry;
 	/* Update loop entries for intermediate states to avoid this
 	 * traversal in future get_loop_entry() calls.
 	 */
-	while (st && st->loop_entry != topmost) {
-		old = st->loop_entry;
-		st->loop_entry = topmost;
+	while (st && extra(st)->loop_entry != topmost) {
+		old = extra(st)->loop_entry;
+		extra(st)->loop_entry = topmost;
 		st = old;
 	}
 	return topmost;
@@ -1601,11 +1606,11 @@ static void update_loop_entry(struct bpf
 	 * comment for get_loop_entry(). If hdr1->branches == 0 then
 	 * head's topmost loop entry is not in current DFS path,
 	 * hence 'cur' and 'hdr' are not in the same loop and there is
-	 * no need to update cur->loop_entry.
+	 * no need to update extra(cur)->loop_entry.
 	 */
-	if (hdr1->branches && hdr1->dfs_depth <= cur1->dfs_depth) {
-		cur->loop_entry = hdr;
-		hdr->used_as_loop_entry = true;
+	if (hdr1->branches && extra(hdr1)->dfs_depth <= extra(cur1)->dfs_depth) {
+		extra(cur)->loop_entry = hdr;
+		extra(hdr)->used_as_loop_entry = true;
 	}
 }
 
@@ -1619,8 +1624,8 @@ static void update_branch_counts(struct
 		 * turned out that st is a part of some loop.
 		 * This is a part of 'case A' in get_loop_entry() comment.
 		 */
-		if (br == 0 && st->parent && st->loop_entry)
-			update_loop_entry(st->parent, st->loop_entry);
+		if (br == 0 && st->parent && extra(st)->loop_entry)
+			update_loop_entry(st->parent, extra(st)->loop_entry);
 
 		/* WARN_ON(br > 1) technically makes sense here,
 		 * but see comment in push_stack(), hence:
@@ -1657,7 +1662,7 @@ static int pop_stack(struct bpf_verifier
 		*prev_insn_idx = head->prev_insn_idx;
 	elem = head->next;
 	free_verifier_state(&head->st, false);
-	kfree(head);
+	kfree(extra(&head->st));
 	env->head = elem;
 	env->stack_size--;
 	return 0;
@@ -1667,14 +1672,15 @@ static struct bpf_verifier_state *push_s
 					     int insn_idx, int prev_insn_idx,
 					     bool speculative)
 {
+	struct suse_bpf_verifier_state_wrapper *elem_wrapper;
 	struct bpf_verifier_state *cur = env->cur_state;
 	struct bpf_verifier_stack_elem *elem;
 	int err;
 
-	elem = kzalloc(sizeof(struct bpf_verifier_stack_elem), GFP_KERNEL);
-	if (!elem)
+	elem_wrapper = kzalloc(sizeof(struct suse_bpf_verifier_state_extra) + sizeof(struct bpf_verifier_stack_elem), GFP_KERNEL);
+	if (!elem_wrapper)
 		goto err;
-
+	elem = (struct bpf_verifier_stack_elem *) &elem_wrapper->state;
 	elem->insn_idx = insn_idx;
 	elem->prev_insn_idx = prev_insn_idx;
 	elem->next = env->head;
@@ -2178,13 +2184,14 @@ static struct bpf_verifier_state *push_a
 						int insn_idx, int prev_insn_idx,
 						int subprog)
 {
+	struct suse_bpf_verifier_state_wrapper *elem_wrapper;
 	struct bpf_verifier_stack_elem *elem;
 	struct bpf_func_state *frame;
 
-	elem = kzalloc(sizeof(struct bpf_verifier_stack_elem), GFP_KERNEL);
-	if (!elem)
+	elem_wrapper = kzalloc(sizeof(struct suse_bpf_verifier_state_extra) + sizeof(struct bpf_verifier_stack_elem), GFP_KERNEL);
+	if (!elem_wrapper)
 		goto err;
-
+	elem = (struct bpf_verifier_stack_elem *) &elem_wrapper->state;
 	elem->insn_idx = insn_idx;
 	elem->prev_insn_idx = prev_insn_idx;
 	elem->next = env->head;
@@ -6843,7 +6850,7 @@ static struct bpf_verifier_state *find_p
 		 */
 		st = &sl->state;
 		if (st->insn_idx == insn_idx && st->branches && same_callsites(st, cur) &&
-		    st->dfs_depth < cur->dfs_depth)
+		    extra(st)->dfs_depth < extra(cur)->dfs_depth)
 			return st;
 	}
 
@@ -8032,7 +8039,7 @@ static int push_callback_call(struct bpf
 	if (err)
 		return err;
 
-	callback_state->callback_unroll_depth++;
+	extra(callback_state)->callback_unroll_depth++;
 	callback_state->frame[callback_state->curframe - 1]->callback_depth++;
 	caller->callback_depth = 0;
 	return 0;
@@ -13272,6 +13279,7 @@ static bool states_maybe_looping(struct
 
 static int is_state_visited(struct bpf_verifier_env *env, int insn_idx)
 {
+	struct suse_bpf_verifier_state_wrapper *new_sl_wrapper;
 	struct bpf_verifier_state_list *new_sl;
 	struct bpf_verifier_state_list *sl, **pprev;
 	struct bpf_verifier_state *cur = env->cur_state, *new, *loop_entry;
@@ -13328,7 +13336,7 @@ static int is_state_visited(struct bpf_v
 			/* attempt to detect infinite loop to avoid unnecessary doomed work */
 			if (states_maybe_looping(&sl->state, cur) &&
 			    states_equal(env, &sl->state, cur, false) &&
-			    sl->state.callback_unroll_depth == cur->callback_unroll_depth) {
+			    extra(&sl->state)->callback_unroll_depth == extra(cur)->callback_unroll_depth) {
 				verbose_linfo(env, insn_idx, "; ");
 				verbose(env, "infinite loop detected at insn %d\n", insn_idx);
 				verbose(env, "cur state:");
@@ -13436,14 +13444,14 @@ miss:
 			 */
 			*pprev = sl->next;
 			if (sl->state.frame[0]->regs[0].live & REG_LIVE_DONE &&
-			    !sl->state.used_as_loop_entry) {
+			    !extra(&sl->state)->used_as_loop_entry) {
 				u32 br = sl->state.branches;
 
 				WARN_ONCE(br,
 					  "BUG live_done but branches_to_explore %d\n",
 					  br);
 				free_verifier_state(&sl->state, false);
-				kfree(sl);
+				kfree(extra(&sl->state));
 				env->peak_states--;
 			} else {
 				/* cannot free this state, since parentage chain may
@@ -13479,9 +13487,10 @@ next:
 	 * When looping the sl->state.branches will be > 0 and this state
 	 * will not be considered for equivalence until branches == 0.
 	 */
-	new_sl = kzalloc(sizeof(struct bpf_verifier_state_list), GFP_KERNEL);
-	if (!new_sl)
+	new_sl_wrapper = kzalloc(sizeof(struct suse_bpf_verifier_state_extra) + sizeof(struct bpf_verifier_state_list), GFP_KERNEL);
+	if (!new_sl_wrapper)
 		return -ENOMEM;
+	new_sl = (struct bpf_verifier_state_list *) &new_sl_wrapper->state;
 	env->total_states++;
 	env->peak_states++;
 	env->prev_jmps_processed = env->jmps_processed;
@@ -13496,7 +13505,7 @@ next:
 	err = copy_verifier_state(new, cur);
 	if (err) {
 		free_verifier_state(new, false);
-		kfree(new_sl);
+		kfree(extra(&new_sl->state));
 		return err;
 	}
 	new->insn_idx = insn_idx;
@@ -13505,7 +13514,7 @@ next:
 
 	cur->parent = new;
 	cur->first_insn_idx = insn_idx;
-	cur->dfs_depth = new->dfs_depth + 1;
+	extra(cur)->dfs_depth = extra(new)->dfs_depth + 1;
 	clear_jmp_history(cur);
 	new_sl->next = *explored_state(env, insn_idx);
 	*explored_state(env, insn_idx) = new_sl;
@@ -15918,7 +15927,7 @@ static void free_states(struct bpf_verif
 	while (sl) {
 		sln = sl->next;
 		free_verifier_state(&sl->state, false);
-		kfree(sl);
+		kfree(extra(&sl->state));
 		sl = sln;
 	}
 	env->free_list = NULL;
@@ -15932,7 +15941,7 @@ static void free_states(struct bpf_verif
 		while (sl) {
 			sln = sl->next;
 			free_verifier_state(&sl->state, false);
-			kfree(sl);
+			kfree(extra(&sl->state));
 			sl = sln;
 		}
 		env->explored_states[i] = NULL;
@@ -15941,6 +15950,7 @@ static void free_states(struct bpf_verif
 
 static int do_check_common(struct bpf_verifier_env *env, int subprog)
 {
+	struct suse_bpf_verifier_state_wrapper *state_wrapper;
 	bool pop_log = !(env->log.level & BPF_LOG_LEVEL2);
 	struct bpf_verifier_state *state;
 	struct bpf_reg_state *regs;
@@ -15949,15 +15959,16 @@ static int do_check_common(struct bpf_ve
 	env->prev_linfo = NULL;
 	env->pass_cnt++;
 
-	state = kzalloc(sizeof(struct bpf_verifier_state), GFP_KERNEL);
-	if (!state)
+	state_wrapper = kzalloc(sizeof(struct suse_bpf_verifier_state_wrapper), GFP_KERNEL);
+	if (!state_wrapper)
 		return -ENOMEM;
+	state = &state_wrapper->state;
 	state->curframe = 0;
 	state->speculative = false;
 	state->branches = 1;
 	state->frame[0] = kzalloc(sizeof(struct bpf_func_state), GFP_KERNEL);
 	if (!state->frame[0]) {
-		kfree(state);
+		kfree(state_wrapper);
 		return -ENOMEM;
 	}
 	env->cur_state = state;
