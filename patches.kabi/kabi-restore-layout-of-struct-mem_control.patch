From: Michal Koutný <mkoutny@suse.com
Date: Fri, 20 Jun 2025 15:11:31 +0200
Subject: kabi: restore layout of struct mem_control
Patch-mainline: Never, kabi workaround
References: jsc#PED-12551

We need to find place for three new members:
        struct list_head           memory_peaks;         /*   640    16 */
        struct list_head           swap_peaks;           /*   656    16 */
        spinlock_t                 peaks_lock;           /*   672     4 */

Fortunately, there are many large holes in cache-line paddings in struct
mem_cgroup. Avoid the first hole after mem_cgroup::id (might be hot) and use
the next available hole:

pahole -C mem_cgroup # 6.4.0-150600.23.47-default
	...
        spinlock_t                 move_lock;            /*  1464     4 */

        /* XXX 4 bytes hole, try to pack */

        /* --- cacheline 23 boundary (1472 bytes) --- */
        long unsigned int          move_lock_flags;      /*  1472     8 */

        /* XXX 56 bytes hole, try to pack */

        /* --- cacheline 24 boundary (1536 bytes) --- */
        struct cacheline_padding   _pad1_;               /*  1536     0 */
        struct memcg_vmstats *     vmstats __attribute__((__aligned__(64))); /*  1536     8 */
	...

There is no sufficient hole on armv7hl, so break KABI there (the arch defines
no KABI actually).

Similarly, the new members won't fit into the hole on -debug kernels because
spinlock_t is larger (and the hole is smaller due to different alignments),
allow breakage there too.

Signed-off-by: Michal Koutný <mkoutny@suse.com
---

---
 include/linux/memcontrol.h |  143 +++++++++++++++++++++++++++++++++++++++++++++
 1 file changed, 143 insertions(+)

--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@ -21,6 +21,8 @@
 #include <linux/vmstat.h>
 #include <linux/writeback.h>
 #include <linux/page-flags.h>
+#include <linux/build_bug.h>  /* for static_assert() */
+#include <linux/stddef.h>     /* for offsetof() */
 
 struct mem_cgroup;
 struct obj_cgroup;
@@ -222,10 +224,142 @@ struct mem_cgroup {
 	struct page_counter kmem;		/* v1 only */
 	struct page_counter tcpmem;		/* v1 only */
 
+	/* Range enforcement for interrupt charges */
+	struct work_struct high_work;
+
+#if defined(CONFIG_MEMCG_KMEM) && defined(CONFIG_ZSWAP)
+	unsigned long zswap_max;
+#endif
+
+	unsigned long soft_limit;
+
+	/* vmpressure notifications */
+	struct vmpressure vmpressure;
+
+	/*
+	 * Should the OOM killer kill all belonging tasks, had it kill one?
+	 */
+	bool oom_group;
+
+	/* protected by memcg_oom_lock */
+	bool		oom_lock;
+	int		under_oom;
+
+	int	swappiness;
+	/* OOM-Killer disable */
+	int		oom_kill_disable;
+
+	/* memory.events and memory.events.local */
+	struct cgroup_file events_file;
+	struct cgroup_file events_local_file;
+
+	/* handle for "memory.swap.events" */
+	struct cgroup_file swap_events_file;
+
+	/* protect arrays of thresholds */
+	struct mutex thresholds_lock;
+
+	/* thresholds for memory usage. RCU-protected */
+	struct mem_cgroup_thresholds thresholds;
+
+	/* thresholds for mem+swap usage. RCU-protected */
+	struct mem_cgroup_thresholds memsw_thresholds;
+
+	/* For oom notifier event fd */
+	struct list_head oom_notify;
+
+	/*
+	 * Should we move charges of a task when a task is moved into this
+	 * mem_cgroup ? And what type of charges should we move ?
+	 */
+	unsigned long move_charge_at_immigrate;
+	/* taken only while moving_account > 0 */
+	spinlock_t		move_lock;
+	unsigned long		move_lock_flags;
+
+#ifndef __GENKSYMS__
 	/* registered local peak watchers */
 	struct list_head memory_peaks;
 	struct list_head swap_peaks;
 	spinlock_t	 peaks_lock;
+#endif
+
+	CACHELINE_PADDING(_pad1_);
+
+	/* memory.stat */
+	struct memcg_vmstats	*vmstats;
+
+	/* memory.events */
+	atomic_long_t		memory_events[MEMCG_NR_MEMORY_EVENTS];
+	atomic_long_t		memory_events_local[MEMCG_NR_MEMORY_EVENTS];
+
+	/*
+	 * Hint of reclaim pressure for socket memroy management. Note
+	 * that this indicator should NOT be used in legacy cgroup mode
+	 * where socket memory is accounted/charged separately.
+	 */
+	unsigned long		socket_pressure;
+
+	/* Legacy tcp memory accounting */
+	bool			tcpmem_active;
+	int			tcpmem_pressure;
+
+#ifdef CONFIG_MEMCG_KMEM
+	int kmemcg_id;
+	struct obj_cgroup __rcu *objcg;
+	/* list of inherited objcgs, protected by objcg_lock */
+	struct list_head objcg_list;
+#endif
+
+	CACHELINE_PADDING(_pad2_);
+
+	/*
+	 * set > 0 if pages under this cgroup are moving to other cgroup.
+	 */
+	atomic_t		moving_account;
+	struct task_struct	*move_lock_task;
+
+	struct memcg_vmstats_percpu __percpu *vmstats_percpu;
+
+#ifdef CONFIG_CGROUP_WRITEBACK
+	struct list_head cgwb_list;
+	struct wb_domain cgwb_domain;
+	struct memcg_cgwb_frn cgwb_frn[MEMCG_CGWB_FRN_CNT];
+#endif
+
+	/* List of events which userspace want to receive */
+	struct list_head event_list;
+	spinlock_t event_list_lock;
+
+#ifdef CONFIG_TRANSPARENT_HUGEPAGE
+	struct deferred_split deferred_split_queue;
+#endif
+
+#ifdef CONFIG_LRU_GEN
+	/* per-memcg mm_struct list */
+	struct lru_gen_mm_list mm_list;
+#endif
+
+	struct mem_cgroup_per_node *nodeinfo[];
+};
+
+struct __orig_mem_cgroup {
+	struct cgroup_subsys_state css;
+
+	/* Private memcg ID. Used to ID objects that outlive the cgroup */
+	struct mem_cgroup_id id;
+
+	/* Accounted resources */
+	struct page_counter memory;		/* Both v1 & v2 */
+
+	union {
+		struct page_counter swap;	/* v2 only */
+		struct page_counter memsw;	/* v1 only */
+	};
+
+	/* Legacy consumer-oriented counters */
+	struct page_counter kmem;		/* v1 only */
+	struct page_counter tcpmem;		/* v1 only */
 
 	/* Range enforcement for interrupt charges */
 	struct work_struct high_work;
@@ -280,6 +414,7 @@ struct mem_cgroup {
 	spinlock_t		move_lock;
 	unsigned long		move_lock_flags;
 
+
 	CACHELINE_PADDING(_pad1_);
 
 	/* memory.stat */
@@ -339,6 +474,14 @@ struct mem_cgroup {
 	struct mem_cgroup_per_node *nodeinfo[];
 };
 
+/* XXX: these macros are proxies for kernels w/out KABI verification, replace
+ *      them with more direct and explicit KABI flag */
+#if !defined(CONFIG_ARM) && !defined(CONFIG_DEBUG_SPINLOCK) && !defined(CONFIG_PREEMPT_RT)
+static_assert(offsetof(struct mem_cgroup, move_lock_flags) ==
+	      offsetof(struct __orig_mem_cgroup, move_lock_flags));
+static_assert(offsetof(struct mem_cgroup, _pad1_) ==
+	      offsetof(struct __orig_mem_cgroup, _pad1_));
+#endif
 /*
  * size of first charge trial.
  * TODO: maybe necessary to use big numbers in big irons or dynamic based of the
