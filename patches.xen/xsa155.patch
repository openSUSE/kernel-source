From: jbeulich@suse.com
Subject: guarantee one time reads of shared ring contents
Patch-mainline: Never, SUSE-Xen specific
References: bsc#957988 CVE-2015-8550 XSA-155

--- a/drivers/xen/blkback/blkback.c
+++ b/drivers/xen/blkback/blkback.c
@@ -511,6 +511,9 @@ static int _do_block_io_op(blkif_t *blki
 
 		++rc;
 
+		/* Apply all sanity checks to /private copy/ of request. */
+		barrier();
+
 		switch (req.operation) {
 		case BLKIF_OP_READ:
 		case BLKIF_OP_WRITE:
@@ -524,15 +527,10 @@ static int _do_block_io_op(blkif_t *blki
 
 			/* before make_response() */
 			blk_rings->common.req_cons = rc;
-
-			/* Apply all sanity checks to /private copy/ of request. */
-			barrier();
-
 			dispatch_rw_block_io(blkif, &req, pending_req);
 			break;
 		case BLKIF_OP_DISCARD:
 			blk_rings->common.req_cons = rc;
-			barrier();
 			dispatch_discard(blkif, (void *)&req);
 			break;
 		case BLKIF_OP_INDIRECT:
@@ -542,12 +540,10 @@ static int _do_block_io_op(blkif_t *blki
 				return 1;
 			}
 			blk_rings->common.req_cons = rc;
-			barrier();
 			dispatch_indirect(blkif, (void *)&req, pending_req);
 			break;
 		case BLKIF_OP_PACKET:
 			blk_rings->common.req_cons = rc;
-			barrier();
 			blkif->st_pk_req++;
 			DPRINTK("error: block operation BLKIF_OP_PACKET not implemented\n");
 			make_response(blkif, req.id, req.operation,
@@ -558,7 +554,6 @@ static int _do_block_io_op(blkif_t *blki
 			 * avoid excessive CPU consumption by a bad guest. */
 			msleep(1);
 			blk_rings->common.req_cons = rc;
-			barrier();
 			DPRINTK("error: unknown block io operation [%d]\n",
 				req.operation);
 			make_response(blkif, req.id, req.operation,
--- a/drivers/xen/netback/netback.c
+++ b/drivers/xen/netback/netback.c
@@ -1138,9 +1138,7 @@ static void tx_add_credit(netif_t *netif
 	 * Allow a burst big enough to transmit a jumbo packet of up to 128kB.
 	 * Otherwise the interface can seize up due to insufficient credit.
 	 */
-	max_burst = RING_GET_REQUEST(&netif->tx, netif->tx.req_cons)->size;
-	max_burst = min(max_burst, 131072UL);
-	max_burst = max(max_burst, netif->credit_bytes);
+	max_burst = max(131072UL, netif->credit_bytes);
 
 	/* Take care that adding a new chunk of credit doesn't wrap to zero. */
 	max_credit = netif->remaining_credit + netif->credit_bytes;
@@ -1358,6 +1356,7 @@ static int netbk_count_requests(netif_t
 		}
 
 		*txp = *RING_GET_REQUEST(&netif->tx, cons + slots);
+		barrier();
 
 		/*
 		 * If the guest submitted a frame >= 64 KiB then first->size
@@ -1636,6 +1635,7 @@ int netbk_get_extras(netif_t *netif, str
 
 		memcpy(&extra, RING_GET_REQUEST(&netif->tx, cons),
 		       sizeof(extra));
+		barrier();
 		if (unlikely(!extra.type ||
 			     extra.type >= XEN_NETIF_EXTRA_TYPE_MAX)) {
 			netif->tx.req_cons = ++cons;
@@ -1739,6 +1739,7 @@ static void net_tx_action(unsigned long
 		i = netif->tx.req_cons;
 		rmb(); /* Ensure that we see the request before we copy it. */
 		memcpy(&txreq, RING_GET_REQUEST(&netif->tx, i), sizeof(txreq));
+		barrier();
 
 		/* Credit-based scheduling. */
 		if (txreq.size > netif->remaining_credit) {
--- a/drivers/xen/xen-pciback/pciback.h
+++ b/drivers/xen/xen-pciback/pciback.h
@@ -44,6 +44,7 @@ struct xen_pcibk_device {
 	struct xen_pci_sharedinfo *sh_info;
 	unsigned long flags;
 	struct work_struct op_work;
+	struct xen_pci_op op;
 };
 
 struct xen_pcibk_dev_data {
--- a/drivers/xen/xen-pciback/pciback_ops.c
+++ b/drivers/xen/xen-pciback/pciback_ops.c
@@ -345,9 +345,11 @@ void xen_pcibk_do_op(struct work_struct
 		container_of(data, struct xen_pcibk_device, op_work);
 	struct pci_dev *dev;
 	struct xen_pcibk_dev_data *dev_data = NULL;
-	struct xen_pci_op *op = &pdev->sh_info->op;
+	struct xen_pci_op *op = &pdev->op;
 	int test_intx = 0;
 
+	*op = pdev->sh_info->op;
+	barrier();
 	dev = xen_pcibk_get_pci_dev(pdev, op->domain, op->bus, op->devfn);
 
 	if (dev == NULL)
@@ -400,6 +402,17 @@ void xen_pcibk_do_op(struct work_struct
 			xen_pcibk_control_isr(dev, 0 /* no reset */);
 	}
 #endif
+	pdev->sh_info->op.err = op->err;
+	pdev->sh_info->op.value = op->value;
+#ifdef CONFIG_PCI_MSI
+	if (op->cmd == XEN_PCI_OP_enable_msix && op->err == 0) {
+		unsigned int i;
+
+		for (i = 0; i < op->value; i++)
+			pdev->sh_info->op.msix_entries[i].vector =
+				op->msix_entries[i].vector;
+	}
+#endif
 	/* Tell the driver domain that we're done. */
 	wmb();
 	clear_bit(_XEN_PCIF_active, (unsigned long *)&pdev->sh_info->flags);
--- a/drivers/xen/scsiback/scsiback.c
+++ b/drivers/xen/scsiback/scsiback.c
@@ -528,11 +528,13 @@ static int prepare_pending_reqs(struct v
 
 	pending_req->info       = info;
 
-	pending_req->v_chn = vir.chn = ring_req->channel;
-	pending_req->v_tgt = vir.tgt = ring_req->id;
+	vir.chn = ring_req->channel;
+	vir.tgt = ring_req->id;
 	vir.lun = ring_req->lun;
-
 	rmb();
+	pending_req->v_chn = vir.chn;
+	pending_req->v_tgt = vir.tgt;
+
 	sdev = scsiback_do_translation(info, &vir);
 	if (!sdev) {
 		pending_req->sdev = NULL;
@@ -591,11 +593,12 @@ static void latch_segments(pending_req_t
 			   const struct vscsiif_sg_list *sgl)
 {
 	unsigned int nr_segs = sgl->nr_segments;
+	const struct scsiif_request_segment *seg = sgl->seg;
 
 	barrier();
 	if (pending_req->nr_segments + nr_segs <= vscsiif_segs) {
 		memcpy(pending_req->segs + pending_req->nr_segments,
-		       sgl->seg, nr_segs * sizeof(*sgl->seg));
+		       seg, nr_segs * sizeof(*seg));
 		pending_req->nr_segments += nr_segs;
 	}
 	else
--- a/drivers/xen/tpmback/tpmback.c
+++ b/drivers/xen/tpmback/tpmback.c
@@ -245,17 +245,16 @@ int _packet_write(struct packet *pak,
 		unsigned int tocopy;
 		struct gnttab_map_grant_ref map_op;
 		struct gnttab_unmap_grant_ref unmap_op;
-		tpmif_tx_request_t *tx;
+		tpmif_tx_request_t tx = tpmif->tx->ring[i].req;
 
-		tx = &tpmif->tx->ring[i].req;
-
-		if (0 == tx->addr) {
+		rmb();
+		if (0 == tx.addr) {
 			DPRINTK("ERROR: Buffer for outgoing packet NULL?! i=%d\n", i);
 			return 0;
 		}
 
 		gnttab_set_map_op(&map_op, idx_to_kaddr(tpmif, i),
-				  GNTMAP_host_map, tx->ref, tpmif->domid);
+				  GNTMAP_host_map, tx.ref, tpmif->domid);
 
 		gnttab_check_GNTST_eagain_do_while(GNTTABOP_map_grant_ref, &map_op);
 
@@ -269,12 +268,12 @@ int _packet_write(struct packet *pak,
 		tocopy = min_t(size_t, size - offset, PAGE_SIZE);
 
 		if (copy_from_buffer((void *)(idx_to_kaddr(tpmif, i) |
-					      (tx->addr & ~PAGE_MASK)),
+					      (tx.addr & ~PAGE_MASK)),
 				     &data[offset], tocopy, isuserbuffer)) {
 			tpmif_put(tpmif);
 			return -EFAULT;
 		}
-		tx->size = tocopy;
+		tpmif->tx->ring[i].req.size = tocopy;
 
 		gnttab_set_unmap_op(&unmap_op, idx_to_kaddr(tpmif, i),
 				    GNTMAP_host_map, handle);
@@ -373,9 +372,6 @@ static int packet_read_shmem(struct pack
 	u32 to_copy;
 	grant_handle_t handle;
 
-	tpmif_tx_request_t *tx;
-
-	tx = &tpmif->tx->ring[0].req;
 	/*
 	 * Start copying data at the page with index 'index'
 	 * and within that page at offset 'offset'.
@@ -386,11 +382,11 @@ static int packet_read_shmem(struct pack
 		void *src;
 		struct gnttab_map_grant_ref map_op;
 		struct gnttab_unmap_grant_ref unmap_op;
+		tpmif_tx_request_t tx = tpmif->tx->ring[i].req;
 
-		tx = &tpmif->tx->ring[i].req;
-
+		rmb();
 		gnttab_set_map_op(&map_op, idx_to_kaddr(tpmif, i),
-				  GNTMAP_host_map, tx->ref, tpmif->domid);
+				  GNTMAP_host_map, tx.ref, tpmif->domid);
 
 		gnttab_check_GNTST_eagain_do_while(GNTTABOP_map_grant_ref, &map_op);
 
@@ -401,19 +397,19 @@ static int packet_read_shmem(struct pack
 
 		handle = map_op.handle;
 
-		if (to_copy > tx->size) {
+		if (to_copy > tx.size) {
 			/*
 			 * User requests more than what's available
 			 */
-			to_copy = min_t(u32, tx->size, to_copy);
+			to_copy = min_t(u32, tx.size, to_copy);
 		}
 
 		DPRINTK("Copying from mapped memory at %08lx\n",
 			(unsigned long)(idx_to_kaddr(tpmif, i) |
-					(tx->addr & ~PAGE_MASK)));
+					(tx.addr & ~PAGE_MASK)));
 
 		src = (void *)(idx_to_kaddr(tpmif, i) |
-			       ((tx->addr & ~PAGE_MASK) + pg_offset));
+			       ((tx.addr & ~PAGE_MASK) + pg_offset));
 		if (copy_to_buffer(&buffer[offset],
 				   src, to_copy, isuserbuffer)) {
 			return -EFAULT;
@@ -874,21 +870,23 @@ static void tpm_tx_action(unsigned long
 {
 	struct list_head *ent;
 	tpmif_t *tpmif;
-	tpmif_tx_request_t *tx;
 
 	DPRINTK("%s: Getting data from front-end(s)!\n", __func__);
 
 	while (!list_empty(&tpm_schedule_list)) {
+		tpmif_tx_request_t tx;
+
 		/* Get a tpmif from the list with work to do. */
 		ent = tpm_schedule_list.next;
 		tpmif = list_entry(ent, tpmif_t, list);
 		tpmif_get(tpmif);
 		remove_from_tpm_schedule_list(tpmif);
 
-		tx = &tpmif->tx->ring[0].req;
+		tx = tpmif->tx->ring[0].req;
+		rmb();
 
 		/* pass it up */
-		vtpm_receive(tpmif, tx->size);
+		vtpm_receive(tpmif, tx.size);
 
 		tpmif_put(tpmif);
 	}
