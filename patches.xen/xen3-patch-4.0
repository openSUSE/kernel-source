From: Linux Kernel Mailing List <linux-kernel@vger.kernel.org>
Subject: Linux: 4.0
Patch-mainline: 4.0

 This patch contains the differences between 3.19 and 4.0.

Automatically created from "patch-4.0" by xen-port-patches.py
Acked-by: jbeulich@suse.com

--- a/arch/x86/include/asm/i8259.h
+++ b/arch/x86/include/asm/i8259.h
@@ -70,8 +70,7 @@ struct legacy_pic {
 extern struct legacy_pic *legacy_pic;
 extern struct legacy_pic null_legacy_pic;
 #else
-extern const struct legacy_pic xen_legacy_pic;
-#define legacy_pic (&xen_legacy_pic)
+extern const struct legacy_pic null_legacy_pic, *legacy_pic;
 #endif
 
 static inline int nr_legacy_irqs(void)
--- a/arch/x86/include/asm/mmu.h
+++ b/arch/x86/include/asm/mmu.h
@@ -23,7 +23,9 @@ typedef struct {
 	struct mutex lock;
 	void __user *vdso;
 
+#ifndef CONFIG_XEN
 	atomic_t perf_rdpmc_allowed;	/* nonzero if rdpmc is allowed */
+#endif
 } mm_context_t;
 
 #if defined(CONFIG_SMP) && !defined(CONFIG_XEN)
--- a/arch/x86/include/mach-xen/asm/mmu_context.h
+++ b/arch/x86/include/mach-xen/asm/mmu_context.h
@@ -25,6 +25,21 @@ static inline void xen_activate_mm(struc
 		mm_pin(next);
 }
 
+#if defined(CONFIG_PERF_EVENTS) && !defined(CONFIG_XEN)
+extern struct static_key rdpmc_always_available;
+
+static inline void load_mm_cr4(struct mm_struct *mm)
+{
+	if (static_key_true(&rdpmc_always_available) ||
+	    atomic_read(&mm->context.perf_rdpmc_allowed))
+		cr4_set_bits(X86_CR4_PCE);
+	else
+		cr4_clear_bits(X86_CR4_PCE);
+}
+#else
+static inline void load_mm_cr4(struct mm_struct *mm) {}
+#endif
+
 /*
  * Used for LDT copy/destruction.
  */
@@ -111,15 +126,20 @@ static inline void switch_mm(struct mm_s
 		op++;
 #endif
 
+		/* Load per-mm CR4 state */
+		load_mm_cr4(next);
+
 		/*
 		 * Load the LDT, if the LDT is different.
 		 *
-		 * It's possible leave_mm(prev) has been called.  If so,
-		 * then prev->context.ldt could be out of sync with the
-		 * LDT descriptor or the LDT register.  This can only happen
-		 * if prev->context.ldt is non-null, since we never free
-		 * an LDT.  But LDTs can't be shared across mms, so
-		 * prev->context.ldt won't be equal to next->context.ldt.
+		 * It's possible that prev->context.ldt doesn't match
+		 * the LDT register.  This can happen if leave_mm(prev)
+		 * was called and then modify_ldt changed
+		 * prev->context.ldt but suppressed an IPI to this CPU.
+		 * In this case, prev->context.ldt != NULL, because we
+		 * never free an LDT while the mm still exists.  That
+		 * means that next->context.ldt != prev->context.ldt,
+		 * because mms never share an LDT.
 		 */
 		if (unlikely(prev->context.ldt != next->context.ldt)) {
 			/* load_LDT_nolock(&next->context) */
@@ -154,6 +174,7 @@ static inline void switch_mm(struct mm_s
 			 */
 			load_cr3(next->pgd);
 			trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
+			load_mm_cr4(next);
 			xen_new_user_pt(__pa(__user_pgd(next->pgd)));
 			load_LDT_nolock(&next->context);
 		}
--- a/arch/x86/include/mach-xen/asm/pgtable-3level.h
+++ b/arch/x86/include/mach-xen/asm/pgtable-3level.h
@@ -175,18 +175,6 @@ static inline pmd_t xen_pmdp_get_and_cle
 #endif
 #endif
 
-/*
- * Bits 0, 6 and 7 are taken in the low part of the pte,
- * put the 32 bits of offset into the high part.
- *
- * For soft-dirty tracking bit 11 is taken from
- * the low part of pte as well.
- */
-#define pte_to_pgoff(pte) ((pte).pte_high)
-#define pgoff_to_pte(off)						\
-	((pte_t) { { .pte_low = _PAGE_FILE, .pte_high = (off) } })
-#define PTE_FILE_MAX_BITS       32
-
 /* Encode and de-code a swap entry */
 #define MAX_SWAPFILES_CHECK() BUILD_BUG_ON(MAX_SWAPFILES_SHIFT > 5)
 #define __swp_type(x)			(((x).val) & 0x1f)
--- a/arch/x86/include/mach-xen/asm/pgtable.h
+++ b/arch/x86/include/mach-xen/asm/pgtable.h
@@ -107,11 +107,6 @@ static inline int pte_write(pte_t pte)
 	return pte_flags(pte) & _PAGE_RW;
 }
 
-static inline int pte_file(pte_t pte)
-{
-	return pte_flags(pte) & _PAGE_FILE;
-}
-
 static inline int pte_huge(pte_t pte)
 {
 	return pte_flags(pte) & _PAGE_PSE;
@@ -129,13 +124,7 @@ static inline int pte_exec(pte_t pte)
 
 static inline int pte_special(pte_t pte)
 {
-	/*
-	 * See CONFIG_NUMA_BALANCING pte_numa in include/asm-generic/pgtable.h.
-	 * On x86 we have _PAGE_BIT_NUMA == _PAGE_BIT_GLOBAL+1 ==
-	 * __PAGE_BIT_SOFTW1 == _PAGE_BIT_SPECIAL.
-	 */
-	return (pte_flags(pte) & _PAGE_SPECIAL) &&
-		(pte_flags(pte) & (_PAGE_PRESENT|_PAGE_PROTNONE));
+	return pte_flags(pte) & _PAGE_SPECIAL;
 }
 
 #define pte_mfn(_pte) ((_pte).pte_low & _PAGE_PRESENT ? \
@@ -300,7 +289,7 @@ static inline pmd_t pmd_mkwrite(pmd_t pm
 
 static inline pmd_t pmd_mknotpresent(pmd_t pmd)
 {
-	return pmd_clear_flags(pmd, _PAGE_PRESENT);
+	return pmd_clear_flags(pmd, _PAGE_PRESENT | _PAGE_PROTNONE);
 }
 #endif
 
@@ -327,21 +316,6 @@ static inline pmd_t pmd_mksoft_dirty(pmd
 }
 #endif
 
-static inline pte_t pte_file_clear_soft_dirty(pte_t pte)
-{
-	return pte_clear_flags(pte, _PAGE_SOFT_DIRTY);
-}
-
-static inline pte_t pte_file_mksoft_dirty(pte_t pte)
-{
-	return pte_set_flags(pte, _PAGE_SOFT_DIRTY);
-}
-
-static inline int pte_file_soft_dirty(pte_t pte)
-{
-	return pte_flags(pte) & _PAGE_SOFT_DIRTY;
-}
-
 #endif /* CONFIG_HAVE_ARCH_SOFT_DIRTY */
 
 /*
@@ -463,13 +437,6 @@ static inline int pte_same(pte_t a, pte_
 
 static inline int pte_present(pte_t a)
 {
-	return pte_flags(a) & (_PAGE_PRESENT | _PAGE_PROTNONE |
-			       _PAGE_NUMA);
-}
-
-#define pte_present_nonuma pte_present_nonuma
-static inline int pte_present_nonuma(pte_t a)
-{
 	return pte_flags(a) & (_PAGE_PRESENT | _PAGE_PROTNONE);
 }
 
@@ -479,7 +446,7 @@ static inline bool pte_accessible(struct
 	if (pte_flags(a) & _PAGE_PRESENT)
 		return true;
 
-	if ((pte_flags(a) & (_PAGE_PROTNONE | _PAGE_NUMA)) &&
+	if ((pte_flags(a) & _PAGE_PROTNONE) &&
 			mm_tlb_flush_pending(mm))
 		return true;
 
@@ -504,11 +471,28 @@ static inline int pmd_present(pmd_t pmd)
 	 * the _PAGE_PSE flag will remain set at all times while the
 	 * _PAGE_PRESENT bit is clear).
 	 */
-	return pmd_flags(pmd) & (_PAGE_PRESENT | _PAGE_PROTNONE | _PAGE_PSE |
-				 _PAGE_NUMA);
+	return pmd_flags(pmd) & (_PAGE_PRESENT | _PAGE_PROTNONE | _PAGE_PSE);
 #endif
 }
 
+#ifdef CONFIG_NUMA_BALANCING
+/*
+ * These work without NUMA balancing but the kernel does not care. See the
+ * comment in include/asm-generic/pgtable.h
+ */
+static inline int pte_protnone(pte_t pte)
+{
+	return (pte_flags(pte) & (_PAGE_PROTNONE | _PAGE_PRESENT))
+		== _PAGE_PROTNONE;
+}
+
+static inline int pmd_protnone(pmd_t pmd)
+{
+	return (pmd_flags(pmd) & (_PAGE_PROTNONE | _PAGE_PRESENT))
+		== _PAGE_PROTNONE;
+}
+#endif /* CONFIG_NUMA_BALANCING */
+
 static inline int pmd_none(pmd_t pmd)
 {
 	/* Only check low word on 32-bit platforms, since it might be
@@ -565,11 +549,6 @@ static inline pte_t *pte_offset_kernel(p
 
 static inline int pmd_bad(pmd_t pmd)
 {
-#ifdef CONFIG_NUMA_BALANCING
-	/* pmd_numa check */
-	if ((pmd_flags(pmd) & (_PAGE_NUMA|_PAGE_PRESENT)) == _PAGE_NUMA)
-		return 0;
-#endif
 #if CONFIG_XEN_COMPAT <= 0x030002
 	return (pmd_flags(pmd) & ~_PAGE_USER & ~_PAGE_PRESENT)
 	       != (_KERNPG_TABLE & ~_PAGE_PRESENT);
@@ -980,19 +959,16 @@ static inline void ptep_modify_prot_comm
 #ifdef CONFIG_HAVE_ARCH_SOFT_DIRTY
 static inline pte_t pte_swp_mksoft_dirty(pte_t pte)
 {
-	VM_BUG_ON(pte_present_nonuma(pte));
 	return pte_set_flags(pte, _PAGE_SWP_SOFT_DIRTY);
 }
 
 static inline int pte_swp_soft_dirty(pte_t pte)
 {
-	VM_BUG_ON(pte_present_nonuma(pte));
 	return pte_flags(pte) & _PAGE_SWP_SOFT_DIRTY;
 }
 
 static inline pte_t pte_swp_clear_soft_dirty(pte_t pte)
 {
-	VM_BUG_ON(pte_present_nonuma(pte));
 	return pte_clear_flags(pte, _PAGE_SWP_SOFT_DIRTY);
 }
 #endif
--- a/arch/x86/include/mach-xen/asm/pgtable_64.h
+++ b/arch/x86/include/mach-xen/asm/pgtable_64.h
@@ -133,10 +133,6 @@ static inline int pgd_large(pgd_t pgd) {
 /* PUD - Level3 access */
 
 /* PMD  - Level 2 access */
-#define pte_to_pgoff(pte) ((__pte_val(pte) & PHYSICAL_PAGE_MASK) >> PAGE_SHIFT)
-#define pgoff_to_pte(off) ((pte_t) { .pte = ((off) << PAGE_SHIFT) |	\
-					    _PAGE_FILE })
-#define PTE_FILE_MAX_BITS __PHYSICAL_MASK_SHIFT
 
 /* PTE - Level 1 access. */
 
@@ -145,13 +141,8 @@ static inline int pgd_large(pgd_t pgd) {
 #define pte_unmap(pte) ((void)(pte))/* NOP */
 
 /* Encode and de-code a swap entry */
-#define SWP_TYPE_BITS (_PAGE_BIT_FILE - _PAGE_BIT_PRESENT - 1)
-#ifdef CONFIG_NUMA_BALANCING
-/* Automatic NUMA balancing needs to be distinguishable from swap entries */
-#define SWP_OFFSET_SHIFT (_PAGE_BIT_PROTNONE + 2)
-#else
+#define SWP_TYPE_BITS 5
 #define SWP_OFFSET_SHIFT (_PAGE_BIT_PROTNONE + 1)
-#endif
 
 #define MAX_SWAPFILES_CHECK() BUILD_BUG_ON(MAX_SWAPFILES_SHIFT > SWP_TYPE_BITS)
 
--- a/arch/x86/include/mach-xen/asm/pgtable_types.h
+++ b/arch/x86/include/mach-xen/asm/pgtable_types.h
@@ -4,7 +4,7 @@
 #include <linux/const.h>
 #include <asm/page_types.h>
 
-#define FIRST_USER_ADDRESS	0
+#define FIRST_USER_ADDRESS	0UL
 
 #define _PAGE_BIT_PRESENT	0	/* is present */
 #define _PAGE_BIT_RW		1	/* writeable */
@@ -28,19 +28,9 @@
 #define _PAGE_BIT_SOFT_DIRTY	_PAGE_BIT_SOFTW3 /* software dirty tracking */
 #define _PAGE_BIT_NX           63       /* No execute: only valid after cpuid check */
 
-/*
- * Swap offsets on configurations that allow automatic NUMA balancing use the
- * bits after _PAGE_BIT_GLOBAL. To uniquely distinguish NUMA hinting PTEs from
- * swap entries, we use the first bit after _PAGE_BIT_GLOBAL and shrink the
- * maximum possible swap space from 16TB to 8TB.
- */
-#define _PAGE_BIT_NUMA		(_PAGE_BIT_GLOBAL+1)
-
 /* If _PAGE_BIT_PRESENT is clear, we use these: */
 /* - if the user mapped it with PROT_NONE; pte_present gives true */
 #define _PAGE_BIT_PROTNONE	_PAGE_BIT_GLOBAL
-/* - set: nonlinear file mapping, saved PTE; unset:swap */
-#define _PAGE_BIT_FILE		_PAGE_BIT_DIRTY
 
 #define _PAGE_PRESENT	(_AT(pteval_t, 1) << _PAGE_BIT_PRESENT)
 #define _PAGE_RW	(_AT(pteval_t, 1) << _PAGE_BIT_RW)
@@ -79,21 +69,6 @@
 #endif
 
 /*
- * _PAGE_NUMA distinguishes between a numa hinting minor fault and a page
- * that is not present. The hinting fault gathers numa placement statistics
- * (see pte_numa()). The bit is always zero when the PTE is not present.
- *
- * The bit picked must be always zero when the pmd is present and not
- * present, so that we don't lose information when we set it while
- * atomically clearing the present bit.
- */
-#ifdef CONFIG_NUMA_BALANCING
-#define _PAGE_NUMA	(_AT(pteval_t, 1) << _PAGE_BIT_NUMA)
-#else
-#define _PAGE_NUMA	(_AT(pteval_t, 0))
-#endif
-
-/*
  * Tracking soft dirty bit when a page goes to a swap is tricky.
  * We need a bit which can be stored in pte _and_ not conflict
  * with swap entry format. On x86 bits 6 and 7 are *not* involved
@@ -115,7 +90,6 @@
 #define _PAGE_NX	(_AT(pteval_t, 0))
 #endif
 
-#define _PAGE_FILE	(_AT(pteval_t, 1) << _PAGE_BIT_FILE)
 #define _PAGE_PROTNONE	(_AT(pteval_t, 1) << _PAGE_BIT_PROTNONE)
 
 #ifndef __ASSEMBLY__
@@ -134,8 +108,8 @@ extern unsigned int __kernel_page_user;
 /* Set of bits not changed in pte_modify */
 #define _PAGE_CHG_MASK	(PTE_PFN_MASK | _PAGE_CACHE_MASK | _PAGE_IOMAP | \
 			 _PAGE_SPECIAL | _PAGE_ACCESSED | _PAGE_DIRTY |	\
-			 _PAGE_SOFT_DIRTY | _PAGE_NUMA)
-#define _HPAGE_CHG_MASK (_PAGE_CHG_MASK | _PAGE_PSE | _PAGE_NUMA)
+			 _PAGE_SOFT_DIRTY)
+#define _HPAGE_CHG_MASK (_PAGE_CHG_MASK | _PAGE_PSE)
 
 /*
  * The cache modes defined here are used to translate between pure SW usage
@@ -379,20 +353,6 @@ static inline pteval_t pte_flags(pte_t p
 	return __pte_val(pte) & PTE_FLAGS_MASK;
 }
 
-#ifdef CONFIG_NUMA_BALANCING
-/* Set of bits that distinguishes present, prot_none and numa ptes */
-#define _PAGE_NUMA_MASK (_PAGE_NUMA|_PAGE_PROTNONE|_PAGE_PRESENT)
-static inline pteval_t ptenuma_flags(pte_t pte)
-{
-	return pte_flags(pte) & _PAGE_NUMA_MASK;
-}
-
-static inline pmdval_t pmdnuma_flags(pmd_t pmd)
-{
-	return pmd_flags(pmd) & _PAGE_NUMA_MASK;
-}
-#endif /* CONFIG_NUMA_BALANCING */
-
 #define pgprot_val(x)	((x).pgprot)
 #define __pgprot(x)	((pgprot_t) { (x) } )
 
--- a/arch/x86/include/mach-xen/asm/processor.h
+++ b/arch/x86/include/mach-xen/asm/processor.h
@@ -578,39 +578,6 @@ native_load_sp0(struct tss_struct *tss, 
 
 #define set_iopl_mask xen_set_iopl_mask
 
-/*
- * Save the cr4 feature set we're using (ie
- * Pentium 4MB enable and PPro Global page
- * enable), so that any CPU's that boot up
- * after us can get the correct flags.
- */
-extern unsigned long mmu_cr4_features;
-#define trampoline_cr4_features ((u32 *)NULL)
-
-static inline void set_in_cr4(unsigned long mask)
-{
-	unsigned long cr4;
-
-	mmu_cr4_features |= mask;
-	if (trampoline_cr4_features)
-		*trampoline_cr4_features = mmu_cr4_features;
-	cr4 = read_cr4();
-	cr4 |= mask;
-	write_cr4(cr4);
-}
-
-static inline void clear_in_cr4(unsigned long mask)
-{
-	unsigned long cr4;
-
-	mmu_cr4_features &= ~mask;
-	if (trampoline_cr4_features)
-		*trampoline_cr4_features = mmu_cr4_features;
-	cr4 = read_cr4();
-	cr4 &= ~mask;
-	write_cr4(cr4);
-}
-
 typedef struct {
 	unsigned long		seg;
 } mm_segment_t;
--- a/arch/x86/include/mach-xen/asm/special_insns.h
+++ b/arch/x86/include/mach-xen/asm/special_insns.h
@@ -184,17 +184,17 @@ static inline void write_cr3(unsigned lo
 	xen_write_cr3(x);
 }
 
-static inline unsigned long read_cr4(void)
+static inline unsigned long __read_cr4(void)
 {
 	return xen_read_cr4();
 }
 
-static inline unsigned long read_cr4_safe(void)
+static inline unsigned long __read_cr4_safe(void)
 {
 	return xen_read_cr4_safe();
 }
 
-static inline void write_cr4(unsigned long x)
+static inline void __write_cr4(unsigned long x)
 {
 	xen_write_cr4(x);
 }
--- a/arch/x86/include/mach-xen/asm/spinlock.h
+++ b/arch/x86/include/mach-xen/asm/spinlock.h
@@ -57,9 +57,14 @@ bool xen_spin_wait(arch_spinlock_t *, st
 		   unsigned int flags);
 void xen_spin_kick(const arch_spinlock_t *, unsigned int ticket);
 
+static inline bool __tickets_equal(__ticket_t one, __ticket_t two)
+{
+	return one == two;
+}
+
 static __always_inline int __ticket_spin_value_unlocked(arch_spinlock_t lock)
 {
-	return lock.tickets.head == lock.tickets.tail;
+	return __tickets_equal(lock.tickets.head, lock.tickets.tail);
 }
 
 /*
@@ -82,7 +87,7 @@ static __always_inline void __ticket_spi
 	unsigned int count, flags = arch_local_irq_save();
 
 	inc = xadd(&lock->tickets, inc);
-	if (likely(inc.head == inc.tail)) {
+	if (likely(__tickets_equal(inc.head, inc.tail))) {
 		arch_local_irq_restore(flags);
 		return;
 	}
@@ -91,7 +96,7 @@ static __always_inline void __ticket_spi
 
 	do {
 		count = SPIN_THRESHOLD;
-		while (inc.head != inc.tail && --count) {
+		while (!__tickets_equal(inc.head, inc.tail) && --count) {
 			cpu_relax();
 			inc.head = READ_ONCE(lock->tickets.head);
 		}
@@ -109,13 +114,13 @@ static __always_inline void __ticket_spi
 	unsigned int count;
 
 	inc = xadd(&lock->tickets, inc);
-	if (likely(inc.head == inc.tail))
+	if (likely(__tickets_equal(inc.head, inc.tail)))
 		return;
 	inc = xen_spin_adjust(lock, inc);
 
 	do {
 		count = SPIN_THRESHOLD;
-		while (inc.head != inc.tail && --count) {
+		while (!__tickets_equal(inc.head, inc.tail) && --count) {
 			cpu_relax();
 			inc.head = READ_ONCE(lock->tickets.head);
 		}
@@ -128,7 +133,7 @@ static __always_inline int __ticket_spin
 	arch_spinlock_t old, new;
 
 	old.tickets = READ_ONCE(lock->tickets);
-	if (old.tickets.head != old.tickets.tail)
+	if (!__tickets_equal(old.tickets.head, old.tickets.tail))
 		return 0;
 
 	new.head_tail = old.head_tail + (TICKET_LOCK_INC << TICKET_SHIFT);
@@ -146,7 +151,7 @@ static __always_inline void __ticket_spi
 # undef UNLOCK_LOCK_PREFIX
 #endif
 	new = READ_ONCE(lock->tickets);
-	if (new.head != new.tail)
+	if (!__tickets_equal(new.head, new.tail))
 		xen_spin_kick(lock, new.head);
 }
 
@@ -154,7 +159,7 @@ static inline int __ticket_spin_is_locke
 {
 	struct __raw_tickets tmp = READ_ONCE(lock->tickets);
 
-	return tmp.tail != tmp.head;
+	return !__tickets_equal(tmp.tail, tmp.head);
 }
 
 static inline int __ticket_spin_is_contended(arch_spinlock_t *lock)
@@ -174,8 +179,8 @@ static inline void __ticket_spin_unlock_
 		 * We need to check "unlocked" in a loop, tmp.head == head
 		 * can be false positive because of overflow.
 		 */
-		if (tmp.head == tmp.tail ||
-		    tmp.head != head)
+		if (__tickets_equal(tmp.head, tmp.tail) ||
+				!__tickets_equal(tmp.head, head))
 			break;
 
 		cpu_relax();
--- a/arch/x86/include/mach-xen/asm/tlbflush.h
+++ b/arch/x86/include/mach-xen/asm/tlbflush.h
@@ -12,6 +12,75 @@
 #define __flush_tlb_global() xen_tlb_flush()
 #define __flush_tlb_single(addr) xen_invlpg(addr)
 
+struct tlb_state {
+#if defined(CONFIG_SMP) && !defined(CONFIG_XEN)
+	struct mm_struct *active_mm;
+	int state;
+#endif
+
+	/*
+	 * Access to this CR4 shadow and to H/W CR4 is protected by
+	 * disabling interrupts when modifying either one.
+	 */
+	unsigned long cr4;
+};
+DECLARE_PER_CPU_SHARED_ALIGNED(struct tlb_state, cpu_tlbstate);
+
+/* Initialize cr4 shadow for this CPU. */
+static inline void cr4_init_shadow(void)
+{
+	this_cpu_write(cpu_tlbstate.cr4, __read_cr4());
+}
+
+/* Set in this cpu's CR4. */
+static inline void cr4_set_bits(unsigned long mask)
+{
+	unsigned long cr4;
+
+	cr4 = this_cpu_read(cpu_tlbstate.cr4);
+	if ((cr4 | mask) != cr4) {
+		cr4 |= mask;
+		this_cpu_write(cpu_tlbstate.cr4, cr4);
+		__write_cr4(cr4);
+	}
+}
+
+/* Clear in this cpu's CR4. */
+static inline void cr4_clear_bits(unsigned long mask)
+{
+	unsigned long cr4;
+
+	cr4 = this_cpu_read(cpu_tlbstate.cr4);
+	if ((cr4 & ~mask) != cr4) {
+		cr4 &= ~mask;
+		this_cpu_write(cpu_tlbstate.cr4, cr4);
+		__write_cr4(cr4);
+	}
+}
+
+/* Read the CR4 shadow. */
+static inline unsigned long cr4_read_shadow(void)
+{
+	return this_cpu_read(cpu_tlbstate.cr4);
+}
+
+/*
+ * Save some of cr4 feature set we're using (e.g.  Pentium 4MB
+ * enable and PPro Global page enable), so that any CPU's that boot
+ * up after us can get the correct flags.  This should only be used
+ * during boot on the boot cpu.
+ */
+extern unsigned long mmu_cr4_features;
+#define trampoline_cr4_features ((u32 *)NULL)
+
+static inline void cr4_set_bits_and_update_boot(unsigned long mask)
+{
+	mmu_cr4_features |= mask;
+	if (trampoline_cr4_features)
+		*trampoline_cr4_features = mmu_cr4_features;
+	cr4_set_bits(mask);
+}
+
 static inline void __flush_tlb_all(void)
 {
 	__flush_tlb_global();
@@ -130,12 +199,6 @@ extern void flush_tlb_kernel_range(unsig
 #define TLBSTATE_OK	1
 #define TLBSTATE_LAZY	2
 
-struct tlb_state {
-	struct mm_struct *active_mm;
-	int state;
-};
-DECLARE_PER_CPU_SHARED_ALIGNED(struct tlb_state, cpu_tlbstate);
-
 static inline void reset_lazy_tlbstate(void)
 {
 	this_cpu_write(cpu_tlbstate.state, 0);
--- a/arch/x86/kernel/apic/apic-xen.c
+++ b/arch/x86/kernel/apic/apic-xen.c
@@ -67,3 +67,10 @@ int __init APIC_init_uniprocessor(void)
 	return 0;
 }
 #endif
+
+#ifdef CONFIG_UP_LATE_INIT
+void __init up_late_init(void)
+{
+	APIC_init_uniprocessor();
+}
+#endif
--- a/arch/x86/kernel/apic/io_apic-xen.c
+++ b/arch/x86/kernel/apic/io_apic-xen.c
@@ -61,12 +61,16 @@
 
 /* Fake i8259 */
 static void make_8259A_irq(unsigned int irq) { io_apic_irqs &= ~(1UL<<irq); }
-#undef legacy_pic
-const struct legacy_pic xen_legacy_pic = {
+static const struct legacy_pic xen_legacy_pic = {
 	.nr_legacy_irqs = NR_IRQS_LEGACY,
 	.make_irq = make_8259A_irq
 };
-#define legacy_pic (&xen_legacy_pic)
+const struct legacy_pic *legacy_pic = &xen_legacy_pic;
+
+static void make_null_irq(unsigned int irq) {};
+const struct legacy_pic null_legacy_pic = {
+	.make_irq = make_null_irq
+};
 
 unsigned long io_apic_irqs;
 #endif /* CONFIG_XEN */
@@ -1593,7 +1597,10 @@ void __init enable_IO_APIC(void)
 	int i8259_apic, i8259_pin;
 	int apic, pin;
 
-	if (!nr_legacy_irqs())
+	if (skip_ioapic_setup)
+		nr_ioapics = 0;
+
+	if (!nr_legacy_irqs() || !nr_ioapics)
 		return;
 
 	for_each_ioapic_pin(apic, pin) {
@@ -2387,7 +2394,7 @@ static inline void __init check_timer(vo
 	}
 	local_irq_disable();
 	apic_printk(APIC_QUIET, KERN_INFO "..... failed :(.\n");
-	if (x2apic_preenabled)
+	if (apic_is_x2apic_enabled())
 		apic_printk(APIC_QUIET, KERN_INFO
 			    "Perhaps problem with the pre-enabled x2apic mode\n"
 			    "Try booting with x2apic and interrupt-remapping disabled in the bios.\n");
@@ -2468,9 +2475,9 @@ void __init setup_IO_APIC(void)
 {
 	int ioapic;
 
-	/*
-	 * calling enable_IO_APIC() is moved to setup_local_APIC for BP
-	 */
+	if (skip_ioapic_setup || !nr_ioapics)
+		return;
+
 	io_apic_irqs = nr_legacy_irqs() ? ~PIC_IRQS : ~0UL;
 
 	apic_printk(APIC_VERBOSE, "ENABLING IO-APIC IRQs\n");
--- a/arch/x86/kernel/cpu/common-xen.c
+++ b/arch/x86/kernel/cpu/common-xen.c
@@ -19,6 +19,7 @@
 #include <asm/archrandom.h>
 #include <asm/hypervisor.h>
 #include <asm/processor.h>
+#include <asm/tlbflush.h>
 #include <asm/debugreg.h>
 #include <asm/sections.h>
 #include <asm/vsyscall.h>
@@ -298,7 +299,7 @@ __setup("nosmep", setup_disable_smep);
 static __always_inline void setup_smep(struct cpuinfo_x86 *c)
 {
 	if (cpu_has(c, X86_FEATURE_SMEP))
-		set_in_cr4(X86_CR4_SMEP);
+		cr4_set_bits(X86_CR4_SMEP);
 }
 
 static __init int setup_disable_smap(char *arg)
@@ -318,9 +319,9 @@ static __always_inline void setup_smap(s
 
 	if (cpu_has(c, X86_FEATURE_SMAP)) {
 #ifdef CONFIG_X86_SMAP
-		set_in_cr4(X86_CR4_SMAP);
+		cr4_set_bits(X86_CR4_SMAP);
 #else
-		clear_in_cr4(X86_CR4_SMAP);
+		cr4_clear_bits(X86_CR4_SMAP);
 #endif
 	}
 }
@@ -533,17 +534,18 @@ u16 __read_mostly tlb_lld_2m[NR_INFO];
 u16 __read_mostly tlb_lld_4m[NR_INFO];
 u16 __read_mostly tlb_lld_1g[NR_INFO];
 
-void cpu_detect_tlb(struct cpuinfo_x86 *c)
+static void cpu_detect_tlb(struct cpuinfo_x86 *c)
 {
 	if (this_cpu->c_detect_tlb)
 		this_cpu->c_detect_tlb(c);
 
-	printk(KERN_INFO "Last level iTLB entries: 4KB %d, 2MB %d, 4MB %d\n"
-		"Last level dTLB entries: 4KB %d, 2MB %d, 4MB %d, 1GB %d\n",
+	pr_info("Last level iTLB entries: 4KB %d, 2MB %d, 4MB %d\n",
 		tlb_lli_4k[ENTRIES], tlb_lli_2m[ENTRIES],
-		tlb_lli_4m[ENTRIES], tlb_lld_4k[ENTRIES],
-		tlb_lld_2m[ENTRIES], tlb_lld_4m[ENTRIES],
-		tlb_lld_1g[ENTRIES]);
+		tlb_lli_4m[ENTRIES]);
+
+	pr_info("Last level dTLB entries: 4KB %d, 2MB %d, 4MB %d, 1GB %d\n",
+		tlb_lld_4k[ENTRIES], tlb_lld_2m[ENTRIES],
+		tlb_lld_4m[ENTRIES], tlb_lld_1g[ENTRIES]);
 }
 
 void detect_ht(struct cpuinfo_x86 *c)
@@ -1419,6 +1421,12 @@ void cpu_init(void)
 	wait_for_master_cpu(cpu);
 
 	/*
+	 * Initialize the CR4 shadow before doing anything that could
+	 * try to read it.
+	 */
+	cr4_init_shadow();
+
+	/*
 	 * Load microcode on this cpu if a valid microcode is available.
 	 * This is early microcode loading procedure.
 	 */
@@ -1442,7 +1450,7 @@ void cpu_init(void)
 
 	pr_debug("Initializing CPU#%d\n", cpu);
 
-	clear_in_cr4(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
+	cr4_clear_bits(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
 
 	/*
 	 * Initialize the per-CPU GDT with the boot GDT,
@@ -1465,7 +1473,7 @@ void cpu_init(void)
 
 	x86_configure_nx();
 #ifdef CONFIG_X86_LOCAL_APIC
-	enable_x2apic();
+	x2apic_setup();
 #endif
 
 #ifndef CONFIG_X86_NO_TSS
@@ -1532,12 +1540,18 @@ void cpu_init(void)
 
 	wait_for_master_cpu(cpu);
 
+	/*
+	 * Initialize the CR4 shadow before doing anything that could
+	 * try to read it.
+	 */
+	cr4_init_shadow();
+
 	show_ucode_info_early();
 
 	printk(KERN_INFO "Initializing CPU#%d\n", cpu);
 
 	if (cpu_feature_enabled(X86_FEATURE_VME) || cpu_has_de)
-		clear_in_cr4(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
+		cr4_clear_bits(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
 
 	switch_to_new_gdt(cpu);
 
--- a/arch/x86/kernel/e820-xen.c
+++ b/arch/x86/kernel/e820-xen.c
@@ -207,9 +207,9 @@ static void __init _e820_print_map(const
  * overwritten in the same location, starting at biosmap.
  *
  * The integer pointed to by pnr_map must be valid on entry (the
- * current number of valid entries located at biosmap) and will
- * be updated on return, with the new number of valid entries
- * (something no more than max_nr_map.)
+ * current number of valid entries located at biosmap). If the
+ * sanitizing succeeds the *pnr_map will be updated with the new
+ * number of valid entries (something no more than max_nr_map).
  *
  * The return value from sanitize_e820_map() is zero if it
  * successfully 'sanitized' the map entries passed in, and is -1
@@ -601,24 +601,16 @@ u64 __init e820_remove_range(u64 start, 
 
 void __init update_e820(void)
 {
-	u32 nr_map;
-
-	nr_map = e820.nr_map;
-	if (sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &nr_map))
+	if (sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map))
 		return;
-	e820.nr_map = nr_map;
 	printk(KERN_INFO "e820: modified physical RAM map:\n");
 	_e820_print_map(&e820, "modified");
 }
 #ifndef CONFIG_XEN_UNPRIVILEGED_GUEST
 static void __init update_e820_saved(void)
 {
-	u32 nr_map;
-
-	nr_map = e820_saved.nr_map;
-	if (sanitize_e820_map(e820_saved.map, ARRAY_SIZE(e820_saved.map), &nr_map))
-		return;
-	e820_saved.nr_map = nr_map;
+	sanitize_e820_map(e820_saved.map, ARRAY_SIZE(e820_saved.map),
+				&e820_saved.nr_map);
 }
 #endif
 
@@ -1010,11 +1002,9 @@ early_param("memmap", parse_memmap_opt);
 void __init finish_e820_parsing(void)
 {
 	if (userdef) {
-		u32 nr = e820.nr_map;
-
-		if (sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &nr) < 0)
+		if (sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map),
+					&e820.nr_map) < 0)
 			early_panic("Invalid user supplied memory map");
-		e820.nr_map = nr;
 
 		printk(KERN_INFO "e820: user-defined physical RAM map:\n");
 		_e820_print_map(&e820, "user");
--- a/arch/x86/kernel/early_printk-xen.c
+++ b/arch/x86/kernel/early_printk-xen.c
@@ -17,6 +17,7 @@
 #include <asm/pgtable.h>
 #include <linux/usb/ehci_def.h>
 #include <linux/efi.h>
+#include <asm/pci_x86.h>
 
 #ifndef CONFIG_XEN
 /* Simple VGA output */
@@ -75,7 +76,7 @@ static struct console early_vga_console 
 
 /* Serial functions loosely based on a similar package from Klaus P. Gerlicher */
 
-static int early_serial_base = 0x3f8;  /* ttyS0 */
+static unsigned long early_serial_base = 0x3f8;  /* ttyS0 */
 
 #define XMTRDY          0x20
 
@@ -93,13 +94,40 @@ static int early_serial_base = 0x3f8;  /
 #define DLL             0       /*  Divisor Latch Low         */
 #define DLH             1       /*  Divisor latch High        */
 
+static void mem32_serial_out(unsigned long addr, int offset, int value)
+{
+	uint32_t *vaddr = (uint32_t *)addr;
+	/* shift implied by pointer type */
+	writel(value, vaddr + offset);
+}
+
+static unsigned int mem32_serial_in(unsigned long addr, int offset)
+{
+	uint32_t *vaddr = (uint32_t *)addr;
+	/* shift implied by pointer type */
+	return readl(vaddr + offset);
+}
+
+static unsigned int io_serial_in(unsigned long addr, int offset)
+{
+	return inb(addr + offset);
+}
+
+static void io_serial_out(unsigned long addr, int offset, int value)
+{
+	outb(value, addr + offset);
+}
+
+static unsigned int (*serial_in)(unsigned long addr, int offset) = io_serial_in;
+static void (*serial_out)(unsigned long addr, int offset, int value) = io_serial_out;
+
 static int early_serial_putc(unsigned char ch)
 {
 	unsigned timeout = 0xffff;
 
-	while ((inb(early_serial_base + LSR) & XMTRDY) == 0 && --timeout)
+	while ((serial_in(early_serial_base, LSR) & XMTRDY) == 0 && --timeout)
 		cpu_relax();
-	outb(ch, early_serial_base + TXR);
+	serial_out(early_serial_base, TXR, ch);
 	return timeout ? 0 : -1;
 }
 
@@ -113,13 +141,28 @@ static void early_serial_write(struct co
 	}
 }
 
+static __init void early_serial_hw_init(unsigned divisor)
+{
+	unsigned char c;
+
+	serial_out(early_serial_base, LCR, 0x3);	/* 8n1 */
+	serial_out(early_serial_base, IER, 0);	/* no interrupt */
+	serial_out(early_serial_base, FCR, 0);	/* no fifo */
+	serial_out(early_serial_base, MCR, 0x3);	/* DTR + RTS */
+
+	c = serial_in(early_serial_base, LCR);
+	serial_out(early_serial_base, LCR, c | DLAB);
+	serial_out(early_serial_base, DLL, divisor & 0xff);
+	serial_out(early_serial_base, DLH, (divisor >> 8) & 0xff);
+	serial_out(early_serial_base, LCR, c & ~DLAB);
+}
+
 #define DEFAULT_BAUD 9600
 
 static __init void early_serial_init(char *s)
 {
-	unsigned char c;
 	unsigned divisor;
-	unsigned baud = DEFAULT_BAUD;
+	unsigned long baud = DEFAULT_BAUD;
 	char *e;
 
 	if (*s == ',')
@@ -144,24 +187,124 @@ static __init void early_serial_init(cha
 			s++;
 	}
 
-	outb(0x3, early_serial_base + LCR);	/* 8n1 */
-	outb(0, early_serial_base + IER);	/* no interrupt */
-	outb(0, early_serial_base + FCR);	/* no fifo */
-	outb(0x3, early_serial_base + MCR);	/* DTR + RTS */
+	if (*s) {
+		if (kstrtoul(s, 0, &baud) < 0 || baud == 0)
+			baud = DEFAULT_BAUD;
+	}
+
+	/* Convert from baud to divisor value */
+	divisor = 115200 / baud;
+
+	/* These will always be IO based ports */
+	serial_in = io_serial_in;
+	serial_out = io_serial_out;
+
+	/* Set up the HW */
+	early_serial_hw_init(divisor);
+}
+
+#ifdef CONFIG_PCI
+/*
+ * early_pci_serial_init()
+ *
+ * This function is invoked when the early_printk param starts with "pciserial"
+ * The rest of the param should be ",B:D.F,baud" where B, D & F describe the
+ * location of a PCI device that must be a UART device.
+ */
+static __init void early_pci_serial_init(char *s)
+{
+	unsigned divisor;
+	unsigned long baud = DEFAULT_BAUD;
+	u8 bus, slot, func;
+	uint32_t classcode, bar0;
+	uint16_t cmdreg;
+	char *e;
+
+
+	/*
+	 * First, part the param to get the BDF values
+	 */
+	if (*s == ',')
+		++s;
+
+	if (*s == 0)
+		return;
+
+	bus = (u8)simple_strtoul(s, &e, 16);
+	s = e;
+	if (*s != ':')
+		return;
+	++s;
+	slot = (u8)simple_strtoul(s, &e, 16);
+	s = e;
+	if (*s != '.')
+		return;
+	++s;
+	func = (u8)simple_strtoul(s, &e, 16);
+	s = e;
 
+	/* A baud might be following */
+	if (*s == ',')
+		s++;
+
+	/*
+	 * Second, find the device from the BDF
+	 */
+	cmdreg = read_pci_config(bus, slot, func, PCI_COMMAND);
+	classcode = read_pci_config(bus, slot, func, PCI_CLASS_REVISION);
+	bar0 = read_pci_config(bus, slot, func, PCI_BASE_ADDRESS_0);
+
+	/*
+	 * Verify it is a UART type device
+	 */
+	if (((classcode >> 16 != PCI_CLASS_COMMUNICATION_MODEM) &&
+	     (classcode >> 16 != PCI_CLASS_COMMUNICATION_SERIAL)) ||
+	   (((classcode >> 8) & 0xff) != 0x02)) /* 16550 I/F at BAR0 */
+		return;
+
+	/*
+	 * Determine if it is IO or memory mapped
+	 */
+	if (bar0 & 0x01) {
+		/* it is IO mapped */
+		serial_in = io_serial_in;
+		serial_out = io_serial_out;
+		early_serial_base = bar0&0xfffffffc;
+		write_pci_config(bus, slot, func, PCI_COMMAND,
+						cmdreg|PCI_COMMAND_IO);
+	} else {
+		/* It is memory mapped - assume 32-bit alignment */
+		serial_in = mem32_serial_in;
+		serial_out = mem32_serial_out;
+		/* WARNING! assuming the address is always in the first 4G */
+		early_serial_base =
+			(unsigned long)early_ioremap(bar0 & 0xfffffff0, 0x10);
+		write_pci_config(bus, slot, func, PCI_COMMAND,
+						cmdreg|PCI_COMMAND_MEMORY);
+	}
+
+	/*
+	 * Lastly, initalize the hardware
+	 */
 	if (*s) {
-		baud = simple_strtoul(s, &e, 0);
-		if (baud == 0 || s == e)
+		if (strcmp(s, "nocfg") == 0)
+			/* Sometimes, we want to leave the UART alone
+			 * and assume the BIOS has set it up correctly.
+			 * "nocfg" tells us this is the case, and we
+			 * should do no more setup.
+			 */
+			return;
+		if (kstrtoul(s, 0, &baud) < 0 || baud == 0)
 			baud = DEFAULT_BAUD;
 	}
 
+	/* Convert from baud to divisor value */
 	divisor = 115200 / baud;
-	c = inb(early_serial_base + LCR);
-	outb(c | DLAB, early_serial_base + LCR);
-	outb(divisor & 0xff, early_serial_base + DLL);
-	outb((divisor >> 8) & 0xff, early_serial_base + DLH);
-	outb(c & ~DLAB, early_serial_base + LCR);
+
+	/* Set up the HW */
+	early_serial_hw_init(divisor);
 }
+#endif
 
 #else /* CONFIG_XEN */
 
@@ -238,6 +381,13 @@ static int __init setup_early_printk(cha
 			early_console_register(&early_serial_console, keep);
 		}
 #ifndef CONFIG_XEN
+#ifdef CONFIG_PCI
+		if (!strncmp(buf, "pciserial", 9)) {
+			early_pci_serial_init(buf + 9);
+			early_console_register(&early_serial_console, keep);
+			buf += 9; /* Keep from match the above "serial" */
+		}
+#endif
 		if (!strncmp(buf, "vga", 3) &&
 		    boot_params.screen_info.orig_video_isVGA == 1) {
 			max_xpos = boot_params.screen_info.orig_video_cols;
@@ -258,11 +408,6 @@ static int __init setup_early_printk(cha
 			early_console_register(&xenboot_console, keep);
 #endif
 #ifdef CONFIG_EARLY_PRINTK_INTEL_MID
-		if (!strncmp(buf, "mrst", 4)) {
-			mrst_early_console_init();
-			early_console_register(&early_mrst_console, keep);
-		}
-
 		if (!strncmp(buf, "hsu", 3)) {
 			hsu_early_console_init(buf + 3);
 			early_console_register(&early_hsu_console, keep);
--- a/arch/x86/kernel/entry_64-xen.S
+++ b/arch/x86/kernel/entry_64-xen.S
@@ -140,33 +140,13 @@ NMI_MASK = 0x80000000
 	/* %rsp:at FRAMEEND */
 	.macro FIXUP_TOP_OF_STACK tmp offset=0
 	movq $__USER_CS,CS+\offset(%rsp)
-	movq $-1,RCX+\offset(%rsp)
+	movq RIP+\offset(%rsp),\tmp  /* get rip */
+	movq \tmp,RCX+\offset(%rsp)  /* copy it to rcx as sysret would do */
 	.endm
 
 	.macro RESTORE_TOP_OF_STACK tmp offset=0
 	.endm
 
-	.macro FAKE_STACK_FRAME child_rip
-	/* push in order ss, rsp, eflags, cs, rip */
-	xorl %eax, %eax
-	pushq_cfi $__KERNEL_DS /* ss */
-	/*CFI_REL_OFFSET	ss,0*/
-	pushq_cfi %rax /* rsp */
-	CFI_REL_OFFSET	rsp,0
-	pushq_cfi $(X86_EFLAGS_IF|X86_EFLAGS_FIXED) /* eflags - interrupts on */
-	/*CFI_REL_OFFSET	rflags,0*/
-	pushq_cfi $__KERNEL_CS /* cs */
-	/*CFI_REL_OFFSET	cs,0*/
-	pushq_cfi \child_rip /* rip */
-	CFI_REL_OFFSET	rip,0
-	pushq_cfi %rax /* orig rax */
-	.endm
-
-	.macro UNFAKE_STACK_FRAME
-	addq $8*6, %rsp
-	CFI_ADJUST_CFA_OFFSET	-(6*8)
-	.endm
-
 /*
  * initial frame state for interrupts (and exceptions without error code)
  */
@@ -272,51 +252,6 @@ NMI_MASK = 0x80000000
 	.endm
 
 #ifndef CONFIG_XEN
-/* save partial stack frame */
-	.macro SAVE_ARGS_IRQ
-	cld
-	/* start from rbp in pt_regs and jump over */
-	movq_cfi rdi, (RDI-RBP)
-	movq_cfi rsi, (RSI-RBP)
-	movq_cfi rdx, (RDX-RBP)
-	movq_cfi rcx, (RCX-RBP)
-	movq_cfi rax, (RAX-RBP)
-	movq_cfi  r8,  (R8-RBP)
-	movq_cfi  r9,  (R9-RBP)
-	movq_cfi r10, (R10-RBP)
-	movq_cfi r11, (R11-RBP)
-
-	/* Save rbp so that we can unwind from get_irq_regs() */
-	movq_cfi rbp, 0
-
-	/* Save previous stack value */
-	movq %rsp, %rsi
-
-	leaq -RBP(%rsp),%rdi	/* arg1 for handler */
-	testl $3, CS-RBP(%rsi)
-	je 1f
-	SWAPGS
-	/*
-	 * irq_count is used to check if a CPU is already on an interrupt stack
-	 * or not. While this is essentially redundant with preempt_count it is
-	 * a little cheaper to use a separate counter in the PDA (short of
-	 * moving irq_enter into assembly, which would be too much work)
-	 */
-1:	incl PER_CPU_VAR(irq_count)
-	cmovzq PER_CPU_VAR(irq_stack_ptr),%rsp
-	CFI_DEF_CFA_REGISTER	rsi
-
-	/* Store previous stack value */
-	pushq %rsi
-	CFI_ESCAPE	0x0f /* DW_CFA_def_cfa_expression */, 6, \
-			0x77 /* DW_OP_breg7 */, 0, \
-			0x06 /* DW_OP_deref */, \
-			0x08 /* DW_OP_const1u */, SS+8-RBP, \
-			0x22 /* DW_OP_plus */
-	/* We entered an interrupt context - irqs are off: */
-	TRACE_IRQS_OFF
-	.endm
-
 ENTRY(save_paranoid)
 	XCPT_FRAME 1 RDI+8
 	cld
@@ -369,11 +304,14 @@ ENTRY(ret_from_fork)
 	testl $3, CS-ARGOFFSET(%rsp)		# from kernel_thread?
 	jz   1f
 
-	testl $_TIF_IA32, TI_flags(%rcx)	# 32-bit compat task needs IRET
-	jnz  int_ret_from_sys_call
-
-	RESTORE_TOP_OF_STACK %rdi, -ARGOFFSET
-	jmp ret_from_sys_call			# go to the SYSRET fastpath
+	/*
+	 * By the time we get here, we have no idea whether our pt_regs,
+	 * ti flags, and ti status came from the 64-bit SYSCALL fast path,
+	 * the slow path, or one of the ia32entry paths.
+	 * Use int_ret_from_sys_call to return, since it can safely handle
+	 * all of the above.
+	 */
+	jmp  int_ret_from_sys_call
 
 1:
 	/* Need to set the proper %ss (not NULL) for ring 3 iretq */
@@ -442,15 +380,21 @@ system_call_fastpath:
  * Has incomplete stack frame and undefined top of stack.
  */
 ret_from_sys_call:
-	movl $_TIF_ALLWORK_MASK,%edi
-	/* edi:	flagmask */
-sysret_check:
 	LOCKDEP_SYS_EXIT
 	DISABLE_INTERRUPTS(CLBR_NONE)
 	TRACE_IRQS_OFF
-	movl TI_flags+THREAD_INFO(%rsp,RIP-ARGOFFSET),%edx
-	andl %edi,%edx
-	jnz  sysret_careful
+
+	/*
+	 * We must check ti flags with interrupts (or at least preemption)
+	 * off because we must *never* return to userspace without
+	 * processing exit work that is enqueued if we're preempted here.
+	 * In particular, returning to userspace with any of the one-shot
+	 * flags (TIF_NOTIFY_RESUME, TIF_USER_RETURN_NOTIFY, etc) set is
+	 * very bad.
+	 */
+	testl $_TIF_ALLWORK_MASK,TI_flags+THREAD_INFO(%rsp,RIP-ARGOFFSET)
+	jnz int_ret_from_sys_call_fixup	/* Go the the slow path */
+
 	CFI_REMEMBER_STATE
 	/*
 	 * sysretq will re-enable interrupts:
@@ -462,49 +406,10 @@ sysret_check:
         HYPERVISOR_IRET VGCF_IN_SYSCALL
 
 	CFI_RESTORE_STATE
-	/* Handle reschedules */
-	/* edx:	work, edi: workmask */
-sysret_careful:
-	bt $TIF_NEED_RESCHED,%edx
-	jnc sysret_signal
-	TRACE_IRQS_ON
-	ENABLE_INTERRUPTS(CLBR_NONE)
-	pushq_cfi %rdi
-	SCHEDULE_USER
-	popq_cfi %rdi
-	jmp sysret_check
 
-	/* Handle a signal */
-sysret_signal:
-	TRACE_IRQS_ON
-	ENABLE_INTERRUPTS(CLBR_NONE)
-#ifdef CONFIG_AUDITSYSCALL
-	bt $TIF_SYSCALL_AUDIT,%edx
-	jc sysret_audit
-#endif
-	/*
-	 * We have a signal, or exit tracing or single-step.
-	 * These all wind up with the iret return path anyway,
-	 * so just join that path right now.
-	 */
+int_ret_from_sys_call_fixup:
 	FIXUP_TOP_OF_STACK %r11, -ARGOFFSET
-	jmp int_check_syscall_exit_work
-
-#ifdef CONFIG_AUDITSYSCALL
-	/*
-	 * Return fast path for syscall audit.  Call __audit_syscall_exit()
-	 * directly and then jump back to the fast path with TIF_SYSCALL_AUDIT
-	 * masked off.
-	 */
-sysret_audit:
-	movq RAX-ARGOFFSET(%rsp),%rsi	/* second arg, syscall return value */
-	cmpq $-MAX_ERRNO,%rsi	/* is it < -MAX_ERRNO? */
-	setbe %al		/* 1 if so, 0 if not */
-	movzbl %al,%edi		/* zero-extend that into %edi */
-	call __audit_syscall_exit
-	movl $(_TIF_ALLWORK_MASK & ~_TIF_SYSCALL_AUDIT),%edi
-	jmp sysret_check
-#endif	/* CONFIG_AUDITSYSCALL */
+	jmp int_ret_from_sys_call_irqs_off
 
 	/* Do syscall tracing */
 tracesys:
@@ -550,6 +455,7 @@ tracesys_phase2:
 GLOBAL(int_ret_from_sys_call)
 	DISABLE_INTERRUPTS(CLBR_NONE)
 	TRACE_IRQS_OFF
+int_ret_from_sys_call_irqs_off:
 	movl $_TIF_ALLWORK_MASK,%edi
 	/* edi:	mask to check */
 GLOBAL(int_with_check)
@@ -640,19 +546,6 @@ END(\label)
 	FORK_LIKE  vfork
 	FIXED_FRAME stub_iopl, sys_iopl
 
-ENTRY(ptregscall_common)
-	DEFAULT_FRAME 1 8	/* offset 8: return address */
-	RESTORE_TOP_OF_STACK %r11, 8
-	movq_cfi_restore R15+8, r15
-	movq_cfi_restore R14+8, r14
-	movq_cfi_restore R13+8, r13
-	movq_cfi_restore R12+8, r12
-	movq_cfi_restore RBP+8, rbp
-	movq_cfi_restore RBX+8, rbx
-	ret $REST_SKIP		/* pop extended registers */
-	CFI_ENDPROC
-END(ptregscall_common)
-
 ENTRY(stub_execve)
 	CFI_STARTPROC
 	addq $8, %rsp
@@ -844,6 +737,11 @@ ENTRY(\sym)
 	.endif
 
 	.if \paranoid
+	.if \paranoid == 1
+	CFI_REMEMBER_STATE
+	testl $3, CS(%rsp)		/* If coming from userspace, switch */
+	jnz 1f				/* stacks. */
+	.endif
 	call save_paranoid
 	.else
 	call error_entry
@@ -884,6 +782,36 @@ ENTRY(\sym)
 	jmp error_exit			/* %ebx: no swapgs flag */
 	.endif
 
+	.if \paranoid == 1
+	CFI_RESTORE_STATE
+	/*
+	 * Paranoid entry from userspace.  Switch stacks and treat it
+	 * as a normal entry.  This means that paranoid handlers
+	 * run in real process context if user_mode(regs).
+	 */
+1:
+	call error_entry
+
+	DEFAULT_FRAME 0
+
+	movq %rsp,%rdi			/* pt_regs pointer */
+	call sync_regs
+	movq %rax,%rsp			/* switch stack */
+
+	movq %rsp,%rdi			/* pt_regs pointer */
+
+	.if \has_error_code
+	movq ORIG_RAX(%rsp),%rsi	/* get error code */
+	movq $-1,ORIG_RAX(%rsp)		/* no syscall to restart */
+	.else
+	xorl %esi,%esi			/* no error code */
+	.endif
+
+	call \do_sym
+
+	jmp error_exit			/* %ebx: no swapgs flag */
+	.endif
+
 	CFI_ENDPROC
 END(\sym)
 .endm
@@ -1056,16 +984,14 @@ idtentry machine_check has_error_code=0 
 
 #ifndef CONFIG_XEN
 	/*
-	 * "Paranoid" exit path from exception stack.
-	 * Paranoid because this is used by NMIs and cannot take
-	 * any kernel state for granted.
-	 * We don't do kernel preemption checks here, because only
-	 * NMI should be common and it does not enable IRQs and
-	 * cannot get reschedule ticks.
+	 * "Paranoid" exit path from exception stack.  This is invoked
+	 * only on return from non-NMI IST interrupts that came
+	 * from kernel space.
 	 *
-	 * "trace" is 0 for the NMI handler only, because irq-tracing
-	 * is fundamentally NMI-unsafe. (we cannot change the soft and
-	 * hard flags at once, atomically)
+	 * We may be returning to very strange contexts (e.g. very early
+	 * in syscall entry), so checking for preemption here would
+	 * be complicated.  Fortunately, we there's no good reason
+	 * to try to handle preemption here.
 	 */
 
 	/* ebx:	no swapgs flag */
@@ -1075,43 +1001,14 @@ ENTRY(paranoid_exit)
 	TRACE_IRQS_OFF_DEBUG
 	testl %ebx,%ebx				/* swapgs needed? */
 	jnz paranoid_restore
-	testl $3,CS(%rsp)
-	jnz   paranoid_userspace
-paranoid_swapgs:
 	TRACE_IRQS_IRETQ 0
 	SWAPGS_UNSAFE_STACK
 	RESTORE_ALL 8
-	jmp irq_return
+	INTERRUPT_RETURN
 paranoid_restore:
 	TRACE_IRQS_IRETQ_DEBUG 0
 	RESTORE_ALL 8
-	jmp irq_return
-paranoid_userspace:
-	GET_THREAD_INFO(%rcx)
-	movl TI_flags(%rcx),%ebx
-	andl $_TIF_WORK_MASK,%ebx
-	jz paranoid_swapgs
-	movq %rsp,%rdi			/* &pt_regs */
-	call sync_regs
-	movq %rax,%rsp			/* switch stack for scheduling */
-	testl $_TIF_NEED_RESCHED,%ebx
-	jnz paranoid_schedule
-	movl %ebx,%edx			/* arg3: thread flags */
-	TRACE_IRQS_ON
-	ENABLE_INTERRUPTS(CLBR_NONE)
-	xorl %esi,%esi 			/* arg2: oldset */
-	movq %rsp,%rdi 			/* arg1: &pt_regs */
-	call do_notify_resume
-	DISABLE_INTERRUPTS(CLBR_NONE)
-	TRACE_IRQS_OFF
-	jmp paranoid_userspace
-paranoid_schedule:
-	TRACE_IRQS_ON
-	ENABLE_INTERRUPTS(CLBR_ANY)
-	SCHEDULE_USER
-	DISABLE_INTERRUPTS(CLBR_ANY)
-	TRACE_IRQS_OFF
-	jmp paranoid_userspace
+	INTERRUPT_RETURN
 	CFI_ENDPROC
 END(paranoid_exit)
 #endif
--- a/arch/x86/kernel/head32-xen.c
+++ b/arch/x86/kernel/head32-xen.c
@@ -48,6 +48,7 @@ asmlinkage __visible void __init i386_st
 	set_cpu_cap(&new_cpu_data, X86_FEATURE_FPU);
 #endif
 
+	cr4_init_shadow();
 #ifndef CONFIG_XEN
 	sanitize_boot_params(&boot_params);
 
--- a/arch/x86/kernel/head64-xen.c
+++ b/arch/x86/kernel/head64-xen.c
@@ -48,7 +48,7 @@ static void __init reset_early_page_tabl
 
 	next_early_pgt = 0;
 
-	write_cr3(__pa(early_level4_pgt));
+	write_cr3(__pa_nodebug(early_level4_pgt));
 }
 
 /* Create a new PMD entry */
@@ -61,7 +61,7 @@ int __init early_make_pgtable(unsigned l
 	pmdval_t pmd, *pmd_p;
 
 	/* Invalid address or early pgt is done ?  */
-	if (physaddr >= MAXMEM || read_cr3() != __pa(early_level4_pgt))
+	if (physaddr >= MAXMEM || read_cr3() != __pa_nodebug(early_level4_pgt))
 		return -1;
 
 again:
@@ -169,6 +169,8 @@ asmlinkage __visible void __init x86_64_
 				(__START_KERNEL & PGDIR_MASK)));
 	BUILD_BUG_ON(__fix_to_virt(__end_of_fixed_addresses) <= MODULES_END);
 
+	cr4_init_shadow();
+
 	xen_start_info = (struct start_info *)real_mode_data;
 	xen_start_kernel();
 
@@ -176,6 +178,8 @@ asmlinkage __visible void __init x86_64_
 	/* Kill off the identity-map trampoline */
 	reset_early_page_tables();
 
+	kasan_map_early_shadow(early_level4_pgt);
+
 	/* clear bss before set_intr_gate with early_idt_handler */
 	clear_bss();
 
@@ -198,6 +202,8 @@ asmlinkage __visible void __init x86_64_
 	clear_page(init_level4_pgt);
 	/* set init_level4_pgt kernel high mapping*/
 	init_level4_pgt[511] = early_level4_pgt[511];
+
+	kasan_map_early_shadow(init_level4_pgt);
 #else
 	xen_switch_pt();
 #endif
--- a/arch/x86/kernel/irq-xen.c
+++ b/arch/x86/kernel/irq-xen.c
@@ -324,6 +324,9 @@ int check_irq_vectors_for_cpu_disable(vo
 		irq = __this_cpu_read(vector_irq[vector]);
 		if (irq >= 0) {
 			desc = irq_to_desc(irq);
+			if (!desc)
+				continue;
+
 			data = irq_desc_get_irq_data(desc);
 			cpumask_copy(&affinity_new, data->affinity);
 			cpumask_clear_cpu(this_cpu, &affinity_new);
--- a/arch/x86/kernel/process-xen.c
+++ b/arch/x86/kernel/process-xen.c
@@ -28,6 +28,7 @@
 #include <asm/fpu-internal.h>
 #include <asm/debugreg.h>
 #include <asm/nmi.h>
+#include <asm/tlbflush.h>
 #include <xen/evtchn.h>
 
 #ifndef CONFIG_X86_NO_TSS
@@ -145,7 +146,7 @@ void flush_thread(void)
 
 static void hard_disable_TSC(void)
 {
-	write_cr4(read_cr4() | X86_CR4_TSD);
+	cr4_set_bits(X86_CR4_TSD);
 }
 
 void disable_TSC(void)
@@ -162,7 +163,7 @@ void disable_TSC(void)
 
 static void hard_enable_TSC(void)
 {
-	write_cr4(read_cr4() & ~X86_CR4_TSD);
+	cr4_clear_bits(X86_CR4_TSD);
 }
 
 static void enable_TSC(void)
--- a/arch/x86/kernel/process_32-xen.c
+++ b/arch/x86/kernel/process_32-xen.c
@@ -104,7 +104,7 @@ void __show_regs(struct pt_regs *regs, i
 	cr0 = read_cr0();
 	cr2 = read_cr2();
 	cr3 = read_cr3();
-	cr4 = read_cr4_safe();
+	cr4 = __read_cr4_safe();
 	printk(KERN_DEFAULT "CR0: %08lx CR2: %08lx CR3: %08lx CR4: %08lx\n",
 			cr0, cr2, cr3, cr4);
 
--- a/arch/x86/kernel/process_64-xen.c
+++ b/arch/x86/kernel/process_64-xen.c
@@ -96,7 +96,7 @@ void __show_regs(struct pt_regs *regs, i
 	cr0 = read_cr0();
 	cr2 = read_cr2();
 	cr3 = read_cr3();
-	cr4 = read_cr4();
+	cr4 = __read_cr4();
 
 	printk(KERN_DEFAULT "FS:  %016lx(%04x) GS:%016lx(%04x) knlGS:%016lx\n",
 	       fs, fsindex, gs, gsindex, shadowgs);
--- a/arch/x86/kernel/setup-xen.c
+++ b/arch/x86/kernel/setup-xen.c
@@ -88,6 +88,7 @@
 #include <asm/cacheflush.h>
 #include <asm/processor.h>
 #include <asm/bugs.h>
+#include <asm/kasan.h>
 
 #include <asm/vsyscall.h>
 #include <asm/cpu.h>
@@ -496,15 +497,13 @@ static void __init parse_setup_data(void
 
 	pa_data = boot_params.hdr.setup_data;
 	while (pa_data) {
-		u32 data_len, map_len, data_type;
+		u32 data_len, data_type;
 
-		map_len = max(PAGE_SIZE - (pa_data & ~PAGE_MASK),
-			      (u64)sizeof(struct setup_data));
-		data = early_memremap(pa_data, map_len);
+		data = early_memremap(pa_data, sizeof(*data));
 		data_len = data->len + sizeof(struct setup_data);
 		data_type = data->type;
 		pa_next = data->next;
-		early_iounmap(data, map_len);
+		early_iounmap(data, sizeof(*data));
 
 		switch (data_type) {
 		case SETUP_E820_EXT:
@@ -1332,9 +1331,11 @@ void __init setup_arch(char **cmdline_p)
 
 	x86_init.paging.pagetable_init();
 
+	kasan_init();
+
 	if (boot_cpu_data.cpuid_level >= 0) {
 		/* A CPU has %cr4 if and only if it has CPUID */
-		mmu_cr4_features = read_cr4();
+		mmu_cr4_features = __read_cr4();
 		if (trampoline_cr4_features)
 			*trampoline_cr4_features = mmu_cr4_features;
 	}
--- a/arch/x86/kernel/traps-xen.c
+++ b/arch/x86/kernel/traps-xen.c
@@ -114,6 +114,88 @@ static inline void preempt_conditional_c
 	preempt_count_dec();
 }
 
+enum ctx_state ist_enter(struct pt_regs *regs)
+{
+	enum ctx_state prev_state;
+
+	if (user_mode_vm(regs)) {
+		/* Other than that, we're just an exception. */
+		prev_state = exception_enter();
+	} else {
+		/*
+		 * We might have interrupted pretty much anything.  In
+		 * fact, if we're a machine check, we can even interrupt
+		 * NMI processing.  We don't want in_nmi() to return true,
+		 * but we need to notify RCU.
+		 */
+		rcu_nmi_enter();
+		prev_state = IN_KERNEL;  /* the value is irrelevant. */
+	}
+
+	/*
+	 * We are atomic because we're on the IST stack (or we're on x86_32,
+	 * in which case we still shouldn't schedule).
+	 *
+	 * This must be after exception_enter(), because exception_enter()
+	 * won't do anything if in_interrupt() returns true.
+	 */
+	preempt_count_add(HARDIRQ_OFFSET);
+
+	/* This code is a bit fragile.  Test it. */
+	rcu_lockdep_assert(rcu_is_watching(), "ist_enter didn't work");
+
+	return prev_state;
+}
+
+void ist_exit(struct pt_regs *regs, enum ctx_state prev_state)
+{
+	/* Must be before exception_exit. */
+	preempt_count_sub(HARDIRQ_OFFSET);
+
+	if (user_mode_vm(regs))
+		return exception_exit(prev_state);
+	else
+		rcu_nmi_exit();
+}
+
+/**
+ * ist_begin_non_atomic() - begin a non-atomic section in an IST exception
+ * @regs:	regs passed to the IST exception handler
+ *
+ * IST exception handlers normally cannot schedule.  As a special
+ * exception, if the exception interrupted userspace code (i.e.
+ * user_mode_vm(regs) would return true) and the exception was not
+ * a double fault, it can be safe to schedule.  ist_begin_non_atomic()
+ * begins a non-atomic section within an ist_enter()/ist_exit() region.
+ * Callers are responsible for enabling interrupts themselves inside
+ * the non-atomic section, and callers must call is_end_non_atomic()
+ * before ist_exit().
+ */
+void ist_begin_non_atomic(struct pt_regs *regs)
+{
+	BUG_ON(!user_mode_vm(regs));
+
+	/*
+	 * Sanity check: we need to be on the normal thread stack.  This
+	 * will catch asm bugs and any attempt to use ist_preempt_enable
+	 * from double_fault.
+	 */
+	BUG_ON(((current_stack_pointer() ^ this_cpu_read_stable(kernel_stack))
+		& ~(THREAD_SIZE - 1)) != 0);
+
+	preempt_count_sub(HARDIRQ_OFFSET);
+}
+
+/**
+ * ist_end_non_atomic() - begin a non-atomic section in an IST exception
+ *
+ * Ends a non-atomic section started with ist_begin_non_atomic().
+ */
+void ist_end_non_atomic(void)
+{
+	preempt_count_add(HARDIRQ_OFFSET);
+}
+
 static nokprobe_inline int
 do_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,
 		  struct pt_regs *regs,	long error_code)
@@ -257,6 +339,8 @@ dotraplinkage void do_double_fault(struc
 	 * end up promoting it to a doublefault.  In that case, modify
 	 * the stack to make it look like we just entered the #GP
 	 * handler from user space, similar to bad_iret.
+	 *
+	 * No need for ist_enter here because we don't use RCU.
 	 */
 	if (((long)regs->sp >> PGDIR_SHIFT) == ESPFIX_PGD_ENTRY &&
 		regs->cs == __KERNEL_CS &&
@@ -269,12 +353,12 @@ dotraplinkage void do_double_fault(struc
 		normal_regs->orig_ax = 0;  /* Missing (lost) #GP error code */
 		regs->ip = (unsigned long)general_protection;
 		regs->sp = (unsigned long)&normal_regs->orig_ax;
+
 		return;
 	}
 #endif
 
-	exception_enter();
-	/* Return not checked because double check cannot be ignored */
+	ist_enter(regs);  /* Discard prev_state because we won't return. */
 	notify_die(DIE_TRAP, str, regs, error_code, X86_TRAP_DF, SIGSEGV);
 
 	tsk->thread.error_code = error_code;
@@ -306,7 +390,7 @@ dotraplinkage void do_bounds(struct pt_r
 		goto exit;
 	conditional_sti(regs);
 
-	if (!user_mode(regs))
+	if (!user_mode_vm(regs))
 		die("bounds", regs, error_code);
 
 	if (!cpu_feature_enabled(X86_FEATURE_MPX)) {
@@ -440,7 +524,7 @@ dotraplinkage void notrace do_int3(struc
 	if (poke_int3_handler(regs))
 		return;
 
-	prev_state = exception_enter();
+	prev_state = ist_enter(regs);
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
 	if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
 				SIGTRAP) == NOTIFY_STOP)
@@ -466,33 +550,20 @@ dotraplinkage void notrace do_int3(struc
 	preempt_conditional_cli(regs);
 	debug_stack_usage_dec();
 exit:
-	exception_exit(prev_state);
+	ist_exit(regs, prev_state);
 }
 NOKPROBE_SYMBOL(do_int3);
 
 #if defined(CONFIG_X86_64) && !defined(CONFIG_XEN)
 /*
- * Help handler running on IST stack to switch back to user stack
- * for scheduling or signal handling. The actual stack switch is done in
- * entry.S
+ * Help handler running on IST stack to switch off the IST stack if the
+ * interrupted code was in user mode. The actual stack switch is done in
+ * entry_64.S
  */
 asmlinkage __visible notrace struct pt_regs *sync_regs(struct pt_regs *eregs)
 {
-	struct pt_regs *regs = eregs;
-	/* Did already sync */
-	if (eregs == (struct pt_regs *)eregs->sp)
-		;
-	/* Exception from user space */
-	else if (user_mode(eregs))
-		regs = task_pt_regs(current);
-	/*
-	 * Exception from kernel and interrupts are enabled. Move to
-	 * kernel process stack.
-	 */
-	else if (eregs->flags & X86_EFLAGS_IF)
-		regs = (struct pt_regs *)(eregs->sp -= sizeof(struct pt_regs));
-	if (eregs != regs)
-		*regs = *eregs;
+	struct pt_regs *regs = task_pt_regs(current);
+	*regs = *eregs;
 	return regs;
 }
 NOKPROBE_SYMBOL(sync_regs);
@@ -560,7 +631,7 @@ dotraplinkage void do_debug(struct pt_re
 	unsigned long dr6;
 	int si_code;
 
-	prev_state = exception_enter();
+	prev_state = ist_enter(regs);
 
 	get_debugreg(dr6, 6);
 
@@ -572,7 +643,7 @@ dotraplinkage void do_debug(struct pt_re
 	 * then it's very likely the result of an icebp/int01 trap.
 	 * User wants a sigtrap for that.
 	 */
-	if (!dr6 && user_mode(regs))
+	if (!dr6 && user_mode_vm(regs))
 		user_icebp = 1;
 
 	/* Catch kmemcheck conditions first of all! */
@@ -635,7 +706,7 @@ dotraplinkage void do_debug(struct pt_re
 	debug_stack_usage_dec();
 
 exit:
-	exception_exit(prev_state);
+	ist_exit(regs, prev_state);
 }
 NOKPROBE_SYMBOL(do_debug);
 
@@ -798,18 +869,16 @@ static void _math_state_restore(void)
 		clts();
 	}
 
+	/* Avoid __kernel_fpu_begin() right after __thread_fpu_begin() */
+	kernel_fpu_disable();
 	xen_thread_fpu_begin(tsk, NULL);
-
-	/*
-	 * Paranoid restore. send a SIGSEGV if we fail to restore the state.
-	 */
 	if (unlikely(restore_fpu_checking(tsk))) {
 		drop_init_fpu(tsk);
 		force_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);
-		return;
+	} else {
+		tsk->thread.fpu_counter++;
 	}
-
-	tsk->thread.fpu_counter++;
+	kernel_fpu_enable();
 }
 
 void math_state_restore(void)
--- a/arch/x86/mm/fault-xen.c
+++ b/arch/x86/mm/fault-xen.c
@@ -609,7 +609,7 @@ show_fault_oops(struct pt_regs *regs, un
 			printk(nx_warning, from_kuid(&init_user_ns, current_uid()));
 		if (pte && pte_present(*pte) && pte_exec(*pte) &&
 				(pgd_flags(*pgd) & _PAGE_USER) &&
-				(read_cr4() & X86_CR4_SMEP))
+				(__read_cr4() & X86_CR4_SMEP))
 			printk(smep_warning, from_kuid(&init_user_ns, current_uid()));
 	}
 
--- a/arch/x86/mm/init-xen.c
+++ b/arch/x86/mm/init-xen.c
@@ -192,11 +192,11 @@ static void __init probe_page_size_mask(
 
 	/* Enable PSE if available */
 	if (cpu_has_pse)
-		set_in_cr4(X86_CR4_PSE);
+		cr4_set_bits_and_update_boot(X86_CR4_PSE);
 
 	/* Enable PGE if available */
 	if (cpu_has_pge) {
-		set_in_cr4(X86_CR4_PGE);
+		cr4_set_bits_and_update_boot(X86_CR4_PGE);
 		__supported_pte_mask |= _PAGE_GLOBAL;
 	} else
 		__supported_pte_mask &= ~_PAGE_GLOBAL;
@@ -258,6 +258,31 @@ static void __init_refok adjust_range_pa
 	}
 }
 
+static const char *page_size_string(struct map_range *mr)
+{
+	static const char str_1g[] = "1G";
+	static const char str_2m[] = "2M";
+	static const char str_4m[] = "4M";
+	static const char str_4k[] = "4k";
+
+	if (mr->page_size_mask & (1<<PG_LEVEL_1G))
+		return str_1g;
+	/*
+	 * 32-bit without PAE has a 4M large page size.
+	 * PG_LEVEL_2M is misnamed, but we can at least
+	 * print out the right size in the string.
+	 */
+	if (IS_ENABLED(CONFIG_X86_32) &&
+	    !IS_ENABLED(CONFIG_X86_PAE) &&
+	    mr->page_size_mask & (1<<PG_LEVEL_2M))
+		return str_4m;
+
+	if (mr->page_size_mask & (1<<PG_LEVEL_2M))
+		return str_2m;
+
+	return str_4k;
+}
+
 static int __meminit split_mem_range(struct map_range *mr, int nr_range,
 				     unsigned long start,
 				     unsigned long end)
@@ -353,8 +378,7 @@ static int __meminit split_mem_range(str
 	for (i = 0; i < nr_range; i++)
 		printk(KERN_DEBUG " [mem %#010lx-%#010lx] page %s\n",
 				mr[i].start, mr[i].end - 1,
-			(mr[i].page_size_mask & (1<<PG_LEVEL_1G))?"1G":(
-			 (mr[i].page_size_mask & (1<<PG_LEVEL_2M))?"2M":"4k"));
+				page_size_string(&mr[i]));
 
 	return nr_range;
 }
@@ -628,7 +652,7 @@ void __init init_mem_mapping(void)
  *
  *
  * On x86, access has to be given to the first megabyte of ram because that area
- * contains bios code and data regions used by X and dosemu and similar apps.
+ * contains BIOS code and data regions used by X and dosemu and similar apps.
  * Access has to be given to non-kernel-ram areas as well, these contain the PCI
  * mmio resources as well as potential bios/acpi data regions.
  */
@@ -753,6 +777,15 @@ void __init zone_sizes_init(void)
 	free_area_init_nodes(max_zone_pfns);
 }
 
+DEFINE_PER_CPU_SHARED_ALIGNED(struct tlb_state, cpu_tlbstate) = {
+#if defined(CONFIG_SMP) && !defined(CONFIG_XEN)
+	.active_mm = &init_mm,
+	.state = 0,
+#endif
+	.cr4 = ~0UL,	/* fail hard if we screw up cr4 shadow initialization */
+};
+EXPORT_SYMBOL_GPL(cpu_tlbstate);
+
 void update_cache_mode_entry(unsigned entry, enum page_cache_mode cache)
 {
 	/* entry 0 MUST be WB (hardwired to speed up translations) */
--- a/arch/x86/mm/pgtable-xen.c
+++ b/arch/x86/mm/pgtable-xen.c
@@ -555,7 +555,7 @@ void pud_populate(struct mm_struct *mm, 
 
 #endif	/* CONFIG_X86_PAE */
 
-static void free_pmds(pmd_t *pmds[], struct mm_struct *mm, bool contig)
+static void free_pmds(struct mm_struct *mm, pmd_t *pmds[], bool contig)
 {
 	int i;
 
@@ -565,11 +565,13 @@ static void free_pmds(pmd_t *pmds[], str
 #endif
 
 	for(i = 0; i < PREALLOCATED_PMDS; i++)
-		if (pmds[i])
+		if (pmds[i]) {
 			pmd_free(mm, pmds[i]);
+			mm_dec_nr_pmds(mm);
+		}
 }
 
-static int preallocate_pmds(pmd_t *pmds[], struct mm_struct *mm)
+static int preallocate_pmds(struct mm_struct *mm, pmd_t *pmds[])
 {
 	int i;
 	bool failed = false;
@@ -578,11 +580,13 @@ static int preallocate_pmds(pmd_t *pmds[
 		pmd_t *pmd = pmd_alloc_one(mm, i << PUD_SHIFT);
 		if (pmd == NULL)
 			failed = true;
+		else
+			mm_inc_nr_pmds(mm);
 		pmds[i] = pmd;
 	}
 
 	if (failed) {
-		free_pmds(pmds, mm, false);
+		free_pmds(mm, pmds, false);
 		return -ENOMEM;
 	}
 
@@ -609,6 +613,7 @@ static void pgd_mop_up_pmds(struct mm_st
 
 			paravirt_release_pmd(pgd_val(pgd) >> PAGE_SHIFT);
 			pmd_free(mm, pmd);
+			mm_dec_nr_pmds(mm);
 		}
 	}
 
@@ -663,7 +668,7 @@ pgd_t *pgd_alloc(struct mm_struct *mm)
 
 	mm->pgd = pgd;
 
-	if (preallocate_pmds(pmds, mm) != 0)
+	if (preallocate_pmds(mm, pmds) != 0)
 		goto out_free_pgd;
 
 	if (paravirt_pgd_alloc(mm) != 0)
@@ -693,7 +698,7 @@ pgd_t *pgd_alloc(struct mm_struct *mm)
 	return pgd;
 
 out_free_pmds:
-	free_pmds(pmds, mm, !xen_feature(XENFEAT_pae_pgdir_above_4gb));
+	free_pmds(mm, pmds, !xen_feature(XENFEAT_pae_pgdir_above_4gb));
 out_free_pgd:
 	free_pages((unsigned long)pgd, PGD_ORDER);
 out:
--- a/drivers/acpi/Kconfig
+++ b/drivers/acpi/Kconfig
@@ -327,7 +327,7 @@ config ACPI_HOTPLUG_MEMORY
 config ACPI_HOTPLUG_IOAPIC
 	bool
 	depends on PCI
-	depends on X86_IO_APIC
+	depends on X86_IO_APIC && !XEN
 	default y
 
 config ACPI_SBS
--- a/drivers/char/tpm/tpm_vtpm.c
+++ b/drivers/char/tpm/tpm_vtpm.c
@@ -477,9 +477,9 @@ struct tpm_chip *init_vtpm(struct device
 	vtpm_state_init(vtpms);
 	vtpms->tpm_private = tp;
 
-	chip = tpm_register_hardware(dev, &tpm_vtpm);
-	if (!chip) {
-		rc = -ENODEV;
+	chip = tpmm_chip_alloc(dev, &tpm_vtpm);
+	if (IS_ERR(chip)) {
+		rc = PTR_ERR(chip);
 		goto err_free_mem;
 	}
 
@@ -499,6 +499,6 @@ void cleanup_vtpm(struct device *dev)
 {
 	struct tpm_chip *chip = dev_get_drvdata(dev);
 	struct vtpm_state *vtpms = (struct vtpm_state*)chip_get_private(chip);
-	tpm_remove_hardware(dev);
+	tpm_chip_unregister(chip);
 	kfree(vtpms);
 }
--- a/drivers/char/tpm/tpm_xen.c
+++ b/drivers/char/tpm/tpm_xen.c
@@ -103,7 +103,7 @@ static void destroy_tpmring(struct tpm_p
 void __exit tpmif_exit(void);
 
 #define DPRINTK(fmt, args...) \
-    pr_debug("xen_tpm_fr (%s:%d) " fmt, __FUNCTION__, __LINE__, ##args)
+    pr_debug("xen_tpm_fr (%s:%d) " fmt, __func__, __LINE__, ##args)
 #define IPRINTK(fmt, args...) \
     pr_info("xen_tpm_fr: " fmt, ##args)
 #define WPRINTK(fmt, args...) \
@@ -629,7 +629,7 @@ static int tpm_xmit(struct tpm_private *
 			spin_unlock_irq(&tp->tx_lock);
 			DPRINTK("Grant table claim reference failed in "
 				"func:%s line:%d file:%s\n",
-				__FUNCTION__, __LINE__, __FILE__);
+				__func__, __LINE__, __FILE__);
 			return -ENOSPC;
 		}
 		gnttab_grant_foreign_access_ref(tx->ref,
--- a/drivers/xen/Makefile
+++ b/drivers/xen/Makefile
@@ -8,7 +8,7 @@ ifeq ($(filter y, $(CONFIG_ARM) $(CONFIG
 obj-$(CONFIG_HOTPLUG_CPU)	+= $(xen-hotplug-y)
 endif
 obj-$(CONFIG_X86)		+= fallback.o
-obj-$(CONFIG_PARAVIRT_XEN)	+= grant-table.o features.o balloon.o manage.o
+obj-$(CONFIG_PARAVIRT_XEN)	+= grant-table.o features.o balloon.o manage.o preempt.o
 obj-$(CONFIG_PARAVIRT_XEN)	+= events/
 
 xen-balloon_$(CONFIG_XEN)	:= balloon/
--- a/drivers/xen/blkback/blkback.c
+++ b/drivers/xen/blkback/blkback.c
@@ -751,7 +751,7 @@ static int __init blkif_init(void)
 	kfree(pending_reqs);
 	kfree(pending_grant_handles);
 	free_empty_pages_and_pagevec(pending_pages, mmap_pages);
-	pr_warning("%s: out of memory\n", __FUNCTION__);
+	pr_warning("%s: out of memory\n", __func__);
 	return -ENOMEM;
 }
 
--- a/drivers/xen/blkback/xenbus.c
+++ b/drivers/xen/blkback/xenbus.c
@@ -24,7 +24,7 @@
 #undef DPRINTK
 #define DPRINTK(fmt, args...)				\
 	pr_debug("blkback/xenbus (%s:%d) " fmt ".\n",	\
-		 __FUNCTION__, __LINE__, ##args)
+		 __func__, __LINE__, ##args)
 
 static void connect(struct backend_info *);
 static int connect_ring(struct backend_info *);
@@ -414,7 +414,7 @@ static void frontend_changed(struct xenb
 	case XenbusStateInitialising:
 		if (dev->state == XenbusStateClosed) {
 			pr_info("%s: %s: prepare for reconnect\n",
-				__FUNCTION__, dev->nodename);
+				__func__, dev->nodename);
 			xenbus_switch_state(dev, XenbusStateInitWait);
 		}
 		break;
--- a/drivers/xen/blktap/blktap.c
+++ b/drivers/xen/blktap/blktap.c
@@ -983,7 +983,7 @@ static int req_increase(void)
  out_of_memory:
 	free_empty_pages_and_pagevec(foreign_pages[mmap_alloc], mmap_pages);
 	kfree(pending_reqs[mmap_alloc]);
-	WPRINTK("%s: out of memory\n", __FUNCTION__);
+	WPRINTK("%s: out of memory\n", __func__);
 	return -ENOMEM;
 }
 
--- a/drivers/xen/blktap/xenbus.c
+++ b/drivers/xen/blktap/xenbus.c
@@ -353,7 +353,7 @@ static void tap_frontend_changed(struct 
 	case XenbusStateInitialising:
 		if (dev->state == XenbusStateClosed) {
 			pr_info("%s: %s: prepare for reconnect\n",
-				__FUNCTION__, dev->nodename);
+				__func__, dev->nodename);
 			xenbus_switch_state(dev, XenbusStateInitWait);
 		}
 		break;
--- a/drivers/xen/char/mem.c
+++ b/drivers/xen/char/mem.c
@@ -13,7 +13,7 @@
 #include <linux/fs.h>
 #include <linux/capability.h>
 #include <linux/ptrace.h>
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 #include <asm/io.h>
 #include <asm/hypervisor.h>
 
--- a/drivers/xen/core/cpu_hotplug.c
+++ b/drivers/xen/core/cpu_hotplug.c
@@ -160,7 +160,7 @@ int cpu_up_check(unsigned int cpu)
 		cpumask_set_cpu(cpu, local_allowed_cpumask);
 		if (!cpumask_test_cpu(cpu, xenbus_allowed_cpumask)) {
 			pr_warning("%s: attempt to bring up CPU %u disallowed "
-				   "by remote admin.\n", __FUNCTION__, cpu);
+				   "by remote admin.\n", __func__, cpu);
 			rc = -EBUSY;
 		}
 	} else if (!cpumask_test_cpu(cpu, local_allowed_cpumask) ||
--- a/drivers/xen/core/spinlock.c
+++ b/drivers/xen/core/spinlock.c
@@ -150,8 +150,8 @@ static unsigned int ticket_get(arch_spin
 					  (struct __raw_tickets)
 					  { .tail = TICKET_LOCK_INC });
 
-	return token.head == token.tail ? token.tail
-					: spin_adjust(prev, lock, token.tail);
+	return __tickets_equal(token.head, token.tail)
+	       ? token.tail : spin_adjust(prev, lock, token.tail);
 }
 
 void xen_spin_irq_enter(void)
--- a/drivers/xen/netback/accel.c
+++ b/drivers/xen/netback/accel.c
@@ -60,7 +60,7 @@ static int match_accelerator(struct xenb
 	if (IS_ERR(eth_name)) {
 		/* Probably means not present */
 		DPRINTK("%s: no match due to xenbus_read accel error %ld\n",
-			__FUNCTION__, PTR_ERR(eth_name));
+			__func__, PTR_ERR(eth_name));
 		return 0;
 	} else {
 		if (!strcmp(eth_name, accelerator->eth_name))
@@ -161,7 +161,7 @@ int netback_connect_accelerator(unsigned
 		kmalloc(sizeof(struct netback_accelerator), GFP_KERNEL);
 	if (!new_accelerator) {
 		DPRINTK("%s: failed to allocate memory for accelerator\n",
-			__FUNCTION__);
+			__func__);
 		return -ENOMEM;
 	}
 
@@ -171,7 +171,7 @@ int netback_connect_accelerator(unsigned
 	new_accelerator->eth_name = kmalloc(eth_name_len, GFP_KERNEL);
 	if (!new_accelerator->eth_name) {
 		DPRINTK("%s: failed to allocate memory for eth_name string\n",
-			__FUNCTION__);
+			__func__);
 		kfree(new_accelerator);
 		return -ENOMEM;
 	}
--- a/drivers/xen/netback/netback.c
+++ b/drivers/xen/netback/netback.c
@@ -1955,7 +1955,7 @@ static int __init netback_init(void)
 
 	mmap_pages = alloc_empty_pages_and_pagevec(MAX_PENDING_REQS);
 	if (mmap_pages == NULL) {
-		pr_err("%s: out of memory\n", __FUNCTION__);
+		pr_err("%s: out of memory\n", __func__);
 		return -ENOMEM;
 	}
 
--- a/drivers/xen/netback/xenbus.c
+++ b/drivers/xen/netback/xenbus.c
@@ -249,7 +249,7 @@ static void frontend_changed(struct xenb
 	case XenbusStateInitialising:
 		if (dev->state == XenbusStateClosed) {
 			pr_info("%s: %s: prepare for reconnect\n",
-				__FUNCTION__, dev->nodename);
+				__func__, dev->nodename);
 			xenbus_switch_state(dev, XenbusStateInitWait);
 		}
 		break;
--- a/drivers/xen/netfront/accel.c
+++ b/drivers/xen/netfront/accel.c
@@ -41,7 +41,7 @@
 
 #define DPRINTK(fmt, args...)				\
 	pr_debug("netfront/accel (%s:%d) " fmt,		\
-	       __FUNCTION__, __LINE__, ##args)
+	       __func__, __LINE__, ##args)
 
 static int netfront_remove_accelerator(struct netfront_info *np,
 				       struct xenbus_device *dev);
@@ -177,7 +177,7 @@ void netfront_accelerator_add_watch(stru
 				 accel_watch_changed);
 	if (err) {
 		DPRINTK("%s: Failed to register accel watch: %d\n",
-                        __FUNCTION__, err);
+                        __func__, err);
 		np->accel_vif_state.accel_watch.node = NULL;
         }
 }
--- a/drivers/xen/netfront/netfront.c
+++ b/drivers/xen/netfront/netfront.c
@@ -210,7 +210,7 @@ static inline grant_ref_t xennet_get_rx_
 
 #define DPRINTK(fmt, args...)				\
 	pr_debug("netfront (%s:%d) " fmt,		\
-		 __FUNCTION__, __LINE__, ##args)
+		 __func__, __LINE__, ##args)
 
 static int setup_device(struct xenbus_device *, struct netfront_info *);
 static struct net_device *create_netdev(struct xenbus_device *);
@@ -226,11 +226,7 @@ static void network_alloc_rx_buffers(str
 static irqreturn_t netif_int(int irq, void *dev_id);
 
 #ifdef CONFIG_SYSFS
-static int xennet_sysfs_addif(struct net_device *netdev);
-static void xennet_sysfs_delif(struct net_device *netdev);
-#else /* !CONFIG_SYSFS */
-#define xennet_sysfs_addif(dev) (0)
-#define xennet_sysfs_delif(dev) do { } while(0)
+static const struct attribute_group xennet_dev_group;
 #endif
 
 static inline bool xennet_can_sg(struct net_device *dev)
@@ -287,24 +283,17 @@ static int netfront_probe(struct xenbus_
 
 	info = netdev_priv(netdev);
 	dev_set_drvdata(&dev->dev, info);
-
+#ifdef CONFIG_SYSFS
+	info->netdev->sysfs_groups[0] = &xennet_dev_group;
+#endif
 	err = register_netdev(info->netdev);
 	if (err) {
-		pr_warning("%s: register_netdev err=%d\n",
-			   __FUNCTION__, err);
+		pr_warn("%s: register_netdev err=%d\n", __func__, err);
 		goto fail;
 	}
 
 	netfront_enable_arp_notify(info);
 
-	err = xennet_sysfs_addif(info->netdev);
-	if (err) {
-		unregister_netdev(info->netdev);
-		pr_warning("%s: add sysfs failed err=%d\n",
-			   __FUNCTION__, err);
-		goto fail;
-	}
-
 	return 0;
 
  fail:
@@ -325,8 +314,6 @@ static int netfront_remove(struct xenbus
 
 	del_timer_sync(&info->rx_refill_timer);
 
-	xennet_sysfs_delif(info->netdev);
-
 	unregister_netdev(info->netdev);
 
 	netfront_free_netdev(info->netdev);
@@ -1652,7 +1639,7 @@ static void netif_release_rx_bufs_flip(s
 	}
 
 	DPRINTK("%s: %d xfer, %d noxfer, %d unused\n",
-		__FUNCTION__, xfer, noxfer, unused);
+		__func__, xfer, noxfer, unused);
 
 	if (xfer) {
 		/* Some pages are no longer absent... */
@@ -1706,7 +1693,7 @@ static void netif_release_rx_bufs_copy(s
 
 	if (busy)
 		DPRINTK("%s: Unable to release %d of %d inuse grant references out of %ld total.\n",
-			__FUNCTION__, busy, inuse, NET_RX_RING_SIZE);
+			__func__, busy, inuse, NET_RX_RING_SIZE);
 
 	spin_unlock_bh(&np->rx_lock);
 }
@@ -1739,8 +1726,7 @@ static int xennet_set_mac_address(struct
 
 static int xennet_change_mtu(struct net_device *dev, int mtu)
 {
-	int max = xennet_can_sg(dev) ? XEN_NETIF_MAX_TX_SIZE - MAX_TCP_HEADER
-				     : ETH_DATA_LEN;
+	int max = xennet_can_sg(dev) ? XEN_NETIF_MAX_TX_SIZE : ETH_DATA_LEN;
 
 	if (mtu > max)
 		return -EINVAL;
@@ -2049,39 +2035,19 @@ static ssize_t show_rxbuf_cur(struct dev
 	return sprintf(buf, "%u\n", info->rx_target);
 }
 
-static struct device_attribute xennet_attrs[] = {
-	__ATTR(rxbuf_min, S_IRUGO|S_IWUSR, show_rxbuf_min, store_rxbuf_min),
-	__ATTR(rxbuf_max, S_IRUGO|S_IWUSR, show_rxbuf_max, store_rxbuf_max),
-	__ATTR(rxbuf_cur, S_IRUGO, show_rxbuf_cur, NULL),
+static DEVICE_ATTR(rxbuf_min, S_IRUGO|S_IWUSR, show_rxbuf_min, store_rxbuf_min);
+static DEVICE_ATTR(rxbuf_max, S_IRUGO|S_IWUSR, show_rxbuf_max, store_rxbuf_max);
+static DEVICE_ATTR(rxbuf_cur, S_IRUGO, show_rxbuf_cur, NULL);
+
+static struct attribute *xennet_dev_attrs[] = {
+	&dev_attr_rxbuf_min.attr,
+	&dev_attr_rxbuf_max.attr,
+	&dev_attr_rxbuf_cur.attr,
+	NULL
+};
+static const struct attribute_group xennet_dev_group = {
+	.attrs = xennet_dev_attrs
 };
-
-static int xennet_sysfs_addif(struct net_device *netdev)
-{
-	int i;
-	int error = 0;
-
-	for (i = 0; i < ARRAY_SIZE(xennet_attrs); i++) {
-		error = device_create_file(&netdev->dev,
-					   &xennet_attrs[i]);
-		if (error)
-			goto fail;
-	}
-	return 0;
-
- fail:
-	while (--i >= 0)
-		device_remove_file(&netdev->dev, &xennet_attrs[i]);
-	return error;
-}
-
-static void xennet_sysfs_delif(struct net_device *netdev)
-{
-	int i;
-
-	for (i = 0; i < ARRAY_SIZE(xennet_attrs); i++)
-		device_remove_file(&netdev->dev, &xennet_attrs[i]);
-}
-
 #endif /* CONFIG_SYSFS */
 
 
@@ -2251,10 +2217,6 @@ static struct net_device *create_netdev(
 	netdev->ethtool_ops = &network_ethtool_ops;
 	SET_NETDEV_DEV(netdev, &dev->dev);
 
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26)
-	netif_set_gso_max_size(netdev, XEN_NETIF_MAX_TX_SIZE - MAX_TCP_HEADER);
-#endif
-
 	np->netdev = netdev;
 
 	netfront_carrier_off(np);
--- a/drivers/xen/scsiback/emulate.c
+++ b/drivers/xen/scsiback/emulate.c
@@ -129,7 +129,7 @@ static int __copy_to_sg(struct scatterli
 	for_each_sg (sgl, sg, nr_sg, i) {
 		if (sg_page(sg) == NULL) {
 			pr_warning("%s: inconsistent length field in "
-				   "scatterlist\n", __FUNCTION__);
+				   "scatterlist\n", __func__);
 			return -ENOMEM;
 		}
 
@@ -148,7 +148,7 @@ static int __copy_to_sg(struct scatterli
 		from += copy_size;
 	}
 
-	pr_warning("%s: no space in scatterlist\n", __FUNCTION__);
+	pr_warning("%s: no space in scatterlist\n", __func__);
 	return -ENOMEM;
 }
 
@@ -168,14 +168,14 @@ static int __maybe_unused __copy_from_sg
 	for_each_sg (sgl, sg, nr_sg, i) {
 		if (sg_page(sg) == NULL) {
 			pr_warning("%s: inconsistent length field in "
-				   "scatterlist\n", __FUNCTION__);
+				   "scatterlist\n", __func__);
 			return -ENOMEM;
 		}
 
 		from_rest = sg->length;
 		if ((from_rest > 0) && (to_capa < from_rest)) {
 			pr_warning("%s: no space in destination buffer\n",
-				   __FUNCTION__);
+				   __func__);
 			return -ENOMEM;
 		}
 		copy_size = from_rest;
@@ -245,7 +245,7 @@ retry:
 	alloc_len  = sizeof(struct scsi_lun) * alloc_luns
 				+ VSCSI_REPORT_LUNS_HEADER;
 	if ((buff = kzalloc(alloc_len, GFP_KERNEL)) == NULL) {
-		pr_err("scsiback:%s kmalloc err\n", __FUNCTION__);
+		pr_err("scsiback:%s kmalloc err\n", __func__);
 		goto fail;
 	}
 
--- a/drivers/xen/scsiback/scsiback.c
+++ b/drivers/xen/scsiback/scsiback.c
@@ -153,7 +153,7 @@ void scsiback_do_resp_with_sense(char *s
 	struct scsi_sense_hdr sshdr;
 	unsigned long flags;
 
-	DPRINTK("%s\n",__FUNCTION__);
+	DPRINTK("%s\n", __func__);
 
 	spin_lock_irqsave(&info->ring_lock, flags);
 
@@ -254,7 +254,7 @@ static int scsiback_gnttab_data_map(vscs
 		pending_req->sgl = kmalloc(sizeof(struct scatterlist) * nr_segments,
 						GFP_KERNEL);
 		if (!pending_req->sgl) {
-			pr_err("scsiback: %s: kmalloc() error\n", __FUNCTION__);
+			pr_err("scsiback: %s: kmalloc() error\n", __func__);
 			return -ENOMEM;
 		}
 
@@ -404,7 +404,7 @@ int scsiback_cmd_exec(pending_req_t *pen
 	struct request *rq;
 	int write;
 
-	DPRINTK("%s\n",__FUNCTION__);
+	DPRINTK("%s\n", __func__);
 
 	/* because it doesn't timeout backend earlier than frontend.*/
 	if (pending_req->timeout_per_command)
@@ -500,7 +500,7 @@ static int prepare_pending_reqs(struct v
 	struct ids_tuple vir;
 	int err = -EINVAL;
 
-	DPRINTK("%s\n",__FUNCTION__);
+	DPRINTK("%s\n", __func__);
 
 	pending_req->rqid       = ring_req->rqid;
 	pending_req->act        = ring_req->act;
@@ -577,7 +577,7 @@ static int _scsiback_do_cmd_fn(struct vs
 	RING_IDX rc, rp;
 	int err, more_to_do = 0;
 
-	DPRINTK("%s\n",__FUNCTION__);
+	DPRINTK("%s\n", __func__);
 
 	rc = ring->req_cons;
 	rp = ring->sring->req_prod;
@@ -669,7 +669,7 @@ int scsiback_schedule(void *data)
 {
 	struct vscsibk_info *info = (struct vscsibk_info *)data;
 
-	DPRINTK("%s\n",__FUNCTION__);
+	DPRINTK("%s\n", __func__);
 
 	while (!kthread_should_stop()) {
 		wait_event_interruptible(
@@ -742,7 +742,7 @@ out_of_memory:
 	kfree(pending_reqs);
 	kfree(pending_grant_handles);
 	free_empty_pages_and_pagevec(pending_pages, mmap_pages);
-	pr_err("scsiback: %s: out of memory\n", __FUNCTION__);
+	pr_err("scsiback: %s: out of memory\n", __func__);
 	return -ENOMEM;
 }
 
--- a/drivers/xen/scsiback/translate.c
+++ b/drivers/xen/scsiback/translate.c
@@ -72,7 +72,7 @@ int scsiback_add_translation_entry(struc
 
 	/* Create a new translation entry and add to the list */
 	if ((new = kmalloc(sizeof(struct v2p_entry), GFP_ATOMIC)) == NULL) {
-		pr_err("scsiback: %s: kmalloc() error\n", __FUNCTION__);
+		pr_err("scsiback: %s: kmalloc() error\n", __func__);
 		err = -ENOMEM;
 		goto out;
 	}
--- a/drivers/xen/scsiback/xenbus.c
+++ b/drivers/xen/scsiback/xenbus.c
@@ -351,7 +351,7 @@ static int scsiback_probe(struct xenbus_
 
 
 fail:
-	pr_warning("scsiback: %s failed\n",__FUNCTION__);
+	pr_warning("scsiback: %s failed\n", __func__);
 	scsiback_remove(dev);
 
 	return err;
--- a/drivers/xen/scsifront/xenbus.c
+++ b/drivers/xen/scsifront/xenbus.c
@@ -123,7 +123,7 @@ static int scsifront_init_ring(struct vs
 	struct xenbus_transaction xbt;
 	int err;
 
-	DPRINTK("%s\n",__FUNCTION__);
+	DPRINTK("%s\n", __func__);
 
 	err = scsifront_alloc_ring(info);
 	if (err)
@@ -307,7 +307,7 @@ static int scsifront_remove(struct xenbu
 {
 	struct vscsifrnt_info *info = dev_get_drvdata(&dev->dev);
 
-	DPRINTK("%s: %s removed\n",__FUNCTION__ ,dev->nodename);
+	DPRINTK("%s: %s removed\n", __func__, dev->nodename);
 
 	if (info->kthread) {
 		kthread_stop(info->kthread);
@@ -325,7 +325,7 @@ static int scsifront_disconnect(struct v
 	struct xenbus_device *dev = info->dev;
 	struct Scsi_Host *host = info->host;
 
-	DPRINTK("%s: %s disconnect\n",__FUNCTION__ ,dev->nodename);
+	DPRINTK("%s: %s disconnect\n", __func__, dev->nodename);
 
 	/* 
 	  When this function is executed,  all devices of 
--- a/drivers/xen/sfc_netback/accel_fwd.c
+++ b/drivers/xen/sfc_netback/accel_fwd.c
@@ -400,7 +400,7 @@ void netback_accel_tx_packet(struct sk_b
 		mac = skb_mac_header(skb)+ETH_ALEN;
 
 		DPRINTK("%s: found gratuitous ARP for %pM\n",
-			__FUNCTION__, mac);
+			__func__, mac);
 
 		spin_lock_irqsave(&fwd_set->fwd_lock, flags);
 		/*
--- a/drivers/xen/sfc_netback/accel_msg.c
+++ b/drivers/xen/sfc_netback/accel_msg.c
@@ -82,7 +82,7 @@ static void netback_accel_msg_tx_localma
 		 * traffic
 		 */
 		EPRINTK("%s: saw full queue, may need ARP timer to recover\n",
-			__FUNCTION__);
+			__func__);
 	}
 }
 
@@ -103,7 +103,7 @@ static int netback_accel_msg_rx_buffer_m
 	log2_pages = log2_ge(msg->u.mapbufs.pages, 0);
 	if (msg->u.mapbufs.pages != pow2(log2_pages)) {
 		EPRINTK("%s: Can only alloc bufs in power of 2 sizes (%d)\n",
-			__FUNCTION__, msg->u.mapbufs.pages);
+			__func__, msg->u.mapbufs.pages);
 		rc = -EINVAL;
 		goto err_out;
 	}
@@ -114,7 +114,7 @@ static int netback_accel_msg_rx_buffer_m
 	 */
 	if (msg->u.mapbufs.pages > NET_ACCEL_MSG_MAX_PAGE_REQ) {
 		EPRINTK("%s: too many pages in a single message: %d %d\n", 
-			__FUNCTION__, msg->u.mapbufs.pages,
+			__func__, msg->u.mapbufs.pages,
 			NET_ACCEL_MSG_MAX_PAGE_REQ);
 		rc = -EINVAL;
 		goto err_out;
@@ -131,7 +131,7 @@ static int netback_accel_msg_rx_buffer_m
 	return 0;
 
  err_out:
-	EPRINTK("%s: err_out\n", __FUNCTION__);
+	EPRINTK("%s: err_out\n", __func__);
 	msg->id |= NET_ACCEL_MSG_ERROR | NET_ACCEL_MSG_REPLY;
 	return rc;
 }
@@ -375,7 +375,7 @@ void netback_accel_msg_rx_handler(void *
 		err = process_rx_msg(bend, &msg);
 		
 		if (err != 0) {
-			EPRINTK("%s: Error %d\n", __FUNCTION__, err);
+			EPRINTK("%s: Error %d\n", __func__, err);
 			goto err;
 		}
 	}
--- a/drivers/xen/sfc_netback/accel_solarflare.c
+++ b/drivers/xen/sfc_netback/accel_solarflare.c
@@ -172,7 +172,7 @@ static int bend_dl_probe(struct efx_dl_d
 	enum net_accel_hw_type type;
 	struct driverlink_port *port;
 
-	DPRINTK("%s: %s\n", __FUNCTION__, silicon_rev);
+	DPRINTK("%s: %s\n", __func__, silicon_rev);
 
 	if (strcmp(silicon_rev, "falcon/a1") == 0)
 		type = NET_ACCEL_MSG_HWTYPE_FALCON_A;
@@ -181,15 +181,14 @@ static int bend_dl_probe(struct efx_dl_d
 	else if (strcmp(silicon_rev, "siena/a0") == 0)
 		type = NET_ACCEL_MSG_HWTYPE_SIENA_A;
 	else {
-		EPRINTK("%s: unsupported silicon %s\n", __FUNCTION__,
-			silicon_rev);
+		EPRINTK("%s: unsupported silicon %s\n", __func__, silicon_rev);
 		rc = -EINVAL;
 		goto fail1;
 	}
 	
 	port = kmalloc(sizeof(struct driverlink_port), GFP_KERNEL);
 	if (port == NULL) {
-		EPRINTK("%s: no memory for dl probe\n", __FUNCTION__);
+		EPRINTK("%s: no memory for dl probe\n", __func__);
 		rc = -ENOMEM;
 		goto fail1;
 	}
@@ -200,14 +199,14 @@ static int bend_dl_probe(struct efx_dl_d
 	port->fwd_priv = netback_accel_init_fwd_port();
 	if (port->fwd_priv == NULL) {
 		EPRINTK("%s: failed to set up forwarding for port\n",
-			__FUNCTION__);
+			__func__);
 		rc = -ENOMEM;
 		goto fail2;
 	}
 
 	rc = efx_dl_register_callbacks(efx_dl_dev, &bend_dl_callbacks);
 	if (rc != 0) {
-		EPRINTK("%s: register_callbacks failed\n", __FUNCTION__);
+		EPRINTK("%s: register_callbacks failed\n", __func__);
 		goto fail3;
 	}
 
@@ -416,14 +415,14 @@ static int alloc_page_state(struct netba
 	struct falcon_bend_accel_priv *accel_hw_priv;
 
 	if (max_pages < 0 || max_pages > bend->quotas.max_buf_pages) {
-		EPRINTK("%s: invalid max_pages: %d\n", __FUNCTION__, max_pages);
+		EPRINTK("%s: invalid max_pages: %d\n", __func__, max_pages);
 		return -EINVAL;
 	}
 
 	accel_hw_priv = kzalloc(sizeof(struct falcon_bend_accel_priv),
 				GFP_KERNEL);
 	if (accel_hw_priv == NULL) {
-		EPRINTK("%s: no memory for accel_hw_priv\n", __FUNCTION__);
+		EPRINTK("%s: no memory for accel_hw_priv\n", __func__);
 		return -ENOMEM;
 	}
 
@@ -431,7 +430,7 @@ static int alloc_page_state(struct netba
 		(sizeof(struct efx_vi_dma_map_state **) * 
 		 (max_pages / NET_ACCEL_MSG_MAX_PAGE_REQ), GFP_KERNEL);
 	if (accel_hw_priv->dma_maps == NULL) {
-		EPRINTK("%s: no memory for dma_maps\n", __FUNCTION__);
+		EPRINTK("%s: no memory for dma_maps\n", __func__);
 		kfree(accel_hw_priv);
 		return -ENOMEM;
 	}
@@ -439,7 +438,7 @@ static int alloc_page_state(struct netba
 	bend->buffer_maps = kzalloc(sizeof(struct vm_struct *) * max_pages, 
 				    GFP_KERNEL);
 	if (bend->buffer_maps == NULL) {
-		EPRINTK("%s: no memory for buffer_maps\n", __FUNCTION__);
+		EPRINTK("%s: no memory for buffer_maps\n", __func__);
 		kfree(accel_hw_priv->dma_maps);
 		kfree(accel_hw_priv);
 		return -ENOMEM;
@@ -463,7 +462,7 @@ static int free_page_state(struct netbac
 {
 	struct falcon_bend_accel_priv *accel_hw_priv;
 
-	DPRINTK("%s: %p\n", __FUNCTION__, bend);
+	DPRINTK("%s: %p\n", __func__, bend);
 
 	accel_hw_priv = bend->accel_hw_priv;
 
@@ -520,7 +519,7 @@ static int ef_get_vnic(struct netback_ac
 
 	rc = efx_vi_alloc(&accel_hw_priv->efx_vih, bend->net_dev->ifindex);
 	if (rc != 0) {
-		EPRINTK("%s: efx_vi_alloc failed %d\n", __FUNCTION__, rc);
+		EPRINTK("%s: efx_vi_alloc failed %d\n", __func__, rc);
 		free_page_state(bend);
 		return rc;
 	}
@@ -529,7 +528,7 @@ static int ef_get_vnic(struct netback_ac
 					     bend_evq_timeout,
 					     bend);
 	if (rc != 0) {
-		EPRINTK("%s: register_callback failed %d\n", __FUNCTION__, rc);
+		EPRINTK("%s: register_callback failed %d\n", __func__, rc);
 		efx_vi_free(accel_hw_priv->efx_vih);
 		free_page_state(bend);
 		return rc;
@@ -608,8 +607,7 @@ static int ef_bend_hwinfo_falcon_common(
 	rc = efx_vi_hw_resource_get_phys(accel_hw_priv->efx_vih, &res_mdata,
 					 res_array, &len);
 	if (rc != 0) {
-		DPRINTK("%s: resource_get_phys returned %d\n",
-			__FUNCTION__, rc);
+		DPRINTK("%s: resource_get_phys returned %d\n", __func__, rc);
 		return rc;
 	}
 
@@ -652,7 +650,7 @@ static int ef_bend_hwinfo_falcon_common(
 			break;
 		default:
 			EPRINTK("%s: Unknown hardware resource type %d\n",
-				__FUNCTION__, res->type);
+				__func__, res->type);
 			break;
 		}
 	}
@@ -900,7 +898,7 @@ int netback_accel_add_buffers(struct net
 	if (accel_hw_priv->dma_maps_index >= 
 	    bend->max_pages / NET_ACCEL_MSG_MAX_PAGE_REQ) {
 		EPRINTK("%s: too many buffer table allocations: %d %d\n",
-			__FUNCTION__, accel_hw_priv->dma_maps_index, 
+			__func__, accel_hw_priv->dma_maps_index,
 			bend->max_pages / NET_ACCEL_MSG_MAX_PAGE_REQ);
 		return -EINVAL;
 	}
@@ -908,13 +906,13 @@ int netback_accel_add_buffers(struct net
 	/* Make sure we can't overflow the buffer_maps array */
 	if (bend->buffer_maps_index + pages > bend->max_pages) {
 		EPRINTK("%s: too many pages mapped: %d + %d > %d\n", 
-			__FUNCTION__, bend->buffer_maps_index,
+			__func__, bend->buffer_maps_index,
 			pages, bend->max_pages);
 		return -EINVAL;
 	}
 
 	for (i = 0; i < pages; i++) {
-		VPRINTK("%s: mapping page %d\n", __FUNCTION__, i);
+		VPRINTK("%s: mapping page %d\n", __func__, i);
 		rc = net_accel_map_device_page
 			(bend->hdev_data, grants[i],
 			 &bend->buffer_maps[bend->buffer_maps_index],
@@ -933,7 +931,7 @@ int netback_accel_add_buffers(struct net
 		addr_array[i] = dev_bus_addr;
 	}
 
-	VPRINTK("%s: mapping dma addresses to vih %p\n", __FUNCTION__, 
+	VPRINTK("%s: mapping dma addresses to vih %p\n", __func__,
 		accel_hw_priv->efx_vih);
 
 	index = accel_hw_priv->dma_maps_index;
@@ -947,12 +945,12 @@ int netback_accel_add_buffers(struct net
 	accel_hw_priv->dma_maps_index++;
 	NETBACK_ACCEL_STATS_OP(bend->stats.num_buffer_pages += pages);
 
-	//DPRINTK("%s: getting map address\n", __FUNCTION__);
+	//DPRINTK("%s: getting map address\n", __func__);
 
 	*buf_addr_out = efx_vi_dma_get_map_addr(accel_hw_priv->efx_vih, 
 						accel_hw_priv->dma_maps[index]);
 
-	//DPRINTK("%s: done\n", __FUNCTION__);
+	//DPRINTK("%s: done\n", __func__);
 
 	return 0;
 }
@@ -1059,7 +1057,7 @@ void netback_accel_free_filter(struct fa
 		if (cuckoo_hash_remove(&accel_hw_priv->filter_hash_table,
 				       (cuckoo_hash_key *)&filter_key)) {
 			EPRINTK("%s: Couldn't find filter to remove from table\n",
-				__FUNCTION__);
+				__func__);
 			BUG();
 		}
 	}
@@ -1215,7 +1213,7 @@ netback_accel_filter_check_add(struct ne
 
 	NETBACK_ACCEL_STATS_OP(bend->stats.num_filters++);
 
-	VPRINTK("%s: success index %d handle %p\n", __FUNCTION__, filter_index, 
+	VPRINTK("%s: success index %d handle %p\n", __func__, filter_index,
 		fs->filter_handle);
 
 	rc = filter_index;
--- a/drivers/xen/sfc_netback/accel_xenbus.c
+++ b/drivers/xen/sfc_netback/accel_xenbus.c
@@ -163,7 +163,7 @@ static int setup_config_accel_watch(stru
 
 	if (err) {
 		EPRINTK("%s: Failed to register xenbus watch: %d\n",
-			__FUNCTION__, err);
+			__func__, err);
 		bend->config_accel_watch.node = NULL;
 		return err;
 	}
@@ -349,13 +349,13 @@ static int publish_frontend_name(struct 
 	do {
 		err = xenbus_transaction_start(&tr);
 		if (err != 0) { 
-			EPRINTK("%s: transaction start failed\n", __FUNCTION__);
+			EPRINTK("%s: transaction start failed\n", __func__);
 			return err;
 		}
 		err = xenbus_printf(tr, dev->nodename, "accel-frontend", 
 				    "%s", frontend_name);
 		if (err != 0) {
-			EPRINTK("%s: xenbus_printf failed\n", __FUNCTION__);
+			EPRINTK("%s: xenbus_printf failed\n", __func__);
 			xenbus_transaction_end(tr, 1);
 			return err;
 		}
@@ -397,16 +397,16 @@ static void cleanup_vnic(struct netback_
 
 	dev = (struct xenbus_device *)bend->hdev_data;
 
-	DPRINTK("%s: bend %p dev %p\n", __FUNCTION__, bend, dev);
+	DPRINTK("%s: bend %p dev %p\n", __func__, bend, dev);
 
 	DPRINTK("%s: Remove %p's mac from fwd table...\n", 
-		__FUNCTION__, bend);
+		__func__, bend);
 	netback_accel_fwd_remove(bend->mac, bend->fwd_priv);
 
 	/* Free buffer table allocations */
 	netback_accel_remove_buffers(bend);
 
-	DPRINTK("%s: Release hardware resources...\n", __FUNCTION__);
+	DPRINTK("%s: Release hardware resources...\n", __func__);
 	if (bend->accel_shutdown)
 		bend->accel_shutdown(bend);
 
@@ -421,7 +421,7 @@ static void cleanup_vnic(struct netback_
 	}
 
 	if (bend->sh_pages_unmap) {
-		DPRINTK("%s: Unmap grants %p\n", __FUNCTION__, 
+		DPRINTK("%s: Unmap grants %p\n", __func__,
 			bend->sh_pages_unmap);
 		net_accel_unmap_grants_contig(dev, bend->sh_pages_unmap);
 		bend->sh_pages_unmap = NULL;
@@ -459,7 +459,7 @@ static void netback_accel_frontend_chang
 	XenbusState backend_state;
 
 	DPRINTK("%s: changing from %s to %s. nodename %s, otherend %s\n",
-		__FUNCTION__, xenbus_strstate(bend->frontend_state),
+		__func__, xenbus_strstate(bend->frontend_state),
 		xenbus_strstate(frontend_state),dev->nodename, dev->otherend);
 
 	/*
@@ -585,7 +585,7 @@ static int setup_domu_accel_watch(struct
 				 bend_domu_accel_change);
 	if (err) {
 		EPRINTK("%s: Failed to register xenbus watch: %d\n",
-			__FUNCTION__, err);
+			__func__, err);
 		goto fail;
 	}
 	return 0;
@@ -601,12 +601,12 @@ int netback_accel_probe(struct xenbus_de
 	struct backend_info *binfo;
 	int err;
 
-	DPRINTK("%s: passed device %s\n", __FUNCTION__, dev->nodename);
+	DPRINTK("%s: passed device %s\n", __func__, dev->nodename);
 
 	/* Allocate structure to store all our state... */
 	bend = kzalloc(sizeof(struct netback_accel), GFP_KERNEL);
 	if (bend == NULL) {
-		DPRINTK("%s: no memory for bend\n", __FUNCTION__);
+		DPRINTK("%s: no memory for bend\n", __func__);
 		return -ENOMEM;
 	}
 	
@@ -620,7 +620,7 @@ int netback_accel_probe(struct xenbus_de
 	/* And vice-versa */
 	bend->hdev_data = dev;
 
-	DPRINTK("%s: Adding bend %p to list\n", __FUNCTION__, bend);
+	DPRINTK("%s: Adding bend %p to list\n", __func__, bend);
 	
 	init_waitqueue_head(&bend->state_wait_queue);
 	bend->vnic_is_setup = 0;
@@ -732,7 +732,7 @@ int netback_accel_remove(struct xenbus_d
 	binfo = dev_get_drvdata(&dev->dev);
 	bend = (struct netback_accel *) binfo->netback_accel_priv;
 
-	DPRINTK("%s: dev %p bend %p\n", __FUNCTION__, dev, bend);
+	DPRINTK("%s: dev %p bend %p\n", __func__, dev, bend);
 	
 	BUG_ON(bend == NULL);
 	
--- a/drivers/xen/sfc_netback/ci/tools/debug.h
+++ b/drivers/xen/sfc_netback/ci/tools/debug.h
@@ -153,77 +153,77 @@
 
 #define CI_TRACE(exp,fmt)						\
   ci_log("%s:%d:%s] " #exp "=" fmt,                                     \
-         __FILE__, __LINE__, __FUNCTION__, (exp))
+         __FILE__, __LINE__, __func__, (exp))
 
 
 #define CI_TRACE_INT(integer)						\
   ci_log("%s:%d:%s] " #integer "=%d",                                   \
-         __FILE__, __LINE__, __FUNCTION__, (integer))
+         __FILE__, __LINE__, __func__, (integer))
 
 
 #define CI_TRACE_INT32(integer)						\
   ci_log("%s:%d:%s] " #integer "=%d",                                   \
-         __FILE__, __LINE__, __FUNCTION__, ((ci_int32)integer))
+         __FILE__, __LINE__, __func__, ((ci_int32)integer))
 
 
 #define CI_TRACE_INT64(integer)						\
   ci_log("%s:%d:%s] " #integer "=%lld",                                 \
-         __FILE__, __LINE__, __FUNCTION__, ((ci_int64)integer))
+         __FILE__, __LINE__, __func__, ((ci_int64)integer))
 
 
 #define CI_TRACE_UINT(integer)						\
   ci_log("%s:%d:%s] " #integer "=%ud",                                  \
-         __FILE__, __LINE__, __FUNCTION__, (integer))
+         __FILE__, __LINE__, __func__, (integer))
 
 
 #define CI_TRACE_UINT32(integer)			  	        \
   ci_log("%s:%d:%s] " #integer "=%ud",                                  \
-         __FILE__, __LINE__, __FUNCTION__, ((ci_uint32)integer))
+         __FILE__, __LINE__, __func__, ((ci_uint32)integer))
 
 
 #define CI_TRACE_UINT64(integer)			  	        \
   ci_log("%s:%d:%s] " #integer "=%ulld",                                \
-         __FILE__, __LINE__, __FUNCTION__, ((ci_uint64)integer))
+         __FILE__, __LINE__, __func__, ((ci_uint64)integer))
 
 
 #define CI_TRACE_HEX(integer)						\
   ci_log("%s:%d:%s] " #integer "=0x%x",                                 \
-         __FILE__, __LINE__, __FUNCTION__, (integer))
+         __FILE__, __LINE__, __func__, (integer))
 
 
 #define CI_TRACE_HEX32(integer)						\
   ci_log("%s:%d:%s] " #integer "=0x%x",                                 \
-         __FILE__, __LINE__, __FUNCTION__, ((ci_uint32)integer))
+         __FILE__, __LINE__, __func__, ((ci_uint32)integer))
 
 
 #define CI_TRACE_HEX64(integer)						\
   ci_log("%s:%d:%s] " #integer "=0x%llx",                               \
-         __FILE__, __LINE__, __FUNCTION__, ((ci_uint64)integer))
+         __FILE__, __LINE__, __func__, ((ci_uint64)integer))
 
 
 #define CI_TRACE_PTR(pointer)				                \
   ci_log("%s:%d:%s] " #pointer "=0x%p",                                 \
-         __FILE__, __LINE__, __FUNCTION__, (pointer))
+         __FILE__, __LINE__, __func__, (pointer))
 
 
 #define CI_TRACE_STRING(string)					        \
   ci_log("%s:%d:%s] " #string "=%s",                                    \
-         __FILE__, __LINE__, __FUNCTION__, (string))
+         __FILE__, __LINE__, __func__, (string))
 
 
 #define CI_TRACE_MAC(mac)						\
   ci_log("%s:%d:%s] " #mac "=" CI_MAC_PRINTF_FORMAT,                    \
-         __FILE__, __LINE__, __FUNCTION__, CI_MAC_PRINTF_ARGS(mac))
+         __FILE__, __LINE__, __func__, CI_MAC_PRINTF_ARGS(mac))
 
 
 #define CI_TRACE_IP(ip_be32)						\
   ci_log("%s:%d:%s] " #ip_be32 "=" CI_IP_PRINTF_FORMAT, __FILE__,       \
-         __LINE__, __FUNCTION__, CI_IP_PRINTF_ARGS(&(ip_be32)))
+         __LINE__, __func__, CI_IP_PRINTF_ARGS(&(ip_be32)))
 
 
 #define CI_TRACE_ARP(arp_pkt)                                           \
   ci_log("%s:%d:%s]\n"CI_ARP_PRINTF_FORMAT,                             \
-         __FILE__, __LINE__, __FUNCTION__, CI_ARP_PRINTF_ARGS(arp_pkt))
+         __FILE__, __LINE__, __func__, CI_ARP_PRINTF_ARGS(arp_pkt))
 
 #endif  /* NDEBUG */
 
--- a/drivers/xen/sfc_netfront/accel_bufs.c
+++ b/drivers/xen/sfc_netfront/accel_bufs.c
@@ -94,8 +94,8 @@ int netfront_accel_alloc_buffer_mem(stru
 	}
 
 	if (n != pages) {
-		EPRINTK("%s: not enough pages: %d != %d\n", __FUNCTION__, n, 
-			pages);
+		EPRINTK("%s: not enough pages: %d != %d\n", __func__, n,
+		        pages);
 		for (; n >= 0; n--)
 			free_page((unsigned long)(bufpages->page_list[n]));
 		rc = -ENOMEM;
@@ -212,7 +212,7 @@ int netfront_accel_buf_map_request(struc
 
 		mfn = virt_to_mfn(bufpages->page_list[offset+i]);
 		VPRINTK("%s: Granting page %d, mfn %08x\n",
-			__FUNCTION__, i, mfn);
+			__func__, i, mfn);
 
 		bufpages->grant_list[offset+i] =
 			net_accel_grant_page(dev, mfn, 0);
@@ -220,7 +220,7 @@ int netfront_accel_buf_map_request(struc
 
 		if (msg->u.mapbufs.grants[i] < 0) {
 			EPRINTK("%s: Failed to grant buffer: %d\n",
-				__FUNCTION__, msg->u.mapbufs.grants[i]);
+				__func__, msg->u.mapbufs.grants[i]);
 			err = -EIO;
 			goto error;
 		}
@@ -251,7 +251,7 @@ int netfront_accel_add_bufs(struct netfr
 	u32 msg_buf;
 	unsigned long flags;
 
-	VPRINTK("%s: manager %p msg %p\n", __FUNCTION__, manager, msg);
+	VPRINTK("%s: manager %p msg %p\n", __func__, manager, msg);
 
 	BUG_ON(msg->id != (NET_ACCEL_MSG_MAPBUF | NET_ACCEL_MSG_REPLY));
 
--- a/drivers/xen/sfc_netfront/accel_msg.c
+++ b/drivers/xen/sfc_netfront/accel_msg.c
@@ -77,7 +77,7 @@ static void vnic_start_fastpath(netfront
 	struct netfront_info *np = netdev_priv(net_dev);
 	unsigned long flags;
 
-	DPRINTK("%s\n", __FUNCTION__);
+	DPRINTK("%s\n", __func__);
 
 	spin_lock_irqsave(&vnic->tx_lock, flags);
 	vnic->tx_enabled = 1;
@@ -97,7 +97,7 @@ void vnic_stop_fastpath(netfront_accel_v
 	struct netfront_info *np = (struct netfront_info *)netdev_priv(net_dev);
 	unsigned long flags1, flags2;
 
-	DPRINTK("%s\n", __FUNCTION__);
+	DPRINTK("%s\n", __func__);
 
 	vnic_stop_interrupts(vnic);
 	
@@ -168,12 +168,12 @@ static int vnic_add_bufs(netfront_accel_
 		netfront_accel_vi_add_bufs(vnic, bufinfo == vnic->rx_bufs);
 
 		if (offset + msg->u.mapbufs.pages == vnic->bufpages.max_pages) {
-			VPRINTK("%s: got all buffers back\n", __FUNCTION__);
+			VPRINTK("%s: got all buffers back\n", __func__);
 			vnic->frontend_ready = 1;
 			if (vnic->backend_netdev_up)
 				vnic_start_fastpath(vnic);
 		} else {
-			VPRINTK("%s: got buffers back %d %d\n", __FUNCTION__, 
+			VPRINTK("%s: got buffers back %d %d\n", __func__,
 				offset, msg->u.mapbufs.pages);
 		}
 	}
@@ -214,13 +214,13 @@ static int vnic_send_buffer_requests(net
 						&vnic->to_dom0, &msg);
 			if (rc < 0) {
 				VPRINTK("%s: queue full, stopping for now\n",
-					__FUNCTION__);
+					__func__);
 				break;
 			}
 			sent++;
 		} else {
 			EPRINTK("%s: problem with grant, stopping for now\n",
-				__FUNCTION__);
+				__func__);
 			break;
 		}
 
@@ -439,7 +439,7 @@ void netfront_accel_msg_from_bend(void *
 
 		if (vnic->shared_page->aflags &
 		    NET_ACCEL_MSG_AFLAGS_NETUPDOWN) {
-			DPRINTK("%s: net interface change\n", __FUNCTION__);
+			DPRINTK("%s: net interface change\n", __func__);
 			clear_bit(NET_ACCEL_MSG_AFLAGS_NETUPDOWN_B,
 				  (unsigned long *)&vnic->shared_page->aflags);
 			if (vnic->shared_page->net_dev_up)
@@ -480,7 +480,7 @@ void netfront_accel_msg_from_bend(void *
 		vnic_set_queue_not_full(vnic);
 
 	if (err != 0) {
-		EPRINTK("%s returned %d\n", __FUNCTION__, err);
+		EPRINTK("%s returned %d\n", __func__, err);
 		netfront_accel_set_closing(vnic);
 	}
 
@@ -535,7 +535,7 @@ irqreturn_t netfront_accel_net_channel_i
 	else {
 		spin_unlock_irqrestore(&vnic->irq_enabled_lock, flags);
 		NETFRONT_ACCEL_STATS_OP(vnic->stats.useless_irq_count++);
-		DPRINTK("%s: irq when disabled\n", __FUNCTION__);
+		DPRINTK("%s: irq when disabled\n", __func__);
 	}
 	
 	return IRQ_HANDLED;
--- a/drivers/xen/sfc_netfront/accel_netfront.c
+++ b/drivers/xen/sfc_netfront/accel_netfront.c
@@ -64,7 +64,7 @@ static int netfront_accel_netdev_start_x
 	handled = netfront_accel_vi_tx_post(vnic, skb);
 	if (handled == NETFRONT_ACCEL_STATUS_BUSY) {
 		BUG_ON(vnic->net_dev != net_dev);
-		DPRINTK("%s stopping queue\n", __FUNCTION__);
+		DPRINTK("%s stopping queue\n", __func__);
 
 		/* Need netfront's tx_lock and vnic tx_lock to write tx_skb */
 		spin_lock_irqsave(&np->tx_lock, flags2);
@@ -105,7 +105,7 @@ static int netfront_accel_netdev_poll(st
 	NETFRONT_ACCEL_STATS_OP(vnic->stats.poll_call_count++);
 
 	VPRINTK("%s: done %d allowed %d\n",
-		__FUNCTION__, rx_done, rx_allowed);
+		__func__, rx_done, rx_allowed);
 
 	netfront_accel_ssr_end_of_burst(vnic, &vnic->ssr_state);
 
@@ -270,7 +270,7 @@ static int __init netfront_accel_init(vo
 		return 0;
 
 	if (!is_pow2(sizeof(struct net_accel_msg)))
-		EPRINTK("%s: bad structure size\n", __FUNCTION__);
+		EPRINTK("%s: bad structure size\n", __func__);
 
 	netfront_accel_workqueue = create_workqueue(frontend_name);
 
@@ -311,12 +311,12 @@ static void __exit netfront_accel_exit(v
 	if (is_initial_xendomain())
 		return;
 
-	DPRINTK("%s: unhooking\n", __FUNCTION__);
+	DPRINTK("%s: unhooking\n", __func__);
 
 	/* Unhook from normal netfront */
 	netfront_accelerator_stop(frontend_name);
 
-	DPRINTK("%s: done\n", __FUNCTION__);
+	DPRINTK("%s: done\n", __func__);
 
 	netfront_accel_debugfs_fini();
 
--- a/drivers/xen/sfc_netfront/accel_ssr.c
+++ b/drivers/xen/sfc_netfront/accel_ssr.c
@@ -118,7 +118,7 @@ static void netfront_accel_ssr_deliver(n
 					     c->iph->ihl);
 	}
 
-	VPRINTK("%s: %d\n", __FUNCTION__, c->skb->len);
+	VPRINTK("%s: %d\n", __func__, c->skb->len);
 
 	netif_receive_skb(c->skb); 
 	c->skb = NULL;
--- a/drivers/xen/sfc_netfront/accel_tso.c
+++ b/drivers/xen/sfc_netfront/accel_tso.c
@@ -163,7 +163,7 @@ static inline int tso_start_new_buffer(n
 	/* Get a mapped packet buffer */
 	buf = netfront_accel_buf_get(vnic->tx_bufs);
 	if (buf == NULL) {
-		DPRINTK("%s: No buffer for TX\n", __FUNCTION__);
+		DPRINTK("%s: No buffer for TX\n", __func__);
 		return -1;
 	}
 
@@ -290,7 +290,7 @@ static inline void tso_unwind(netfront_a
 	struct netfront_accel_tso_buffer *tso_buf;
 	struct netfront_accel_tso_output_packet *output_packet;
 
-	DPRINTK("%s\n", __FUNCTION__);
+	DPRINTK("%s\n", __func__);
 
 	while (st->output_packets != NULL) {
 		output_packet = st->output_packets;
@@ -386,8 +386,7 @@ int netfront_accel_enqueue_skb_tso(netfr
 	}
 
 	if (tso_start_new_packet(vnic, skb, &state) < 0) {
-		DPRINTK("%s: out of first start-packet memory\n",
-			__FUNCTION__);
+		DPRINTK("%s: out of first start-packet memory\n", __func__);
 		goto unwind;
 	}
 
@@ -409,8 +408,7 @@ int netfront_accel_enqueue_skb_tso(netfr
 		if ((state.output_packets->tso_bufs->length == 
 		     NETFRONT_ACCEL_TSO_BUF_LENGTH) &&
 		    tso_start_new_buffer(vnic, &state, 0)) {
-			DPRINTK("%s: out of start-buffer memory\n",
-				__FUNCTION__);
+			DPRINTK("%s: out of start-buffer memory\n", __func__);
 			goto unwind;
 		}
 
@@ -421,8 +419,7 @@ int netfront_accel_enqueue_skb_tso(netfr
 		      (state.output_packets->tso_bufs->length >= 
 		       NETFRONT_ACCEL_TSO_BUF_LENGTH))) &&
 		    tso_start_new_packet(vnic, skb, &state) < 0) {
-			DPRINTK("%s: out of start-packet memory\n",
-				__FUNCTION__);
+			DPRINTK("%s: out of start-packet memory\n", __func__);
 			goto unwind;
 		}
 
@@ -431,7 +428,7 @@ int netfront_accel_enqueue_skb_tso(netfr
 	/* Check for space */
 	if (ef_vi_transmit_space(&vnic->vi) < state.buffers) {
 		DPRINTK("%s: Not enough TX space (%d)\n",
-			__FUNCTION__, state.buffers);
+			__func__, state.buffers);
 		goto unwind;
 	}
 
--- a/drivers/xen/sfc_netfront/accel_vi.c
+++ b/drivers/xen/sfc_netfront/accel_vi.c
@@ -81,7 +81,7 @@ int netfront_accel_vi_init_fini(netfront
 			(vnic->dev, hw_msg->resources.falcon_a.evq_rptr_gnt,
 			 &vnic->hw.falcon.evq_rptr_mapping);
 		if (io_kva == NULL) {
-			EPRINTK("%s: evq_rptr permission failed\n", __FUNCTION__);
+			EPRINTK("%s: evq_rptr permission failed\n", __func__);
 			goto evq_rptr_fail;
 		}
 
@@ -110,7 +110,7 @@ int netfront_accel_vi_init_fini(netfront
 		net_accel_map_grants_contig(vnic->dev, evq_gnts, 1 << evq_order,
 					    &vnic->evq_mapping);
 	if (evq_base == NULL) {
-		EPRINTK("%s: evq_base failed\n", __FUNCTION__);
+		EPRINTK("%s: evq_base failed\n", __func__);
 		goto evq_fail;
 	}
 
@@ -120,7 +120,7 @@ int netfront_accel_vi_init_fini(netfront
 		net_accel_map_iomem_page(vnic->dev, hw_info->doorbell_gnt,
 					 &vnic->hw.falcon.doorbell_mapping);
 	if (doorbell_kva == NULL) {
-		EPRINTK("%s: doorbell permission failed\n", __FUNCTION__);
+		EPRINTK("%s: doorbell permission failed\n", __func__);
 		goto doorbell_fail;
 	}
 	vnic->hw.falcon.doorbell = doorbell_kva;
@@ -140,7 +140,7 @@ int netfront_accel_vi_init_fini(netfront
 		(vnic->dev, &(hw_info->txdmaq_gnt), 1, 
 		 &vnic->hw.falcon.txdmaq_mapping);
 	if (tx_dma_kva == NULL) {
-		EPRINTK("%s: TX dma failed\n", __FUNCTION__);
+		EPRINTK("%s: TX dma failed\n", __func__);
 		goto tx_dma_fail;
 	}
 
@@ -148,7 +148,7 @@ int netfront_accel_vi_init_fini(netfront
 		(vnic->dev, &(hw_info->rxdmaq_gnt), 1, 
 		 &vnic->hw.falcon.rxdmaq_mapping);
 	if (rx_dma_kva == NULL) {
-		EPRINTK("%s: RX dma failed\n", __FUNCTION__);
+		EPRINTK("%s: RX dma failed\n", __func__);
 		goto rx_dma_fail;
 	}
 
@@ -186,7 +186,7 @@ int netfront_accel_vi_init_fini(netfront
 					       hw_info->tx_capacity);
 	vnic->vi_state = (ef_vi_state *)kmalloc(vi_state_size, GFP_KERNEL);
 	if (vnic->vi_state == NULL) {
-		EPRINTK("%s: kmalloc for VI state failed\n", __FUNCTION__);
+		EPRINTK("%s: kmalloc for VI state failed\n", __func__);
 		goto vi_state_fail;
 	}
 	ef_vi_init(&vnic->vi, vi_data, vnic->vi_state, &vnic->evq_state, 0);
@@ -288,7 +288,7 @@ void netfront_accel_vi_post_rx_or_free(n
 				       netfront_accel_pkt_desc *buf)
 {
 
-	VPRINTK("%s: %d\n", __FUNCTION__, id);
+	VPRINTK("%s: %d\n", __func__, id);
 
 	if (ef_vi_receive_space(&vnic->vi) <= vnic->rx_dma_batched) {
 		VPRINTK("RX space is full\n");
@@ -322,7 +322,7 @@ void netfront_accel_vi_add_bufs(netfront
 	       ef_vi_receive_space(&vnic->vi) > vnic->rx_dma_batched) {
 		netfront_accel_pkt_desc *buf;
 		
-		VPRINTK("%s: %d\n", __FUNCTION__, vnic->rx_dma_level);
+		VPRINTK("%s: %d\n", __func__, vnic->rx_dma_level);
 		
 		/* Try to allocate a buffer. */
 		buf = netfront_accel_buf_get(vnic->rx_bufs);
@@ -334,7 +334,7 @@ void netfront_accel_vi_add_bufs(netfront
 		netfront_accel_vi_post_rx(vnic, buf->buf_id, buf);
 	}
 
-	VPRINTK("%s: done\n", __FUNCTION__);
+	VPRINTK("%s: done\n", __func__);
 }
 
 
@@ -374,7 +374,7 @@ static int multi_post_start_new_buffer(n
 	/* Get a mapped packet buffer */
 	buf = netfront_accel_buf_get(vnic->tx_bufs);
 	if (buf == NULL) {
-		DPRINTK("%s: No buffer for TX\n", __FUNCTION__);
+		DPRINTK("%s: No buffer for TX\n", __func__);
 		return -1;
 	}
 
@@ -438,7 +438,7 @@ static inline void multi_post_unwind(net
 {
 	struct netfront_accel_tso_buffer *tso_buf;
 
-	DPRINTK("%s\n", __FUNCTION__);
+	DPRINTK("%s\n", __func__);
 
 	while (st->output_buffers != NULL) {
 		tso_buf = st->output_buffers;
@@ -469,7 +469,7 @@ netfront_accel_enqueue_skb_multi(netfron
 	}
 
 	if (multi_post_start_new_buffer(vnic, &state)) {
-		DPRINTK("%s: out of buffers\n", __FUNCTION__);
+		DPRINTK("%s: out of buffers\n", __func__);
 		goto unwind;
 	}
 
@@ -491,14 +491,14 @@ netfront_accel_enqueue_skb_multi(netfron
 		if ((state.output_buffers->length == 
 		     NETFRONT_ACCEL_TX_BUF_LENGTH) &&
 		    multi_post_start_new_buffer(vnic, &state)) {
-			DPRINTK("%s: out of buffers\n", __FUNCTION__);
+			DPRINTK("%s: out of buffers\n", __func__);
 			goto unwind;
 		}
 	}
 
 	/* Check for space */
 	if (ef_vi_transmit_space(&vnic->vi) < state.buffers) {
-		DPRINTK("%s: Not enough TX space (%d)\n", __FUNCTION__, state.buffers);
+		DPRINTK("%s: Not enough TX space (%d)\n", __func__, state.buffers);
 		goto unwind;
 	}
 
@@ -554,14 +554,14 @@ netfront_accel_enqueue_skb_single(netfro
 	int rc;
 
 	if (ef_vi_transmit_space(&vnic->vi) < 1) {
-		DPRINTK("%s: No TX space\n", __FUNCTION__);
+		DPRINTK("%s: No TX space\n", __func__);
 		NETFRONT_ACCEL_STATS_OP(vnic->stats.fastpath_tx_busy++);
 		return NETFRONT_ACCEL_STATUS_BUSY;
 	}
 
 	buf = netfront_accel_buf_get(vnic->tx_bufs);
 	if (buf == NULL) {
-		DPRINTK("%s: No buffer for TX\n", __FUNCTION__);
+		DPRINTK("%s: No buffer for TX\n", __func__);
 		NETFRONT_ACCEL_STATS_OP(vnic->stats.fastpath_tx_busy++);
 		return NETFRONT_ACCEL_STATUS_BUSY;
 	}
@@ -597,7 +597,7 @@ netfront_accel_enqueue_skb_single(netfro
 			kva += frag_len;
 		});
 
-	VPRINTK("%s: id %d pkt %p kva %p buff_addr 0x%08x\n", __FUNCTION__,
+	VPRINTK("%s: id %d pkt %p kva %p buff_addr 0x%08x\n", __func__,
 		buf->buf_id, buf, buf->pkt_kva, buf->pkt_buff_addr);
 
 
@@ -653,7 +653,7 @@ netfront_accel_vi_tx_post(netfront_accel
 
 	/* Check to see if the packet can be sent. */
 	if (skb_headlen(skb) < sizeof(*pkt_eth_hdr) + sizeof(*pkt_ipv4_hdr)) {
-		EPRINTK("%s: Packet header is too small\n", __FUNCTION__);
+		EPRINTK("%s: Packet header is too small\n", __func__);
 		return NETFRONT_ACCEL_STATUS_CANT;
 	}
 
@@ -661,7 +661,7 @@ netfront_accel_vi_tx_post(netfront_accel
 	pkt_ipv4_hdr = (void*)(pkt_eth_hdr+1);
 
 	if (be16_to_cpu(pkt_eth_hdr->h_proto) != ETH_P_IP) {
-		DPRINTK("%s: Packet is not IPV4 (ether_type=0x%04x)\n", __FUNCTION__,
+		DPRINTK("%s: Packet is not IPV4 (ether_type=0x%04x)\n", __func__,
 			be16_to_cpu(pkt_eth_hdr->h_proto));
 		return NETFRONT_ACCEL_STATUS_CANT;
 	}
@@ -669,11 +669,11 @@ netfront_accel_vi_tx_post(netfront_accel
 	if (pkt_ipv4_hdr->protocol != IPPROTO_TCP &&
 	    pkt_ipv4_hdr->protocol != IPPROTO_UDP) {
 		DPRINTK("%s: Packet is not TCP/UDP (ip_protocol=0x%02x)\n",
-			__FUNCTION__, pkt_ipv4_hdr->protocol);
+			__func__, pkt_ipv4_hdr->protocol);
 		return NETFRONT_ACCEL_STATUS_CANT;
 	}
 	
-	VPRINTK("%s: %d bytes, gso %d\n", __FUNCTION__, skb->len, 
+	VPRINTK("%s: %d bytes, gso %d\n", __func__, skb->len,
 		skb_shinfo(skb)->gso_size);
 	
 	if (skb_is_gso(skb))
@@ -772,7 +772,7 @@ static void  netfront_accel_vi_rx_comple
 		u16 port;
 
 		DPRINTK("%s: saw wrong MAC address %pM\n",
-			__FUNCTION__, skb->data);
+			__func__, skb->data);
 
 		if (ip->protocol == IPPROTO_TCP) {
 			struct tcphdr *tcp = (struct tcphdr *)
@@ -880,7 +880,7 @@ static int netfront_accel_vi_poll_proces
 
 			if (skb == NULL) {
 				DPRINTK("%s: Couldn't get an rx skb.\n",
-					__FUNCTION__);
+					__func__);
 				netfront_accel_vi_post_rx_or_free(vnic, (u16)id, buf);
 				/*
 				 * Dropping this fragment means we
@@ -901,7 +901,7 @@ static int netfront_accel_vi_poll_proces
 			 * received
 			 */
 			EPRINTK("%s: Rx packet too large (%d)\n",
-				__FUNCTION__, len);
+				__func__, len);
 			netfront_accel_vi_post_rx_or_free(vnic, (u16)id, buf);
 			discard_jumbo_state(vnic);
 			return 0;
@@ -930,28 +930,28 @@ static int netfront_accel_vi_poll_proces
 		    == EF_EVENT_RX_DISCARD_TRUNC) {
 			DPRINTK("%s: " EF_EVENT_FMT 
 				" buffer %d FRM_TRUNC q_id %d\n",
-				__FUNCTION__, EF_EVENT_PRI_ARG(*ev), id,
+				__func__, EF_EVENT_PRI_ARG(*ev), id,
 				EF_EVENT_RX_DISCARD_Q_ID(*ev) );
 			NETFRONT_ACCEL_STATS_OP(++vnic->stats.fastpath_frm_trunc);
 		} else if (EF_EVENT_RX_DISCARD_TYPE(*ev) 
 			  == EF_EVENT_RX_DISCARD_OTHER) {
 			DPRINTK("%s: " EF_EVENT_FMT 
 				" buffer %d RX_DISCARD_OTHER q_id %d\n",
-				__FUNCTION__, EF_EVENT_PRI_ARG(*ev), id,
+				__func__, EF_EVENT_PRI_ARG(*ev), id,
 				EF_EVENT_RX_DISCARD_Q_ID(*ev) );
 			NETFRONT_ACCEL_STATS_OP(++vnic->stats.fastpath_discard_other);
 		} else if (EF_EVENT_RX_DISCARD_TYPE(*ev) ==
 			   EF_EVENT_RX_DISCARD_CSUM_BAD) {
 			DPRINTK("%s: " EF_EVENT_FMT 
 				" buffer %d DISCARD CSUM_BAD q_id %d\n",
-				__FUNCTION__, EF_EVENT_PRI_ARG(*ev), id,
+				__func__, EF_EVENT_PRI_ARG(*ev), id,
 				EF_EVENT_RX_DISCARD_Q_ID(*ev) );
 			NETFRONT_ACCEL_STATS_OP(++vnic->stats.fastpath_csum_bad);
 		} else if (EF_EVENT_RX_DISCARD_TYPE(*ev) ==
 			   EF_EVENT_RX_DISCARD_CRC_BAD) {
 			DPRINTK("%s: " EF_EVENT_FMT 
 				" buffer %d DISCARD CRC_BAD q_id %d\n",
-				__FUNCTION__, EF_EVENT_PRI_ARG(*ev), id,
+				__func__, EF_EVENT_PRI_ARG(*ev), id,
 				EF_EVENT_RX_DISCARD_Q_ID(*ev) );
 			NETFRONT_ACCEL_STATS_OP(++vnic->stats.fastpath_crc_bad);
 		} else {
@@ -959,7 +959,7 @@ static int netfront_accel_vi_poll_proces
 			       EF_EVENT_RX_DISCARD_RIGHTS);
 			DPRINTK("%s: " EF_EVENT_FMT 
 				" buffer %d DISCARD RIGHTS q_id %d\n",
-				__FUNCTION__, EF_EVENT_PRI_ARG(*ev), id,
+				__func__, EF_EVENT_PRI_ARG(*ev), id,
 				EF_EVENT_RX_DISCARD_Q_ID(*ev) );
 			NETFRONT_ACCEL_STATS_OP(++vnic->stats.fastpath_rights_bad);
 		}
@@ -982,7 +982,7 @@ missing_head:
 	vnic->netdev_stats.fastpath_rx_errors++;
 
 	DPRINTK("%s experienced bad packet/missing fragment error: %d \n",
-		__FUNCTION__, ev->rx.flags);
+		__func__, ev->rx.flags);
 
 	return 0;
 }
@@ -1001,12 +1001,12 @@ static void netfront_accel_vi_not_busy(n
 	 */
 
 	if (vnic->tx_skb != NULL) {
-		DPRINTK("%s trying to send spare buffer\n", __FUNCTION__);
+		DPRINTK("%s trying to send spare buffer\n", __func__);
 		
 		handled = netfront_accel_vi_tx_post(vnic, vnic->tx_skb);
 		
 		if (handled != NETFRONT_ACCEL_STATUS_BUSY) {
-			DPRINTK("%s restarting tx\n", __FUNCTION__);
+			DPRINTK("%s restarting tx\n", __func__);
 
 			/* Need netfront tx_lock and vnic tx_lock to
 			 * write tx_skb */
@@ -1111,11 +1111,11 @@ int netfront_accel_vi_poll(netfront_acce
 	i = 0;
 	NETFRONT_ACCEL_STATS_OP(n_evs_polled += events);
 
-	VPRINTK("%s: %d events\n", __FUNCTION__, events);
+	VPRINTK("%s: %d events\n", __func__, events);
 
 	/* Loop over each event */
 	while (events) {
-		VPRINTK("%s: Event "EF_EVENT_FMT", index %lu\n", __FUNCTION__, 
+		VPRINTK("%s: Event "EF_EVENT_FMT", index %lu\n", __func__,
 			EF_EVENT_PRI_ARG(ev[i]),	
 			(unsigned long)(vnic->vi.evq_state->evq_ptr));
 
@@ -1131,7 +1131,7 @@ int netfront_accel_vi_poll(netfront_acce
 		} else if (EF_EVENT_TYPE(ev[i]) == 
 			   EF_EVENT_TYPE_RX_NO_DESC_TRUNC) {
 			DPRINTK("%s: RX_NO_DESC_TRUNC " EF_EVENT_FMT "\n",
-				__FUNCTION__, EF_EVENT_PRI_ARG(ev[i]));
+				__func__, EF_EVENT_PRI_ARG(ev[i]));
 			discard_jumbo_state(vnic);
 			NETFRONT_ACCEL_STATS_OP(vnic->stats.rx_no_desc_trunc++);
 		} else {
@@ -1174,14 +1174,14 @@ int netfront_accel_vi_enable_interrupts(
 {
 	u32 sw_evq_ptr;
 
-	VPRINTK("%s: checking for event on %p\n", __FUNCTION__, &vnic->vi.evq_state);
+	VPRINTK("%s: checking for event on %p\n", __func__, &vnic->vi.evq_state);
 
 	BUG_ON(vnic == NULL);
 	BUG_ON(vnic->vi.evq_state == NULL);
 
 	/* Do a quick check for an event. */
 	if (ef_eventq_has_event(&vnic->vi)) {
-		VPRINTK("%s: found event\n",  __FUNCTION__);
+		VPRINTK("%s: found event\n", __func__);
 		return 0;
 	}
 
--- a/drivers/xen/sfc_netfront/accel_xenbus.c
+++ b/drivers/xen/sfc_netfront/accel_xenbus.c
@@ -52,7 +52,7 @@ static void mac_address_change(struct xe
 	struct xenbus_device *dev;
 	int rc;
 
-	DPRINTK("%s\n", __FUNCTION__);
+	DPRINTK("%s\n", __func__);
 	
 	vnic = container_of(watch, netfront_accel_vnic, 
 				mac_address_watch);
@@ -61,7 +61,7 @@ static void mac_address_change(struct xe
 	rc = net_accel_xen_net_read_mac(dev, vnic->mac);
 
 	if (rc != 0)
-		EPRINTK("%s: failed to read mac (%d)\n", __FUNCTION__, rc);
+		EPRINTK("%s: failed to read mac (%d)\n", __func__, rc);
 }
 
 
@@ -77,7 +77,7 @@ static int setup_mac_address_watch(struc
 				 mac_address_change);
 	if (err) {
 		EPRINTK("%s: Failed to register xenbus watch: %d\n",
-			__FUNCTION__, err);
+			__func__, err);
 		goto fail;
 	}
 
@@ -104,12 +104,12 @@ static int make_named_grant(struct xenbu
 		err = xenbus_transaction_start(&tr);
 		if (err != 0) {
 			EPRINTK("%s: transaction start failed %d\n",
-				__FUNCTION__, err);
+				__func__, err);
 			return err;
 		}
 		err = xenbus_printf(tr, dev->nodename, name, "%d", gnt);
 		if (err != 0) {
-			EPRINTK("%s: xenbus_printf failed %d\n", __FUNCTION__,
+			EPRINTK("%s: xenbus_printf failed %d\n", __func__,
 				err);
 			xenbus_transaction_end(tr, 1);
 			return err;
@@ -118,7 +118,7 @@ static int make_named_grant(struct xenbu
 	} while (err == -EAGAIN);
 	
 	if (err != 0) {
-		EPRINTK("%s: transaction end failed %d\n", __FUNCTION__, err);
+		EPRINTK("%s: transaction end failed %d\n", __func__, err);
 		return err;
 	}
 	
@@ -140,12 +140,12 @@ static int remove_named_grant(struct xen
 		err = xenbus_transaction_start(&tr);
 		if (err != 0) {
 			EPRINTK("%s: transaction start failed %d\n",
-				__FUNCTION__, err);
+				__func__, err);
 			return err;
 		}
 		err = xenbus_rm(tr, dev->nodename, name);
 		if (err != 0) {
-			EPRINTK("%s: xenbus_rm failed %d\n", __FUNCTION__,
+			EPRINTK("%s: xenbus_rm failed %d\n", __func__,
 				err);
 			xenbus_transaction_end(tr, 1);
 			return err;
@@ -154,7 +154,7 @@ static int remove_named_grant(struct xen
 	} while (err == -EAGAIN);
 	
 	if (err != 0) {
-		EPRINTK("%s: transaction end failed %d\n", __FUNCTION__, err);
+		EPRINTK("%s: transaction end failed %d\n", __func__, err);
 		return err;
 	}
 
@@ -182,7 +182,7 @@ netfront_accel_vnic *netfront_accel_vnic
 	/* Alloc mem for state */
 	vnic = kzalloc(sizeof(netfront_accel_vnic), GFP_KERNEL);
 	if (vnic == NULL) {
-		EPRINTK("%s: no memory for vnic state\n", __FUNCTION__);
+		EPRINTK("%s: no memory for vnic state\n", __func__);
 		return ERR_PTR(-ENOMEM);
 	}
 
@@ -263,7 +263,7 @@ static void netfront_accel_vnic_dtor(net
 	 * this watch and synchonrise with the completion of
 	 * watches
 	 */
-	DPRINTK("%s: unregistering xenbus mac watch\n", __FUNCTION__);
+	DPRINTK("%s: unregistering xenbus mac watch\n", __func__);
 	unregister_xenbus_watch(&vnic->mac_address_watch);
 	kfree(vnic->mac_address_watch.node);
 
@@ -303,14 +303,14 @@ static int vnic_setup_domU_shared_state(
 	vnic->tx_bufs = netfront_accel_init_bufs(&vnic->tx_lock);
 	if (vnic->tx_bufs == NULL) {
 		err = -ENOMEM;
-		EPRINTK("%s: Failed to allocate tx buffers\n", __FUNCTION__);
+		EPRINTK("%s: Failed to allocate tx buffers\n", __func__);
 		goto fail_tx_bufs;
 	}
 
 	vnic->rx_bufs = netfront_accel_init_bufs(NULL);
 	if (vnic->rx_bufs == NULL) {
 		err = -ENOMEM;
-		EPRINTK("%s: Failed to allocate rx buffers\n", __FUNCTION__);
+		EPRINTK("%s: Failed to allocate rx buffers\n", __func__);
 		goto fail_rx_bufs;
 	}
 
@@ -321,7 +321,7 @@ static int vnic_setup_domU_shared_state(
 	vnic->shared_page = (struct net_accel_shared_page *)
 		__get_free_pages(GFP_KERNEL, 1);
 	if (vnic->shared_page == NULL) {
-		EPRINTK("%s: no memory for shared pages\n", __FUNCTION__);
+		EPRINTK("%s: no memory for shared pages\n", __func__);
 		err = -ENOMEM;
 		goto fail_shared_page;
 	}
@@ -385,7 +385,7 @@ static int vnic_setup_domU_shared_state(
 		err = xenbus_transaction_start(&tr);
 		if (err != 0) {
 			EPRINTK("%s: Transaction start failed %d\n",
-				__FUNCTION__, err);
+				__func__, err);
 			goto fail_transaction;
 		}
 
@@ -393,7 +393,7 @@ static int vnic_setup_domU_shared_state(
 				    "%u", vnic->msg_channel);
 		if (err != 0) {
 			EPRINTK("%s: event channel xenbus write failed %d\n",
-				__FUNCTION__, err);
+				__func__, err);
 			xenbus_transaction_end(tr, 1);
 			goto fail_transaction;
 		}
@@ -402,7 +402,7 @@ static int vnic_setup_domU_shared_state(
 				    "%u", vnic->net_channel);
 		if (err != 0) {
 			EPRINTK("%s: net channel xenbus write failed %d\n",
-				__FUNCTION__, err);
+				__func__, err);
 			xenbus_transaction_end(tr, 1);
 			goto fail_transaction;
 		}
@@ -411,7 +411,7 @@ static int vnic_setup_domU_shared_state(
 	} while (err == -EAGAIN);
 
 	if (err != 0) {
-		EPRINTK("%s: Transaction end failed %d\n", __FUNCTION__, err);
+		EPRINTK("%s: Transaction end failed %d\n", __func__, err);
 		goto fail_transaction;
 	}
 
@@ -465,7 +465,7 @@ static void vnic_remove_domU_shared_stat
 	 */
 
 	DPRINTK("%s: removing event channel irq handlers %d %d\n",
-		__FUNCTION__, vnic->net_channel_irq, vnic->msg_channel_irq);
+		__func__, vnic->net_channel_irq, vnic->msg_channel_irq);
 	do {
 		if (xenbus_transaction_start(&tr) != 0)
 			break;
@@ -556,7 +556,7 @@ static void netfront_accel_backend_accel
 	int state;
 
 	DPRINTK("%s: changing from %s to %s. nodename %s, otherend %s\n",
-		__FUNCTION__, xenbus_strstate(vnic->backend_state),
+		__func__, xenbus_strstate(vnic->backend_state),
 		xenbus_strstate(backend_state), dev->nodename, dev->otherend);
 
 	/*
@@ -646,7 +646,7 @@ static void backend_accel_state_change(s
 	netfront_accel_vnic *vnic;
 	struct xenbus_device *dev;
 
-	DPRINTK("%s\n", __FUNCTION__);
+	DPRINTK("%s\n", __func__);
 
 	vnic = container_of(watch, struct netfront_accel_vnic,
 				backend_accel_watch);
@@ -675,7 +675,7 @@ static int setup_dom0_accel_watch(struct
 				 backend_accel_state_change);
 	if (err) {
 		EPRINTK("%s: Failed to register xenbus watch: %d\n",
-			__FUNCTION__, err);
+			__func__, err);
 		goto fail;
 	}
 	return 0;
@@ -703,7 +703,7 @@ int netfront_accel_probe(struct net_devi
 	err = setup_dom0_accel_watch(dev, vnic);
 	if (err) {
 		netfront_accel_vnic_dtor(vnic);
-		EPRINTK("%s: probe failed with code %d\n", __FUNCTION__, err);
+		EPRINTK("%s: probe failed with code %d\n", __func__, err);
 		return err;
 	}
 
@@ -730,7 +730,7 @@ int netfront_accel_remove(struct xenbus_
 	struct netfront_info *np = dev_get_drvdata(&dev->dev);
 	netfront_accel_vnic *vnic = (netfront_accel_vnic *)np->accel_priv;
 
-	DPRINTK("%s %s\n", __FUNCTION__, dev->nodename);
+	DPRINTK("%s %s\n", __func__, dev->nodename);
 
 	BUG_ON(vnic == NULL);
 
@@ -747,7 +747,7 @@ int netfront_accel_remove(struct xenbus_
 
 	mutex_unlock(&vnic->vnic_mutex);
 
-	DPRINTK("%s waiting for release of %s\n", __FUNCTION__, dev->nodename);
+	DPRINTK("%s waiting for release of %s\n", __func__, dev->nodename);
 
 	/*
 	 * Wait for the xenbus watch to release the shared resources.
@@ -762,14 +762,13 @@ int netfront_accel_remove(struct xenbus_
 	 * Now we don't need this watch anymore it is safe to remove
 	 * it (and so synchronise with it completing if outstanding)
 	 */
-	DPRINTK("%s: unregistering xenbus accel watch\n",
-		__FUNCTION__);
+	DPRINTK("%s: unregistering xenbus accel watch\n", __func__);
 	unregister_xenbus_watch(&vnic->backend_accel_watch);
 	kfree(vnic->backend_accel_watch.node);
 
 	netfront_accel_vnic_dtor(vnic);
 
-	DPRINTK("%s done %s\n", __FUNCTION__, dev->nodename);
+	DPRINTK("%s done %s\n", __func__, dev->nodename);
 
 	return 0;
 }
--- a/drivers/xen/sfc_netutil/accel_cuckoo_hash.c
+++ b/drivers/xen/sfc_netutil/accel_cuckoo_hash.c
@@ -345,7 +345,7 @@ int cuckoo_hash_rehash(cuckoo_hash_table
 
 	return 0;
  err:
-	EPRINTK("%s: Rehash failed, giving up\n", __FUNCTION__);
+	EPRINTK("%s: Rehash failed, giving up\n", __func__);
 	/* Some other error, give up, at least restore table to how it was */
 	memcpy(hashtab, &old_hashtab, sizeof(cuckoo_hash_table));
 	if (new_table)
@@ -422,7 +422,7 @@ int cuckoo_hash_add(cuckoo_hash_table *h
 		goto again;
 	}
   
-	EPRINTK("%s: failed hash add\n", __FUNCTION__);
+	EPRINTK("%s: failed hash add\n", __func__);
 	/*
 	 * Couldn't do it - bad as we've now removed some random thing
 	 * from the table, and will just drop it on the floor.  Better
@@ -556,7 +556,7 @@ void cuckoo_hash_valid(cuckoo_hash_table
 	}
 	
 	if (entry_count != hashtab->entries) {
-		EPRINTK("%s: bad count\n", __FUNCTION__);
+		EPRINTK("%s: bad count\n", __func__);
 		cuckoo_hash_dump(hashtab);
 		return;
 	}
@@ -567,7 +567,7 @@ void cuckoo_hash_valid(cuckoo_hash_table
 						     &hashtab->table0[i].key, 
 						     &hashtab->a0)) {
 				EPRINTK("%s: Bad key table 0 index %d\n",
-					__FUNCTION__, i);
+					__func__, i);
 				cuckoo_hash_dump(hashtab);
 				return;
 			}
@@ -576,7 +576,7 @@ void cuckoo_hash_valid(cuckoo_hash_table
 						     &hashtab->table1[i].key, 
 						     &hashtab->a1)) {
 				EPRINTK("%s: Bad key table 1 index %d\n",
-					__FUNCTION__, i);
+					__func__, i);
 				cuckoo_hash_dump(hashtab);
 				return;
 			}
--- a/drivers/xen/sfc_netutil/accel_msg_iface.c
+++ b/drivers/xen/sfc_netutil/accel_msg_iface.c
@@ -37,7 +37,7 @@
 #define NET_ACCEL_CHECK_MAGIC(_p, _errval)				\
 	if (_p->magic != NET_ACCEL_MSG_MAGIC) {				\
 		pr_err("%s: passed invalid shared page %p!\n",		\
-		       __FUNCTION__, _p);				\
+		       __func__, _p);					\
 		return _errval;						\
 	}
 #define NET_ACCEL_SHOW_QUEUE(_t, _q, _id)				\
--- a/drivers/xen/sfc_netutil/accel_util.c
+++ b/drivers/xen/sfc_netutil/accel_util.c
@@ -272,7 +272,7 @@ EXPORT_SYMBOL_GPL(net_accel_grant_page);
 int net_accel_ungrant_page(grant_ref_t gntref)
 {
 	if (unlikely(gnttab_query_foreign_access(gntref) != 0)) {
-		EPRINTK("%s: remote domain still using grant %d\n", __FUNCTION__, 
+		EPRINTK("%s: remote domain still using grant %d\n", __func__,
 			gntref);
 		return -EBUSY;
 	}
@@ -312,11 +312,11 @@ void net_accel_update_state(struct xenbu
 	struct xenbus_transaction tr;
 	int err;
 
-	DPRINTK("%s: setting accelstate to %s\n", __FUNCTION__,
+	DPRINTK("%s: setting accelstate to %s\n", __func__,
 		xenbus_strstate(state));
 
 	if (xenbus_exists(XBT_NIL, dev->nodename, "")) {
-		VPRINTK("%s: nodename %s\n", __FUNCTION__, dev->nodename);
+		VPRINTK("%s: nodename %s\n", __func__, dev->nodename);
 	again:
 		err = xenbus_transaction_start(&tr);
 		if (err == 0)
--- a/drivers/xen/tpmback/interface.c
+++ b/drivers/xen/tpmback/interface.c
@@ -50,7 +50,7 @@ static tpmif_t *alloc_tpmif(domid_t domi
  out_of_memory:
 	if (tpmif != NULL)
 		kmem_cache_free(tpmif_cachep, tpmif);
-	pr_err("%s: out of memory\n", __FUNCTION__);
+	pr_err("%s: out of memory\n", __func__);
 	return ERR_PTR(-ENOMEM);
 }
 
--- a/drivers/xen/tpmback/tpmback.c
+++ b/drivers/xen/tpmback/tpmback.c
@@ -876,7 +876,7 @@ static void tpm_tx_action(unsigned long 
 	tpmif_t *tpmif;
 	tpmif_tx_request_t *tx;
 
-	DPRINTK("%s: Getting data from front-end(s)!\n", __FUNCTION__);
+	DPRINTK("%s: Getting data from front-end(s)!\n", __func__);
 
 	while (!list_empty(&tpm_schedule_list)) {
 		/* Get a tpmif from the list with work to do. */
--- a/drivers/xen/usbback/xenbus.c
+++ b/drivers/xen/usbback/xenbus.c
@@ -272,7 +272,7 @@ static void frontend_changed(struct xenb
 	case XenbusStateInitialising:
 		if (dev->state == XenbusStateClosed) {
 			pr_info("%s: %s: prepare for reconnect\n",
-				__FUNCTION__, dev->nodename);
+				__func__, dev->nodename);
 			xenbus_switch_state(dev, XenbusStateInitWait);
 		}
 		break;
--- a/drivers/xen/xenbus/xenbus_probe.c
+++ b/drivers/xen/xenbus/xenbus_probe.c
@@ -35,7 +35,7 @@
 
 #define DPRINTK(fmt, args...)				\
 	pr_debug("xenbus_probe (%s:%d) " fmt ".\n",	\
-		 __FUNCTION__, __LINE__, ##args)
+		 __func__, __LINE__, ##args)
 
 #include <linux/kernel.h>
 #include <linux/version.h>
@@ -351,7 +351,7 @@ void xenbus_dev_shutdown(struct device *
 
 	get_device(&dev->dev);
 	if (dev->state != XenbusStateConnected) {
-		dev_info(&dev->dev, "%s: %s: %s != Connected, skipping\n", __FUNCTION__,
+		dev_info(&dev->dev, "%s: %s: %s != Connected, skipping\n", __func__,
 		         dev->nodename, xenbus_strstate(dev->state));
 		goto out;
 	}
@@ -363,7 +363,7 @@ void xenbus_dev_shutdown(struct device *
 	timeout = wait_for_completion_timeout(&dev->down, timeout);
 	if (!timeout)
 		dev_info(&dev->dev, "%s: %s timeout closing device\n",
-		         __FUNCTION__, dev->nodename);
+		         __func__, dev->nodename);
  out:
 	put_device(&dev->dev);
 }
